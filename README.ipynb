{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glasses ðŸ˜Ž\n",
    "\n",
    "![alt](https://github.com/FrancescoSaverioZuppichini/glasses/blob/develop/docs/_static/images/background.png?raw=true)\n",
    "\n",
    "[![codecov](https://codecov.io/gh/FrancescoSaverioZuppichini/glasses/branch/develop/graph/badge.svg)](https://codecov.io/gh/FrancescoSaverioZuppichini/glasses)\n",
    "\n",
    "Compact, concise and customizable \n",
    "deep learning computer vision library\n",
    "\n",
    "**This is an early beta, code will change and pretrained weights are not available (I need to find a place to store them online, any advice?)**\n",
    "\n",
    "Doc is [here](https://francescosaveriozuppichini.github.io/glasses/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "You can install `glasses` using pip by running\n",
    "\n",
    "```\n",
    "pip install git+https://github.com/FrancescoSaverioZuppichini/glasses\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "\n",
    "All the existing implementation of the most famous model are written with very bad coding practices, what today is called *research code*. I struggled myself to understand some of the implementation that in the end were just few lines of code. \n",
    "\n",
    "Most of them are missing a global structure, they used tons of code repetition, they are not easily customizable and not tested. Since I do computer vision for living, so I needed a way to make my life easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "The API are shared across **all** models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from glasses import AutoModel, AutoConfig\n",
    "from torch import nn\n",
    "# load one model\n",
    "model = AutoModel.from_pretrained('resnet18')\n",
    "cfg = AutoConfig.from_name('resnet18')\n",
    "model.summary(device='cpu') # thanks to torchsummary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from glasses.interpretability import GradCam, SaliencyMap\n",
    "from torchvision.transforms import Normalize\n",
    "r = requests.get('https://i.insider.com/5df126b679d7570ad2044f3e?width=700&format=jpeg&auto=webp')\n",
    "im = Image.open(BytesIO(r.content))\n",
    "# un normalize when done\n",
    "postprocessing = Normalize(-cfg.mean / cfg.std, (1.0 / cfg.std))\n",
    "# apply preprocessing\n",
    "x =  cfg.transform(im).unsqueeze(0)\n",
    "_ = model.interpret(x, using=GradCam(), postprocessing=postprocessing).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](https://github.com/FrancescoSaverioZuppichini/glasses/blob/develop/docs/_static/images/grad_cam.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from glasses.nn.models import ResNet\n",
    "# change activation\n",
    "ResNet.resnet18(activation = nn.SELU)\n",
    "# change number of classes\n",
    "ResNet.resnet18(n_classes=100)\n",
    "# freeze only the convolution weights\n",
    "model = ResNet.resnet18(pretrained=True)\n",
    "model.freeze(who=model.encoder)\n",
    "# get the last layer, usuful to hook to it if you want to get the embeeded vector\n",
    "model.encoder.layers[-1]\n",
    "# what about resnet with inverted residuals?\n",
    "from glasses.nn.models.classification.efficientnet import InvertedResidualBlock\n",
    "ResNet.resnet18(block = InvertedResidualBlock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from glasses.nn.models.segmentation.unet import UNet, UNetDecoder\n",
    "# vanilla Unet\n",
    "unet = UNet()\n",
    "# let's change the encoder\n",
    "unet = UNet.from_encoder(partial(AutoModel.from_name, 'efficientnet_b1'))\n",
    "# mmm I want more layers in the decoder!\n",
    "unet = UNet(decoder=partial(UNetDecoder, widths=[256, 128, 64, 32, 16]))\n",
    "# maybe resnet was better\n",
    "unet = UNet(encoder=lambda **kwargs: ResNet.resnet26(**kwargs).encoder)\n",
    "# same API\n",
    "unet.summary(input_shape=(1,224,224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the decoder part\n",
    "model = ResNet.resnet18(pretrained=True)\n",
    "my_head = nn.Sequential(\n",
    "    nn.AdaptiveAvgPool2d((1,1)),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(model.encoder.widths[-1], 512),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 1000))\n",
    "\n",
    "model.head = my_head\n",
    "\n",
    "x = torch.rand((1,3,224,224))\n",
    "model(x).shape #torch.Size([1, 1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Models\n",
    "\n",
    "This is a list of all the pretrained models available so far!. They are all trained on *ImageNet*\n",
    "\n",
    "|                  |    top1 |    top5 |     time |\n",
    "|:-----------------|--------:|--------:|---------:|\n",
    "| efficientnet_b3  | 0.8204  | 0.96044 | 233.535  |\n",
    "| cse_resnet50     | 0.80236 | 0.9507  | 103.796  |\n",
    "| efficientnet_b2  | 0.8011  | 0.95118 | 143.739  |\n",
    "| resnext101_32x8d | 0.79312 | 0.94526 | 332.005  |\n",
    "| wide_resnet101_2 | 0.78848 | 0.94284 | 234.597  |\n",
    "| wide_resnet50_2  | 0.78468 | 0.94086 | 146.662  |\n",
    "| efficientnet_b1  | 0.78338 | 0.94078 | 109.463  |\n",
    "| resnet152        | 0.78312 | 0.94046 | 207.622  |\n",
    "| resnext50_32x4d  | 0.77618 | 0.93698 | 135.172  |\n",
    "| resnet101        | 0.77374 | 0.93546 | 151.992  |\n",
    "| efficientnet_b0  | 0.77364 | 0.9356  |  74.3195 |\n",
    "| densenet161      | 0.77138 | 0.9356  | 201.173  |\n",
    "| densenet201      | 0.76896 | 0.9337  | 143.988  |\n",
    "| resnet50         | 0.7613  | 0.92862 |  92.408  |\n",
    "| densenet169      | 0.756   | 0.92806 | 115.986  |\n",
    "| resnet26         | 0.75292 | 0.9257  |  65.2226 |\n",
    "| resnet34         | 0.75112 | 0.92288 |  61.9156 |\n",
    "| densenet121      | 0.74434 | 0.91972 |  95.5099 |\n",
    "| vgg19_bn         | 0.74218 | 0.91842 | 172.343  |\n",
    "| vgg16_bn         | 0.7336  | 0.91516 | 152.662  |\n",
    "| vgg19            | 0.72376 | 0.90876 | 160.982  |\n",
    "| mobilenet_v2     | 0.71878 | 0.90286 |  53.3237 |\n",
    "| vgg16            | 0.71592 | 0.90382 | 141.572  |\n",
    "| vgg13_bn         | 0.71586 | 0.90374 | 129.88   |\n",
    "| vgg11_bn         | 0.7037  | 0.8981  |  91.5699 |\n",
    "| vgg13            | 0.69928 | 0.89246 | 119.631  |\n",
    "| resnet18         | 0.69758 | 0.89078 |  46.7778 |\n",
    "| vgg11            | 0.6902  | 0.88628 |  84.9438 |\n",
    "\n",
    "Assuming you want to load `efficientnet_b1`, you can also grab it from its class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from glasses.nn.models import EfficientNet\n",
    "\n",
    "model = EfficientNet.efficientnet_b1(pretrained=True)\n",
    "# you may also need to get the correct transformation that must be applied on the input\n",
    "cfg = AutoConfig.from_name('efficientnet_b1')\n",
    "transform = cfg.transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, `transform` is \n",
    "\n",
    "```\n",
    "Compose(\n",
    "    Resize(size=240, interpolation=PIL.Image.BICUBIC)\n",
    "    CenterCrop(size=(240, 240))\n",
    "    ToTensor()\n",
    "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Customization\n",
    "\n",
    "All models are composed by sharable parts:\n",
    "- `Block`\n",
    "- `Layer`\n",
    "- `Encoder`\n",
    "- `Head`\n",
    "- `Decoder`\n",
    "\n",
    "### Block\n",
    "\n",
    "Each model has its building block, they are noted by `*Block`. In each block, all the weights are in the `.block` field. This makes it very easy to customize one specific model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glasses.nn.models.classification.vgg import VGGBasicBlock\n",
    "from glasses.nn.models.classification.resnet import ResNetBasicBlock, ResNetBottleneckBlock, ResNetBasicPreActBlock, ResNetBottleneckPreActBlock\n",
    "from glasses.nn.models.classification.senet import SENetBasicBlock, SENetBottleneckBlock\n",
    "from glasses.nn.models.classification.resnetxt import ResNetXtBottleNeckBlock\n",
    "from glasses.nn.models.classification.densenet import DenseBottleNeckBlock\n",
    "from glasses.nn.models.classification.wide_resnet import WideResNetBottleNeckBlock\n",
    "from glasses.nn.models.classification.efficientnet import EfficientNetBasicBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if we want to add Squeeze and Excitation to the resnet bottleneck block, we can just"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from glasses.nn.att import SpatialSE\n",
    "from  glasses.nn.models.classification.resnet import ResNetBottleneckBlock\n",
    "\n",
    "class SEResNetBottleneckBlock(ResNetBottleneckBlock):\n",
    "    def __init__(self, in_features: int, out_features: int, squeeze: int = 16, *args, **kwargs):\n",
    "        super().__init__(in_features, out_features, *args, **kwargs)\n",
    "        # all the weights are in block, we want to apply se after the weights\n",
    "        self.block.add_module('se', SpatialSE(out_features, reduction=squeeze))\n",
    "        \n",
    "SEResNetBottleneckBlock(32, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can use the class methods to create the new models following the existing architecture blueprint, for example, to create `se_resnet50`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ResNet.resnet50(block=ResNetBottleneckBlock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cool thing is each model has the same api, if I want to create a vgg13 with the `ResNetBottleneckBlock` I can just"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from glasses.nn.models import VGG\n",
    "model = VGG.vgg13(block=SEResNetBottleneckBlock)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some specific model can require additional parameter to the block, for example `MobileNetV2` also required a `expansion` parameter so our `SEResNetBottleneckBlock` won't work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer\n",
    "\n",
    "A `Layer` is a collection of blocks, it is used to stack multiple blocks together following some logic. For example, `ResNetLayer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from glasses.nn.models.classification.resnet import ResNetLayer\n",
    "\n",
    "ResNetLayer(64, 128, depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "The encoder is what encoders a vector, so the convolution layers. It has always two very important parameters.\n",
    "\n",
    "- widths\n",
    "- depths\n",
    "\n",
    "\n",
    "**widths** is the wide at each layer, so how much features there are\n",
    "**depths** is the depth at each layer, so how many blocks there are\n",
    "\n",
    "For example, `ResNetEncoder` will creates multiple `ResNetLayer` based on the len of `widths` and `depths`. Let's see some example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from glasses.nn.models.classification.resnet import ResNetEncoder\n",
    "# 3 layers, with 32,64,128 features and 1,2,3 block each\n",
    "ResNetEncoder(\n",
    "    widths=[32,64,128],\n",
    "    depths=[1,2,3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All encoders are subclass of `Encoder` that allows us to hook on specific stages to get the featuers. All you have to do is first call `.features` to notify the model you want to receive the features, and then pass an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "enc = ResNetEncoder()\n",
    "enc.features\n",
    "enc(torch.randn((1,3,224,224)))\n",
    "print([f.shape for f in enc.features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember** each model has always a `.decoder` field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glasses.nn.models import ResNet\n",
    "\n",
    "model = ResNet.resnet18()\n",
    "model.encoder.widths[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder knows the number of output features, you can access them by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features\n",
    "\n",
    "Each encoder can return a list of features accessable by the `.features` field. You need to call it once before in order to notify the encoder we wish to also store the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glasses.nn.models.classification.resnet import ResNetEncoder\n",
    "\n",
    "x = torch.randn(1,3,224,224)\n",
    "enc = ResNetEncoder()\n",
    "enc.features # call it once\n",
    "enc(x)\n",
    "features = enc.features # now we have all the features from each layer (stage)\n",
    "[print(f.shape) for f in features]\n",
    "# torch.Size([1, 64, 112, 112])\n",
    "# torch.Size([1, 64, 56, 56])\n",
    "# torch.Size([1, 128, 28, 28])\n",
    "# torch.Size([1, 256, 14, 14])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Head\n",
    "\n",
    "Head is the last part of the model, it usually perform the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glasses.nn.models.classification.resnet import ResNetHead\n",
    "\n",
    "\n",
    "ResNetHead(512, n_classes=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "The decoder takes the last feature from the `.encoder` and decode it. This is usually done in `segmentation` models, such as Unet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from glasses.nn.models.segmentation.unet import UNetDecoder\n",
    "x = torch.randn(1,3,224,224)\n",
    "enc = ResNetEncoder()\n",
    "enc.features # call it once\n",
    "x = enc(x)\n",
    "features = enc.features\n",
    "# we need to tell the decoder the first feature size and the size of the lateral features\n",
    "dec = UNetDecoder(start_features=enc.widths[-1],\n",
    "                  lateral_widths=enc.features_widths[::-1])\n",
    "out = dec(x, features[::-1])\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This object oriented structure allows to reuse most of the code across the models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "\n",
    "The models so far\n",
    "\n",
    "| name               | Parameters   |   Size (MB) |\n",
    "|:-------------------|:-------------|------------:|\n",
    "| resnet18           | 11,689,512   |       44.59 |\n",
    "| resnet26           | 15,995,176   |       61.02 |\n",
    "| resnet26d          | 16,014,408   |       61.09 |\n",
    "| resnet34           | 21,797,672   |       83.15 |\n",
    "| resnet50           | 25,557,032   |       97.49 |\n",
    "| resnet50d          | 25,576,264   |       97.57 |\n",
    "| resnet101          | 44,549,160   |      169.94 |\n",
    "| resnet152          | 60,192,808   |      229.62 |\n",
    "| resnet200          | 64,673,832   |      246.71 |\n",
    "| se_resnet18        | 11,776,552   |       44.92 |\n",
    "| se_resnet34        | 21,954,856   |       83.75 |\n",
    "| se_resnet50        | 28,071,976   |      107.09 |\n",
    "| se_resnet101       | 49,292,328   |      188.04 |\n",
    "| se_resnet152       | 66,770,984   |      254.71 |\n",
    "| cse_resnet18       | 11,778,592   |       44.93 |\n",
    "| cse_resnet34       | 21,958,868   |       83.77 |\n",
    "| cse_resnet50       | 28,088,024   |      107.15 |\n",
    "| cse_resnet101      | 49,326,872   |      188.17 |\n",
    "| cse_resnet152      | 66,821,848   |      254.91 |\n",
    "| resnext50_32x4d    | 25,028,904   |       95.48 |\n",
    "| resnext101_32x8d   | 88,791,336   |      338.71 |\n",
    "| resnext101_32x16d  | 194,026,792  |      740.15 |\n",
    "| resnext101_32x32d  | 468,530,472  |     1787.3  |\n",
    "| resnext101_32x48d  | 828,411,176  |     3160.14 |\n",
    "| wide_resnet50_2    | 68,883,240   |      262.77 |\n",
    "| wide_resnet101_2   | 126,886,696  |      484.03 |\n",
    "| densenet121        | 7,978,856    |       30.44 |\n",
    "| densenet169        | 14,149,480   |       53.98 |\n",
    "| densenet201        | 20,013,928   |       76.35 |\n",
    "| densenet161        | 28,681,000   |      109.41 |\n",
    "| fishnet99          | 16,630,312   |       63.44 |\n",
    "| fishnet150         | 24,960,808   |       95.22 |\n",
    "| vgg11              | 132,863,336  |      506.83 |\n",
    "| vgg13              | 133,047,848  |      507.54 |\n",
    "| vgg16              | 138,357,544  |      527.79 |\n",
    "| vgg19              | 143,667,240  |      548.05 |\n",
    "| vgg11_bn           | 132,868,840  |      506.85 |\n",
    "| vgg13_bn           | 133,053,736  |      507.56 |\n",
    "| vgg16_bn           | 138,365,992  |      527.82 |\n",
    "| vgg19_bn           | 143,678,248  |      548.09 |\n",
    "| efficientnet_b0    | 5,288,548    |       20.17 |\n",
    "| efficientnet_b1    | 7,794,184    |       29.73 |\n",
    "| efficientnet_b2    | 9,109,994    |       34.75 |\n",
    "| efficientnet_b3    | 12,233,232   |       46.67 |\n",
    "| efficientnet_b4    | 19,341,616   |       73.78 |\n",
    "| efficientnet_b5    | 30,389,784   |      115.93 |\n",
    "| efficientnet_b6    | 43,040,704   |      164.19 |\n",
    "| efficientnet_b7    | 66,347,960   |      253.1  |\n",
    "| efficientnet_b8    | 87,413,142   |      333.45 |\n",
    "| efficientnet_l2    | 480,309,308  |     1832.23 |\n",
    "| efficientnet_lite0 | 4,652,008    |       17.75 |\n",
    "| efficientnet_lite1 | 5,416,680    |       20.66 |\n",
    "| efficientnet_lite2 | 6,092,072    |       23.24 |\n",
    "| efficientnet_lite3 | 8,197,096    |       31.27 |\n",
    "| efficientnet_lite4 | 13,006,568   |       49.62 |\n",
    "| mobilenetv2        | 3,504,872    |       13.37 |\n",
    "| unet               | 23,202,530   |       88.51 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "\n",
    "Most of the weights were trained by other people and adapted to glasses. It is worth cite\n",
    "\n",
    "- [pytorch-image-models](https://github.com/rwightman/pytorch-image-models)\n",
    "- [torchvision](hhttps://github.com/pytorch/vision)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
