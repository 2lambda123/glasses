{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glasses.utils.PretrainedWeightsProvider import PretrainedWeightsProvider\n",
    "from glasses.models import AutoModel\n",
    "from torch import nn\n",
    "\n",
    "class Dummy(nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super().__init__(nn.Conv2d(3, 32, kernel_size=3), \n",
    "                        nn.Conv2d(32, 64, kernel_size=3))\n",
    "\n",
    "dummy = Dummy()\n",
    "        \n",
    "provider = PretrainedWeightsProvider()\n",
    "# state_dict = provider['dummy']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                Models                 </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Name                   </span>┃<span style=\"font-weight: bold\"> Pretrained </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "│ resnet18               │ true       │\n",
       "│ resnet26               │ true       │\n",
       "│ resnet26d              │ true       │\n",
       "│ resnet34               │ true       │\n",
       "│ resnet34d              │ true       │\n",
       "│ resnet50               │ true       │\n",
       "│ resnet50d              │ true       │\n",
       "│ resnet101              │ true       │\n",
       "│ resnet152              │ true       │\n",
       "│ resnet200              │ false      │\n",
       "│ se_resnet18            │ false      │\n",
       "│ se_resnet34            │ false      │\n",
       "│ se_resnet50            │ false      │\n",
       "│ se_resnet101           │ false      │\n",
       "│ se_resnet152           │ false      │\n",
       "│ cse_resnet18           │ false      │\n",
       "│ cse_resnet34           │ false      │\n",
       "│ cse_resnet50           │ true       │\n",
       "│ cse_resnet101          │ false      │\n",
       "│ cse_resnet152          │ false      │\n",
       "│ resnext50_32x4d        │ true       │\n",
       "│ resnext101_32x8d       │ true       │\n",
       "│ resnext101_32x16d      │ false      │\n",
       "│ resnext101_32x32d      │ false      │\n",
       "│ resnext101_32x48d      │ false      │\n",
       "│ regnetx_002            │ true       │\n",
       "│ regnetx_004            │ true       │\n",
       "│ regnetx_006            │ true       │\n",
       "│ regnetx_008            │ true       │\n",
       "│ regnetx_016            │ true       │\n",
       "│ regnetx_032            │ true       │\n",
       "│ regnety_002            │ true       │\n",
       "│ regnety_004            │ true       │\n",
       "│ regnety_006            │ true       │\n",
       "│ regnety_008            │ true       │\n",
       "│ regnety_016            │ true       │\n",
       "│ regnety_032            │ true       │\n",
       "│ resnest14d             │ false      │\n",
       "│ resnest26d             │ false      │\n",
       "│ resnest50d             │ false      │\n",
       "│ resnest50d_1s4x24d     │ false      │\n",
       "│ resnest50d_4s2x40d     │ false      │\n",
       "│ resnest101e            │ false      │\n",
       "│ resnest200e            │ false      │\n",
       "│ resnest269e            │ false      │\n",
       "│ wide_resnet50_2        │ true       │\n",
       "│ wide_resnet101_2       │ true       │\n",
       "│ densenet121            │ true       │\n",
       "│ densenet169            │ true       │\n",
       "│ densenet201            │ true       │\n",
       "│ densenet161            │ true       │\n",
       "│ fishnet99              │ false      │\n",
       "│ fishnet150             │ false      │\n",
       "│ vgg11                  │ true       │\n",
       "│ vgg13                  │ true       │\n",
       "│ vgg16                  │ true       │\n",
       "│ vgg19                  │ true       │\n",
       "│ vgg11_bn               │ true       │\n",
       "│ vgg13_bn               │ true       │\n",
       "│ vgg16_bn               │ true       │\n",
       "│ vgg19_bn               │ true       │\n",
       "│ efficientnet_b0        │ true       │\n",
       "│ efficientnet_b1        │ true       │\n",
       "│ efficientnet_b2        │ true       │\n",
       "│ efficientnet_b3        │ true       │\n",
       "│ efficientnet_b4        │ false      │\n",
       "│ efficientnet_b5        │ false      │\n",
       "│ efficientnet_b6        │ false      │\n",
       "│ efficientnet_b7        │ false      │\n",
       "│ efficientnet_b8        │ false      │\n",
       "│ efficientnet_l2        │ false      │\n",
       "│ efficientnet_lite0     │ false      │\n",
       "│ efficientnet_lite1     │ false      │\n",
       "│ efficientnet_lite2     │ false      │\n",
       "│ efficientnet_lite3     │ false      │\n",
       "│ efficientnet_lite4     │ false      │\n",
       "│ vit_small_patch16_224  │ false      │\n",
       "│ vit_base_patch16_224   │ false      │\n",
       "│ vit_base_patch16_384   │ false      │\n",
       "│ vit_base_patch32_384   │ false      │\n",
       "│ vit_huge_patch16_224   │ false      │\n",
       "│ vit_huge_patch32_384   │ false      │\n",
       "│ vit_large_patch16_224  │ false      │\n",
       "│ vit_large_patch16_384  │ false      │\n",
       "│ vit_large_patch32_384  │ false      │\n",
       "│ deit_tiny_patch16_224  │ true       │\n",
       "│ deit_small_patch16_224 │ true       │\n",
       "│ deit_base_patch16_224  │ false      │\n",
       "│ deit_base_patch16_384  │ false      │\n",
       "│ mobilenetv2            │ false      │\n",
       "│ unet                   │ false      │\n",
       "└────────────────────────┴────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.table.Table at 0x131274d90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AutoModel.models_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_state_dict = dummy.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ...,  True,  True,  True],\n",
       "         [False, False, False,  ...,  True,  True,  True],\n",
       "         [False, False, False,  ...,  True,  True,  True]],\n",
       "\n",
       "        [[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        [[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests \n",
    "import io\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from glasses.nn.regularization import DropBlock\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "res = requests.get('https://images.everyeye.it/img-notizie/the-mandalorian-baby-yoda-vittima-leak-stuntman-spoilerato-fa-v6-483983-1280x720.jpg')\n",
    "img = Image.open(io.BytesIO(res.content))\n",
    "x = Compose([Resize(256), ToTensor()])(img)\n",
    "# x = torch.ones((1, 3, 56, 56))\n",
    "\n",
    "out = DropBlock(p=0.2)(x)\n",
    "\n",
    "# fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,10))\n",
    "# fig.suptitle('DropBlock')\n",
    "# ax1.set_title('input')\n",
    "# ax1.imshow(np.array(img))\n",
    "# ax2.set_title('output')\n",
    "# ax2.imshow((out.squeeze() == 1).int().permute(1,2,0).numpy() * 255)\n",
    "# plt.show()\n",
    "\n",
    "# fig.savefig('test.png', dpi=300)\n",
    "\n",
    "out.squeeze() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RadixSoftmax(nn.Module):\n",
    "    def __init__(self, radix, cardinality):\n",
    "        super(RadixSoftmax, self).__init__()\n",
    "        self.radix = radix\n",
    "        self.cardinality = cardinality\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch = x.size(0)\n",
    "        print(f'4) {x.shape}')\n",
    "        if self.radix > 1:\n",
    "            x = x.view(batch, self.cardinality, self.radix, -1).transpose(1, 2)\n",
    "            print(f'5) {x.shape}')\n",
    "            x = F.softmax(x, dim=1)\n",
    "            x = x.reshape(batch, -1)\n",
    "            print(f'6) {x.shape}')\n",
    "        else:\n",
    "            x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SplitAttnConv2d(nn.Module):\n",
    "    \"\"\"Split-Attention Conv2d\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0,\n",
    "                 dilation=1, groups=1, bias=False, radix=2, reduction_factor=4,\n",
    "                 act_layer=nn.ReLU, norm_layer=nn.BatchNorm2d, drop_block=None, **kwargs):\n",
    "        super(SplitAttnConv2d, self).__init__()\n",
    "        self.radix = radix\n",
    "        self.drop_block = drop_block\n",
    "        mid_chs = out_channels * radix\n",
    "        print(mid_chs)\n",
    "        attn_chs = max(in_channels * radix // reduction_factor, 32)\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels, mid_chs, kernel_size, stride, padding, dilation,\n",
    "            groups=groups * radix, bias=bias, **kwargs)\n",
    "        self.bn0 = norm_layer(mid_chs) if norm_layer is not None else None\n",
    "        self.act0 = act_layer(inplace=True)\n",
    "        self.fc1 = nn.Conv2d(out_channels, attn_chs, 1, groups=groups)\n",
    "        self.bn1 = norm_layer(attn_chs) if norm_layer is not None else None\n",
    "        self.act1 = act_layer(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(attn_chs, mid_chs, 1, groups=groups)\n",
    "        self.rsoftmax = RadixSoftmax(radix, groups)\n",
    "\n",
    "    @property\n",
    "    def in_channels(self):\n",
    "        return self.conv.in_channels\n",
    "\n",
    "    @property\n",
    "    def out_channels(self):\n",
    "        return self.fc1.out_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn0 is not None:\n",
    "            x = self.bn0(x)\n",
    "        if self.drop_block is not None:\n",
    "            x = self.drop_block(x)\n",
    "        x = self.act0(x)\n",
    "\n",
    "        B, RC, H, W = x.shape\n",
    "        print(f'0) {x.shape}')\n",
    "        if self.radix > 1:\n",
    "            x = x.reshape((B, self.radix, RC // self.radix, H, W))\n",
    "            print(f'1) {x.shape}')\n",
    "            x_gap = x.sum(dim=1)\n",
    "            print(f'2) {x_gap.shape}')\n",
    "        else:\n",
    "            x_gap = x\n",
    "        x_gap = F.adaptive_avg_pool2d(x_gap, 1)\n",
    "        print(f'3) {x_gap.shape}')\n",
    "\n",
    "        x_gap = self.fc1(x_gap)\n",
    "        if self.bn1 is not None:\n",
    "            x_gap = self.bn1(x_gap)\n",
    "        x_gap = self.act1(x_gap)\n",
    "        x_attn = self.fc2(x_gap)\n",
    "\n",
    "        x_attn = self.rsoftmax(x_attn).view(B, -1, 1, 1)\n",
    "        print(f'x_attn: {x_attn.shape}')\n",
    "        if self.radix > 1:\n",
    "            x_attn = x_attn.reshape((B, self.radix, RC // self.radix, 1, 1))\n",
    "            print(f'7){x_attn.shape}')\n",
    "            print(f'x){x.shape}')\n",
    "            out = (x * x_attn).sum(dim=1)\n",
    "        else:\n",
    "            out = x * x_attn\n",
    "        return out.contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "SplitAttnConv2d(\n",
      "  (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
      "  (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (act0): ReLU(inplace=True)\n",
      "  (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (act1): ReLU(inplace=True)\n",
      "  (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (rsoftmax): RadixSoftmax()\n",
      ")\n",
      "0) torch.Size([3, 128, 28, 28])\n",
      "1) torch.Size([3, 2, 64, 28, 28])\n",
      "2) torch.Size([3, 64, 28, 28])\n",
      "3) torch.Size([3, 64, 1, 1])\n",
      "4) torch.Size([3, 128, 1, 1])\n",
      "5) torch.Size([3, 2, 1, 64])\n",
      "6) torch.Size([3, 128])\n",
      "x_attn: torch.Size([3, 128, 1, 1])\n",
      "7)torch.Size([3, 2, 64, 1, 1])\n",
      "x)torch.Size([3, 2, 64, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 28, 28])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AutoModelx = torch.randn((3, 64, 28, 28))\n",
    "\n",
    "split  = SplitAttnConv2d(64, 64, kernel_size=3, padding=1)\n",
    "print(split)\n",
    "split(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glasses.models.classification.resnest import SplitAtt, ResNeStBottleneckBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNeStBottleneckBlock(\n",
       "  (block): Sequential(\n",
       "    (0): ConvBnDropAct(\n",
       "      (conv): Conv2dPad(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (reg): DropBlock(p=0)\n",
       "      (act): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Identity()\n",
       "    (2): ConvBnDropAct(\n",
       "      (conv): Conv2dPad(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (reg): DropBlock(p=0)\n",
       "      (act): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): SplitAtt(\n",
       "      (att): Sequential(\n",
       "        (0): Reduce('b r (k c) h w-> b (k c) h w', 'sum', r=2, k=1)\n",
       "        (1): AdaptiveAvgPool2d(output_size=1)\n",
       "        (2): ConvBnAct(\n",
       "          (conv): Conv2dPad(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): Rearrange('b (r k c) h w -> b r k c h w', r=2, k=1)\n",
       "        (5): Softmax(dim=1)\n",
       "        (6): Rearrange('b r k c h w -> b r (k c) h w', r=2, k=1)\n",
       "      )\n",
       "    )\n",
       "    (4): Identity()\n",
       "    (5): ConvBnDropAct(\n",
       "      (conv): Conv2dPad(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (reg): DropBlock(p=0)\n",
       "      (act): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (shortcut): Identity()\n",
       "  (act): ReLU(inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ResNeStBottleneckBlock(64, 64, radix=2, groups=1, base_width=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SplitAtt(\n",
      "  (att): Sequential(\n",
      "    (0): Reduce('b r (k c) h w-> b (k c) h w', 'sum', r=2, k=1)\n",
      "    (1): AdaptiveAvgPool2d(output_size=1)\n",
      "    (2): ConvBnAct(\n",
      "      (conv): Conv2dPad(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (4): Rearrange('b (r k c) h w -> b r k c h w', r=2, k=1)\n",
      "    (5): Softmax(dim=1)\n",
      "    (6): Rearrange('b r k c h w -> b r (k c) h w', r=2, k=1)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 28, 28])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((3, 128, 28, 28))\n",
    "att = SplitAtt(64, features=32, radix=2, groups=1)\n",
    "print(att)\n",
    "att(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 112, 112]             864\n",
      "       BatchNorm2d-2         [-1, 32, 112, 112]              64\n",
      "              ReLU-3         [-1, 32, 112, 112]               0\n",
      "            Conv2d-4         [-1, 32, 112, 112]           9,216\n",
      "       BatchNorm2d-5         [-1, 32, 112, 112]              64\n",
      "              ReLU-6         [-1, 32, 112, 112]               0\n",
      "            Conv2d-7         [-1, 64, 112, 112]          18,432\n",
      "       BatchNorm2d-8         [-1, 64, 112, 112]             128\n",
      "              ReLU-9         [-1, 64, 112, 112]               0\n",
      "        MaxPool2d-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11           [-1, 64, 56, 56]           4,096\n",
      "      BatchNorm2d-12           [-1, 64, 56, 56]             128\n",
      "             ReLU-13           [-1, 64, 56, 56]               0\n",
      "           Conv2d-14          [-1, 128, 56, 56]          36,864\n",
      "      BatchNorm2d-15          [-1, 128, 56, 56]             256\n",
      "             ReLU-16          [-1, 128, 56, 56]               0\n",
      "           Conv2d-17             [-1, 32, 1, 1]           2,080\n",
      "      BatchNorm2d-18             [-1, 32, 1, 1]              64\n",
      "             ReLU-19             [-1, 32, 1, 1]               0\n",
      "           Conv2d-20            [-1, 128, 1, 1]           4,224\n",
      "     RadixSoftmax-21                  [-1, 128]               0\n",
      "  SplitAttnConv2d-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "         Identity-25           [-1, 64, 56, 56]               0\n",
      "           Conv2d-26          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-27          [-1, 256, 56, 56]             512\n",
      "             ReLU-28          [-1, 256, 56, 56]               0\n",
      "ResNestBottleneck-29          [-1, 256, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 128, 56, 56]          36,864\n",
      "      BatchNorm2d-34          [-1, 128, 56, 56]             256\n",
      "             ReLU-35          [-1, 128, 56, 56]               0\n",
      "           Conv2d-36             [-1, 32, 1, 1]           2,080\n",
      "      BatchNorm2d-37             [-1, 32, 1, 1]              64\n",
      "             ReLU-38             [-1, 32, 1, 1]               0\n",
      "           Conv2d-39            [-1, 128, 1, 1]           4,224\n",
      "     RadixSoftmax-40                  [-1, 128]               0\n",
      "  SplitAttnConv2d-41           [-1, 64, 56, 56]               0\n",
      "           Conv2d-42          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-43          [-1, 256, 56, 56]             512\n",
      "             ReLU-44          [-1, 256, 56, 56]               0\n",
      "ResNestBottleneck-45          [-1, 256, 56, 56]               0\n",
      "           Conv2d-46          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-47          [-1, 128, 56, 56]             256\n",
      "             ReLU-48          [-1, 128, 56, 56]               0\n",
      "           Conv2d-49          [-1, 256, 56, 56]         147,456\n",
      "      BatchNorm2d-50          [-1, 256, 56, 56]             512\n",
      "             ReLU-51          [-1, 256, 56, 56]               0\n",
      "           Conv2d-52             [-1, 64, 1, 1]           8,256\n",
      "      BatchNorm2d-53             [-1, 64, 1, 1]             128\n",
      "             ReLU-54             [-1, 64, 1, 1]               0\n",
      "           Conv2d-55            [-1, 256, 1, 1]          16,640\n",
      "     RadixSoftmax-56                  [-1, 256]               0\n",
      "  SplitAttnConv2d-57          [-1, 128, 56, 56]               0\n",
      "        AvgPool2d-58          [-1, 128, 28, 28]               0\n",
      "           Conv2d-59          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 512, 28, 28]           1,024\n",
      "        AvgPool2d-61          [-1, 256, 28, 28]               0\n",
      "           Conv2d-62          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-63          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-64          [-1, 512, 28, 28]               0\n",
      "ResNestBottleneck-65          [-1, 512, 28, 28]               0\n",
      "           Conv2d-66          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-67          [-1, 128, 28, 28]             256\n",
      "             ReLU-68          [-1, 128, 28, 28]               0\n",
      "           Conv2d-69          [-1, 256, 28, 28]         147,456\n",
      "      BatchNorm2d-70          [-1, 256, 28, 28]             512\n",
      "             ReLU-71          [-1, 256, 28, 28]               0\n",
      "           Conv2d-72             [-1, 64, 1, 1]           8,256\n",
      "      BatchNorm2d-73             [-1, 64, 1, 1]             128\n",
      "             ReLU-74             [-1, 64, 1, 1]               0\n",
      "           Conv2d-75            [-1, 256, 1, 1]          16,640\n",
      "     RadixSoftmax-76                  [-1, 256]               0\n",
      "  SplitAttnConv2d-77          [-1, 128, 28, 28]               0\n",
      "           Conv2d-78          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-79          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-80          [-1, 512, 28, 28]               0\n",
      "ResNestBottleneck-81          [-1, 512, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-83          [-1, 256, 28, 28]             512\n",
      "             ReLU-84          [-1, 256, 28, 28]               0\n",
      "           Conv2d-85          [-1, 512, 28, 28]         589,824\n",
      "      BatchNorm2d-86          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-87          [-1, 512, 28, 28]               0\n",
      "           Conv2d-88            [-1, 128, 1, 1]          32,896\n",
      "      BatchNorm2d-89            [-1, 128, 1, 1]             256\n",
      "             ReLU-90            [-1, 128, 1, 1]               0\n",
      "           Conv2d-91            [-1, 512, 1, 1]          66,048\n",
      "     RadixSoftmax-92                  [-1, 512]               0\n",
      "  SplitAttnConv2d-93          [-1, 256, 28, 28]               0\n",
      "        AvgPool2d-94          [-1, 256, 14, 14]               0\n",
      "           Conv2d-95         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-96         [-1, 1024, 14, 14]           2,048\n",
      "        AvgPool2d-97          [-1, 512, 14, 14]               0\n",
      "           Conv2d-98         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-99         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-100         [-1, 1024, 14, 14]               0\n",
      "ResNestBottleneck-101         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-102          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-103          [-1, 256, 14, 14]             512\n",
      "            ReLU-104          [-1, 256, 14, 14]               0\n",
      "          Conv2d-105          [-1, 512, 14, 14]         589,824\n",
      "     BatchNorm2d-106          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-107          [-1, 512, 14, 14]               0\n",
      "          Conv2d-108            [-1, 128, 1, 1]          32,896\n",
      "     BatchNorm2d-109            [-1, 128, 1, 1]             256\n",
      "            ReLU-110            [-1, 128, 1, 1]               0\n",
      "          Conv2d-111            [-1, 512, 1, 1]          66,048\n",
      "    RadixSoftmax-112                  [-1, 512]               0\n",
      " SplitAttnConv2d-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-115         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-116         [-1, 1024, 14, 14]               0\n",
      "ResNestBottleneck-117         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-118          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-119          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-120          [-1, 512, 14, 14]               0\n",
      "          Conv2d-121         [-1, 1024, 14, 14]       2,359,296\n",
      "     BatchNorm2d-122         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-123         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-124            [-1, 256, 1, 1]         131,328\n",
      "     BatchNorm2d-125            [-1, 256, 1, 1]             512\n",
      "            ReLU-126            [-1, 256, 1, 1]               0\n",
      "          Conv2d-127           [-1, 1024, 1, 1]         263,168\n",
      "    RadixSoftmax-128                 [-1, 1024]               0\n",
      " SplitAttnConv2d-129          [-1, 512, 14, 14]               0\n",
      "       AvgPool2d-130            [-1, 512, 7, 7]               0\n",
      "          Conv2d-131           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-132           [-1, 2048, 7, 7]           4,096\n",
      "       AvgPool2d-133           [-1, 1024, 7, 7]               0\n",
      "          Conv2d-134           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-135           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-136           [-1, 2048, 7, 7]               0\n",
      "ResNestBottleneck-137           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-138            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-139            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-140            [-1, 512, 7, 7]               0\n",
      "          Conv2d-141           [-1, 1024, 7, 7]       2,359,296\n",
      "     BatchNorm2d-142           [-1, 1024, 7, 7]           2,048\n",
      "            ReLU-143           [-1, 1024, 7, 7]               0\n",
      "          Conv2d-144            [-1, 256, 1, 1]         131,328\n",
      "     BatchNorm2d-145            [-1, 256, 1, 1]             512\n",
      "            ReLU-146            [-1, 256, 1, 1]               0\n",
      "          Conv2d-147           [-1, 1024, 1, 1]         263,168\n",
      "    RadixSoftmax-148                 [-1, 1024]               0\n",
      " SplitAttnConv2d-149            [-1, 512, 7, 7]               0\n",
      "          Conv2d-150           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-151           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-152           [-1, 2048, 7, 7]               0\n",
      "ResNestBottleneck-153           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-154           [-1, 2048, 1, 1]               0\n",
      "SelectAdaptivePool2d-155                 [-1, 2048]               0\n",
      "          Linear-156                 [-1, 1000]       2,049,000\n",
      "================================================================\n",
      "Total params: 17,069,448\n",
      "Trainable params: 17,069,448\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 256.41\n",
      "Params size (MB): 65.11\n",
      "Estimated Total Size (MB): 322.10\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "import timm\n",
    "model = timm.create_model('resnest26d')\n",
    "\n",
    "summary(model, (3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['resnest14d',\n",
       " 'resnest26d',\n",
       " 'resnest50d',\n",
       " 'resnest50d_1s4x24d',\n",
       " 'resnest50d_4s2x40d',\n",
       " 'resnest101e',\n",
       " 'resnest200e',\n",
       " 'resnest269e']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm.list_models('resnest*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([1, 64, 112, 112]), torch.Size([1, 256, 56, 56]), torch.Size([1, 512, 28, 28]), torch.Size([1, 1024, 14, 14])]\n"
     ]
    }
   ],
   "source": [
    "model = ResNetSt.resnest14d().eval()\n",
    "model.encoder.features\n",
    "model(torch.randn((1,3,224,224)))\n",
    "features = model.encoder.features\n",
    "print([x.shape for x in features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glasses.models import AutoModel, AutoConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViT(\n",
       "  (embedding): PatchEmbedding(\n",
       "    (projection): Sequential(\n",
       "      (0): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (1): Rearrange('b e (h) (w) -> b (h w) e')\n",
       "    )\n",
       "    (tokens): ViTTokens()\n",
       "  )\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderBlock(\n",
       "        (0): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): MultiHeadAttention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (att_drop): Dropout(p=0.0, inplace=False)\n",
       "              (projection): Sequential(\n",
       "                (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForwardBlock(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): TransformerEncoderBlock(\n",
       "        (0): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): MultiHeadAttention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (att_drop): Dropout(p=0.0, inplace=False)\n",
       "              (projection): Sequential(\n",
       "                (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForwardBlock(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): TransformerEncoderBlock(\n",
       "        (0): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): MultiHeadAttention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (att_drop): Dropout(p=0.0, inplace=False)\n",
       "              (projection): Sequential(\n",
       "                (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForwardBlock(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): TransformerEncoderBlock(\n",
       "        (0): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): MultiHeadAttention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (att_drop): Dropout(p=0.0, inplace=False)\n",
       "              (projection): Sequential(\n",
       "                (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForwardBlock(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): TransformerEncoderBlock(\n",
       "        (0): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): MultiHeadAttention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (att_drop): Dropout(p=0.0, inplace=False)\n",
       "              (projection): Sequential(\n",
       "                (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForwardBlock(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): TransformerEncoderBlock(\n",
       "        (0): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): MultiHeadAttention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (att_drop): Dropout(p=0.0, inplace=False)\n",
       "              (projection): Sequential(\n",
       "                (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForwardBlock(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): TransformerEncoderBlock(\n",
       "        (0): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): MultiHeadAttention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (att_drop): Dropout(p=0.0, inplace=False)\n",
       "              (projection): Sequential(\n",
       "                (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForwardBlock(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): TransformerEncoderBlock(\n",
       "        (0): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): MultiHeadAttention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (att_drop): Dropout(p=0.0, inplace=False)\n",
       "              (projection): Sequential(\n",
       "                (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForwardBlock(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): TransformerEncoderBlock(\n",
       "        (0): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): MultiHeadAttention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (att_drop): Dropout(p=0.0, inplace=False)\n",
       "              (projection): Sequential(\n",
       "                (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForwardBlock(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): TransformerEncoderBlock(\n",
       "        (0): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): MultiHeadAttention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (att_drop): Dropout(p=0.0, inplace=False)\n",
       "              (projection): Sequential(\n",
       "                (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForwardBlock(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): TransformerEncoderBlock(\n",
       "        (0): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): MultiHeadAttention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (att_drop): Dropout(p=0.0, inplace=False)\n",
       "              (projection): Sequential(\n",
       "                (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForwardBlock(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): TransformerEncoderBlock(\n",
       "        (0): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): MultiHeadAttention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (att_drop): Dropout(p=0.0, inplace=False)\n",
       "              (projection): Sequential(\n",
       "                (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForwardBlock(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): ViTClassificationHead(\n",
       "    (pool): Lambda()\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (fc): Linear(in_features=768, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModel.from_name('vit_base_patch16_224')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[ 1.8139e+00,  4.3294e-01, -1.0594e+00,  1.2178e+00, -5.1684e-01,\n",
       "          -5.7020e-01, -1.6492e+00,  5.9889e-01, -1.4083e+00, -1.7492e+00,\n",
       "           1.4628e+00,  2.0315e+00,  3.5535e-02, -1.5276e+00, -3.4329e-01,\n",
       "          -9.6625e-01,  1.4201e+00, -2.9760e-01, -1.1336e+00,  6.0696e-01,\n",
       "           2.8885e-01,  5.0535e-02,  1.6611e+00, -2.4462e-01,  1.3862e+00,\n",
       "          -8.6118e-01,  4.1896e-01,  1.0207e+00, -4.9626e-01,  1.4439e+00,\n",
       "           3.5824e-01, -7.6022e-01,  9.1688e-02, -1.4409e+00, -6.7320e-01,\n",
       "          -3.2345e-01, -3.0303e-01, -1.9116e-01,  1.6442e-01, -9.5486e-01,\n",
       "          -2.4387e-01, -3.6958e-01, -1.4860e+00, -5.9129e-01, -7.7090e-01,\n",
       "           6.2197e-01,  1.2709e+00,  3.5272e-01, -1.2842e+00, -4.1972e-01,\n",
       "           2.7664e-01,  1.0724e+00,  7.4461e-01,  1.2605e+00,  1.4620e+00,\n",
       "          -1.8566e+00,  1.3715e+00,  6.6508e-01,  8.6190e-01,  1.2123e+00,\n",
       "          -5.7741e-01,  4.9280e-01, -7.3030e-03, -1.4707e+00, -3.5965e-01,\n",
       "          -9.7532e-01,  9.6502e-01, -7.3845e-01, -5.2182e-02, -1.0651e+00,\n",
       "           7.7002e-01, -2.4219e-01, -1.7283e-01,  1.7869e-01, -6.0344e-01,\n",
       "           3.4246e-02,  8.2835e-03, -4.0392e-01, -9.9744e-02, -6.0269e-01,\n",
       "           9.5327e-01, -7.1028e-01,  8.1939e-01, -2.4603e+00,  2.8454e-01,\n",
       "          -1.6007e+00, -3.5960e-01, -7.0993e-01, -1.5762e+00, -1.2527e-01,\n",
       "          -2.5027e-01,  9.6097e-01, -1.1339e+00,  1.8623e+00,  1.2793e-01,\n",
       "           4.2821e-01,  9.9042e-03, -6.5567e-01,  7.1452e-01, -4.8957e-01,\n",
       "           1.0479e+00,  8.6139e-01,  1.1277e+00,  3.9987e-01,  1.1984e+00,\n",
       "          -8.8133e-01, -8.3950e-01, -7.9492e-01,  1.0876e+00,  1.2356e-01,\n",
       "           1.1891e-01, -5.4077e-01,  1.3196e+00, -1.8241e+00, -2.0228e-01,\n",
       "          -8.4910e-01, -2.3588e-01, -2.8037e-01, -1.5393e+00,  9.2047e-01,\n",
       "           3.3985e-01,  8.1749e-01,  1.9358e-01,  2.2779e-01, -9.3421e-01,\n",
       "          -1.1794e+00,  3.7513e-01,  5.8304e-02,  6.7013e-01,  1.2662e+00,\n",
       "          -1.5332e+00,  1.8927e+00,  8.5979e-01, -1.1226e+00, -5.2721e-01,\n",
       "           3.3367e-01,  7.2250e-01,  1.4823e+00, -1.4246e+00, -3.4319e-01,\n",
       "           7.8573e-01, -1.5112e-01, -1.1006e+00,  1.6856e+00, -1.1037e-01,\n",
       "          -7.7116e-02, -8.1090e-01,  6.4895e-01, -3.2494e-01,  7.4978e-01,\n",
       "          -1.3419e+00, -1.8421e+00, -6.0228e-01,  3.8926e-01, -1.1135e+00,\n",
       "           3.5194e-01, -7.0691e-01, -7.5198e-01, -1.9428e-01,  2.6040e-01,\n",
       "          -1.4240e+00, -5.9828e-01,  2.8585e-02, -2.7695e-01, -7.1065e-01,\n",
       "          -4.0663e-01,  1.7765e-01, -5.7728e-01,  1.7325e+00, -4.1757e-01,\n",
       "           1.3586e-01, -9.8247e-02, -7.4801e-01, -2.6601e-01,  8.0842e-01,\n",
       "           1.6504e+00, -1.1276e+00,  1.1520e+00,  2.8023e-02,  1.9064e-01,\n",
       "           1.5528e+00, -5.4741e-01, -1.3614e+00, -1.7071e+00,  8.8002e-01,\n",
       "          -1.9354e+00, -1.5750e-01, -5.1527e-01,  8.6794e-01,  6.8593e-01,\n",
       "           4.4949e-01, -3.1952e-01,  5.2007e-01,  3.8316e-01, -1.1336e+00,\n",
       "           1.2628e+00, -8.7897e-01, -1.2157e+00, -9.2394e-01,  7.7582e-01,\n",
       "           4.8456e-01,  8.3730e-01,  5.7589e-01, -3.1768e-01, -1.9758e+00,\n",
       "          -4.0297e-01,  1.7344e+00,  2.5674e-01,  3.8863e-01, -9.5818e-01,\n",
       "          -7.3792e-01,  1.7944e-01, -9.0241e-01, -1.6489e+00,  5.8960e-01,\n",
       "           4.8865e-01, -3.3306e-01, -7.5672e-01,  1.0007e+00, -6.1977e-01,\n",
       "          -5.1976e-01,  1.2950e+00,  1.5028e-02, -7.2850e-03,  5.2445e-01,\n",
       "           1.9069e+00, -1.1955e-01, -4.7955e-01,  2.4032e-01,  1.2011e-01,\n",
       "           1.1277e+00, -2.6080e-01,  8.6061e-01,  1.5208e+00,  4.5283e-01,\n",
       "           1.1622e-01, -1.0019e+00, -1.3483e+00, -2.1923e+00,  3.6744e-01,\n",
       "          -2.5892e-01,  4.3945e-01, -1.5654e+00, -4.0145e-05, -5.6016e-01,\n",
       "          -6.1407e-01, -1.3604e+00, -5.6048e-01,  2.0058e-01, -9.9371e-01,\n",
       "          -1.0561e+00, -8.1898e-02, -1.2517e+00, -1.2831e-01, -1.1359e+00,\n",
       "           2.9246e-01,  1.2026e-01, -1.7159e+00, -1.0812e+00, -1.5715e+00,\n",
       "           1.1611e+00,  3.5039e-01,  2.5807e-01, -3.0682e-01, -3.0024e-02,\n",
       "           3.2150e-01, -6.6055e-01, -1.7877e-01, -1.4778e+00,  2.2065e-01,\n",
       "           5.5313e-01, -1.1950e+00,  5.6818e-01,  1.6241e+00, -2.2526e+00,\n",
       "          -5.6429e-01, -6.7947e-01, -8.4809e-01,  9.7740e-03,  2.5944e-01,\n",
       "           9.2140e-01,  1.2018e+00,  8.7465e-01,  9.5847e-01,  1.2487e-01,\n",
       "           7.5351e-01, -2.0497e+00, -9.4742e-01,  2.0433e-01, -1.4094e+00,\n",
       "           2.6259e-01,  7.1495e-01,  1.6682e-01, -6.4356e-01, -3.5195e-01,\n",
       "          -4.2110e-01, -1.4052e-01,  2.6902e+00, -1.1871e-01,  1.0092e+00,\n",
       "          -6.3060e-01,  8.4616e-01, -4.7427e-01,  3.0174e-01,  1.3922e+00,\n",
       "          -1.5354e+00, -6.3307e-01, -1.0713e+00, -4.1496e-02,  4.4456e-01,\n",
       "           2.4606e-01, -7.6533e-01, -1.3810e+00, -2.3894e-01, -8.0713e-01,\n",
       "           1.4871e+00, -7.4243e-02, -1.3436e+00,  1.7816e+00, -9.0246e-01,\n",
       "          -1.3241e+00,  2.6555e+00, -9.4155e-01, -5.1292e-01, -3.6134e-01,\n",
       "           2.1396e-01, -1.8242e-01,  2.5181e-01,  3.5961e-01,  2.4050e+00,\n",
       "           3.9493e-01, -4.1120e-01, -7.8758e-01,  1.8307e+00, -1.3637e+00,\n",
       "           5.6286e-01, -1.0045e+00,  1.3972e+00,  5.3250e-02,  2.4936e+00,\n",
       "          -1.4357e+00,  1.2059e+00,  1.1612e+00, -7.4039e-01, -5.0588e-01,\n",
       "          -1.5385e+00,  6.4697e-01, -4.4997e-01,  6.9075e-01,  1.2588e+00,\n",
       "          -8.4130e-02,  1.6993e-02, -4.1838e-01, -1.6830e+00, -7.3915e-01,\n",
       "           1.9481e-01, -2.5074e+00,  7.9205e-01,  1.1342e-01, -2.0043e-01,\n",
       "          -1.0205e+00,  4.6587e-01,  4.1214e-01, -2.7895e-01,  1.3618e+00,\n",
       "           5.6626e-04, -2.3921e-02, -1.6859e+00,  3.2281e-01,  6.7527e-01,\n",
       "          -5.2811e-01,  2.0922e+00,  6.8540e-02, -5.2127e-01,  2.1176e-01,\n",
       "          -9.4636e-02,  4.3281e-01,  1.1103e+00, -3.4521e-01,  9.8545e-01,\n",
       "          -1.2207e+00, -1.2327e+00,  5.4376e-01,  5.0453e-02, -1.3847e-01,\n",
       "          -3.6396e-01,  4.5269e-01,  2.8164e-01, -2.9254e-01,  3.9688e-01,\n",
       "          -1.2859e+00, -1.3584e+00, -2.3656e-01,  7.5646e-01, -2.8164e-01,\n",
       "           1.2068e+00, -1.7132e+00,  4.9443e-01, -2.8184e-01, -4.7021e-01,\n",
       "          -6.1919e-01, -3.5209e-01, -9.1010e-01, -7.4143e-01, -3.4028e-01,\n",
       "          -8.9371e-01,  3.7828e-01,  1.0548e+00, -1.3850e+00, -7.3786e-01,\n",
       "          -2.4733e+00,  2.3741e+00,  1.6238e-02, -1.6664e+00, -6.8438e-01,\n",
       "          -9.2909e-01,  8.8043e-01, -7.8072e-01,  1.8441e+00, -2.2159e+00,\n",
       "          -1.2909e+00,  1.8808e+00, -2.9632e-01, -1.7207e+00,  1.7091e-02,\n",
       "           1.9864e+00, -3.1108e-03,  1.3257e+00,  3.9873e-01, -1.4871e+00,\n",
       "           1.2624e+00, -2.0672e+00,  8.8845e-01, -1.1345e-01,  6.2069e-01,\n",
       "           7.5225e-01, -1.1004e+00, -1.0783e-01,  2.4847e+00,  4.1249e-01,\n",
       "          -4.4823e-01, -1.4457e-01,  3.8699e-02,  1.2030e+00,  8.1285e-03,\n",
       "           9.4681e-01, -5.8134e-01, -9.4354e-01,  2.6675e-01,  7.8443e-01,\n",
       "           5.3984e-01, -1.4697e-01,  1.0134e+00,  9.9850e-01, -4.9514e-01,\n",
       "          -9.4602e-01, -1.3186e-01,  1.5135e+00,  8.8586e-01,  4.8937e-01,\n",
       "           6.1704e-01,  3.0228e-01, -1.8607e+00, -7.6390e-01,  2.1057e+00,\n",
       "          -1.9414e+00, -1.2548e-01,  8.1441e-01, -2.3524e-01,  1.0483e-01,\n",
       "           1.6265e+00,  5.2613e-01,  2.9935e-01, -1.1567e+00, -4.0606e-01,\n",
       "           3.3555e-01, -1.0478e-02,  5.6365e-02,  2.5648e-01, -2.7876e-01,\n",
       "          -1.0829e+00, -3.5439e-01,  2.1701e+00,  1.4732e+00,  4.1472e-01,\n",
       "          -9.8305e-01, -8.1625e-01,  2.1374e-02,  2.6820e+00, -7.9978e-01,\n",
       "          -7.6448e-01,  2.4099e-03, -7.1248e-01, -1.3124e-01,  5.2265e-04,\n",
       "           4.6092e-02,  8.0950e-01, -7.4676e-01, -1.2142e+00,  1.9331e+00,\n",
       "           3.3342e-01, -1.4238e-01,  1.0906e+00,  1.7110e+00,  1.3669e+00,\n",
       "           5.9872e-02, -1.5639e+00, -1.0310e+00, -1.5129e-02,  1.4192e-01,\n",
       "          -8.7688e-01, -1.4869e-01,  3.5687e-01, -3.5269e-01, -1.6610e+00,\n",
       "           1.3827e+00,  1.7624e+00, -1.8775e+00, -7.7172e-01,  4.1359e-01,\n",
       "          -6.4904e-01, -1.1960e+00,  6.5401e-01,  3.6539e-01, -1.3464e+00,\n",
       "          -6.5826e-02,  6.6850e-01,  2.4414e-01, -1.0469e+00, -2.2585e-01,\n",
       "          -8.8768e-01,  7.5862e-02,  1.9879e-01, -1.5189e-02, -1.1154e+00,\n",
       "           4.8317e-01, -1.9429e-01,  8.9911e-02, -6.1698e-01,  4.1345e-01,\n",
       "           8.7724e-01, -4.6840e-01, -3.5062e-01,  2.1093e-01, -1.6385e+00,\n",
       "          -6.0431e-01, -9.8147e-01,  1.9544e+00,  1.0621e+00,  6.2200e-01,\n",
       "          -1.4383e+00,  1.3238e+00, -1.8581e+00, -8.9725e-01, -7.5292e-01,\n",
       "           1.2503e-01,  4.0369e-01,  1.5623e+00, -1.2881e+00,  1.5149e+00,\n",
       "          -1.6124e+00, -1.0436e+00, -3.1555e-01, -1.4651e+00,  1.0181e+00,\n",
       "          -5.2547e-01, -1.4923e+00,  9.2972e-01,  8.5572e-01,  4.2014e-01,\n",
       "           1.0013e+00,  6.4428e-02, -4.6975e-01,  5.0172e-01,  9.1108e-01,\n",
       "           3.0970e-01,  7.7993e-01,  7.2423e-01,  5.6993e-01,  1.2027e+00,\n",
       "          -8.0715e-02, -8.9639e-02, -4.1844e-01, -7.2892e-01, -9.9343e-01,\n",
       "          -1.1141e+00, -6.9668e-01,  2.2278e-01,  1.4188e+00,  1.1195e+00,\n",
       "           1.4805e+00,  1.5488e+00,  3.6675e-01, -1.0988e+00,  3.6621e-01,\n",
       "          -3.1378e-01,  8.1073e-01, -1.3987e+00, -6.2584e-01, -5.6322e-01,\n",
       "          -2.2377e-01,  1.6320e+00, -5.0669e-01, -1.2759e-02, -1.0923e+00,\n",
       "           1.0796e-01,  1.1017e+00,  1.0094e-01, -2.0626e+00, -1.9511e+00,\n",
       "          -3.6331e-01, -9.8718e-01, -1.6294e+00,  6.6120e-01,  2.9916e-01,\n",
       "           2.0023e+00, -4.4901e-01,  1.0463e+00,  1.8793e+00,  3.3678e-02,\n",
       "          -8.8203e-01,  3.0586e-01, -2.8919e-01, -1.7835e+00,  1.0072e+00,\n",
       "           1.1796e-01, -8.6468e-01, -1.5331e+00,  1.1263e+00, -1.3535e+00,\n",
       "          -5.2784e-01, -1.3647e+00, -1.2787e+00, -4.6816e-01, -1.3793e+00,\n",
       "           1.5055e+00,  1.5106e-01, -1.8549e-01,  2.0392e-01, -2.1868e-01,\n",
       "          -7.9336e-01, -1.8347e+00, -9.3172e-01,  1.2911e-01,  7.9684e-01,\n",
       "          -7.9324e-01, -1.9830e+00,  1.9965e-01, -4.1109e-01,  2.2402e-02,\n",
       "           1.0457e+00,  4.1241e-01, -7.7377e-02,  4.0786e-01, -1.1693e+00,\n",
       "           5.2776e-01,  7.4219e-02, -1.5488e-01, -2.1875e-02, -3.8023e-01,\n",
       "           3.9195e-01, -5.6421e-01, -2.4272e-01, -1.9468e-01, -1.8827e-01,\n",
       "           6.5694e-01,  2.0433e-01, -1.4452e+00, -1.3756e+00,  1.2735e+00,\n",
       "           5.4322e-01,  1.7863e-01, -5.4463e-02,  4.4415e-01,  4.3527e-01,\n",
       "          -8.6284e-01,  9.2688e-01,  5.4466e-01, -5.9036e-01, -1.8335e+00,\n",
       "           2.0670e+00, -2.7076e-02, -1.7330e+00, -7.7118e-01,  2.9961e-01,\n",
       "           8.1937e-01,  6.8908e-01, -7.1240e-01, -7.5901e-01, -7.9002e-01,\n",
       "           1.0692e-01,  4.8137e-01,  5.7419e-01,  4.9611e-01, -1.3505e+00,\n",
       "           1.2730e+00,  5.9790e-01, -1.3443e+00,  9.8331e-02,  7.6046e-01,\n",
       "          -1.4230e-01,  5.7052e-01, -9.8907e-01,  7.4453e-02, -9.0099e-01,\n",
       "           7.6232e-01,  2.2041e-01, -1.6854e+00,  2.0269e-01,  9.4686e-01,\n",
       "          -2.2273e-02, -3.3942e-01, -4.4318e-01, -2.2134e+00,  2.1631e-01,\n",
       "          -1.1257e+00,  1.7304e-01, -1.0662e+00, -1.7506e+00, -1.4910e-01,\n",
       "           2.1760e+00,  1.8102e+00,  2.0101e+00,  8.1423e-01,  5.2824e-02,\n",
       "          -6.9687e-01,  6.7787e-01, -1.3274e+00,  6.3288e-01, -1.5475e+00,\n",
       "          -5.2179e-01, -7.0537e-02,  2.6624e-01, -8.4764e-01, -5.7337e-01,\n",
       "           1.1278e+00,  1.2797e+00,  1.8392e+00, -1.0623e+00,  9.2908e-01,\n",
       "          -1.2245e+00,  9.3030e-01,  4.3585e-01, -5.5576e-01, -1.5871e-01,\n",
       "          -7.5421e-01, -7.8206e-01,  6.0620e-01,  1.7566e+00,  1.1185e+00,\n",
       "           1.0922e+00, -3.2242e-01,  2.4087e+00,  1.1046e+00, -2.8995e-01,\n",
       "           6.9049e-01,  2.6876e-02,  1.1677e+00, -3.1092e-01, -9.7719e-01,\n",
       "           2.7456e-01,  1.9422e+00, -5.3468e-01,  6.3199e-01,  8.6037e-02,\n",
       "          -4.9941e-01,  8.9881e-01,  1.7705e+00]]], requires_grad=True)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glasses.models.classification.deit import DeiT\n",
    "DeiT().embedding.tokens.dist_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/zuppif/.cache/torch/hub/facebookresearch_deit_main\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/deit/deit_base_distilled_patch16_224-df68dfff.pth\" to /home/zuppif/.cache/torch/hub/checkpoints/deit_base_distilled_patch16_224-df68dfff.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ea5da9b3134041854a26537f49e51b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=349402210.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model = torch.hub.load('facebookresearch/deit:main', 'deit_base_distilled_patch16_224', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilledVisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       "  (head_dist): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 197, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0454, -0.5833,  0.2980,  ..., -0.5219, -0.5729, -0.6744],\n",
       "        [-0.0938, -0.6276,  0.3209,  ..., -0.5340, -0.5642, -0.7091]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ViT()(torch.randn((2,3,224,224)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([2, 1, 768])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0454, -0.0183,  0.6613,  ..., -0.3889,  0.4319,  0.7054],\n",
       "        [ 0.0427, -0.0224,  0.6951,  ..., -0.4088,  0.4498,  0.6888]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model(torch.randn((2,3,224,224)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(AutoModel.models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = AutoConfig.from_name('resnet18')\n",
    "cfg.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glasses.utils.PretrainedWeightsProvider import PretrainedWeightsProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PretrainedWeightsProvider.BASE_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of glasses.utils.ModuleTransfer failed: Traceback (most recent call last):\n",
      "  File \"/home/zuppif/anaconda3/envs/dl/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/zuppif/anaconda3/envs/dl/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 410, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/home/zuppif/anaconda3/envs/dl/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/zuppif/anaconda3/envs/dl/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 302, in update_class\n",
      "    if update_generic(old_obj, new_obj): continue\n",
      "  File \"/home/zuppif/anaconda3/envs/dl/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/zuppif/anaconda3/envs/dl/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 266, in update_function\n",
      "    setattr(old, name, getattr(new, name))\n",
      "ValueError: __init__() requires a code object with 0 free vars, not 3\n",
      "]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument of type 'NoneType' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-64818d56c52a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vit_base_patch16_224'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest_skip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mViTTokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/torcheyes/transfer_weights.py\u001b[0m in \u001b[0;36mclone_model\u001b[0;34m(src, dst, x, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0mModuleTransfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/torcheyes/glasses/utils/ModuleTransfer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0msrc_traced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTracker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparametrized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0msrc_traced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_skip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_traced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mdest_traced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdest_skip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest_traced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/torcheyes/glasses/utils/ModuleTransfer.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0msrc_traced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTracker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparametrized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0msrc_traced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_skip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_traced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mdest_traced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdest_skip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest_traced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: argument of type 'NoneType' is not iterable"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from glasses.models import AutoModel, AutoConfig, EfficientNetLite\n",
    "import timm\n",
    "from transfer_weights import clone_model\n",
    "from benchmark import benchmark\n",
    "from glasses.models.classification.vit import ViTTokens\n",
    "\n",
    "src = timm.create_model('vit_base_patch16_224', pretrained='True')\n",
    "dst = AutoModel.from_name('vit_base_patch16_224')\n",
    "transform = AutoConfig.from_name('vit_base_patch16_224').transform\n",
    "\n",
    "dst = clone_model(src, dst, torch.randn((1, 3, 224, 224)), dest_skip = [ViTTokens]).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./imagenet_val_targets.pkl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2228f36321a4be6ad79b4b133ce2293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=391.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark(src.cuda().eval(), transform, batch_size=128, fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./imagenet_val_targets.pkl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "930963a279c340e6ae767791405ab118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=391.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dst.embedding.positions.data.copy_(src.pos_embed.data.squeeze(0))\n",
    "dst.embedding.tokens.cls.data.copy_(src.cls_token.data)\n",
    "benchmark(dst.cuda().eval(), transform, batch_size=128, fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary(timm.create_model('vit_base_patch16_224', pretrained=True), (3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst.embedding.positions.data.copy_(src.pos_embed.data.squeeze(0))\n",
    "dst.embedding.cls_token.data.copy_(src.cls_token.data)\n",
    "\n",
    "transform = AutoConfig.from_name('vit_base_patch16_384').transform\n",
    "\n",
    "benchmark(dst.cuda().eval(), transform, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "from glasses.models.classification.vit import ViT\n",
    "\n",
    "summary(VisionTransformer(), (3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dst.training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "root = Path('/home/zuppif/Documents/glasses-weights/')\n",
    "\n",
    "models = map(lambda x: x.stem, root.glob('*.pth'))\n",
    "\n",
    "for model in models:\n",
    "    print(f\"'{model}': BasicUrlHandler('https://github.com/FrancescoSaverioZuppichini/glasses-weights/blob/main/{model}.pth?raw=true'),\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glasses.utils.Tracker import Tracker\n",
    "import torch\n",
    "\n",
    "def trace(model, x):\n",
    "    return Tracker(model)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((1, 3, 224, 224))\n",
    "\n",
    "trace(ViT(), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((1, 3, 224, 224))\n",
    "\n",
    "trace(timm.create_model('vit_base_patch16_224', pretrained=True), x).traced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace(timm.create_model('vit_base_patch16_224', pretrained=True), x).parametrized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ViT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timm.create_model('vit_base_patch16_224', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
