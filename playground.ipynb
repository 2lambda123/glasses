{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "b = [1,2,3]\n",
    "\n",
    "a == b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('cls_token',\n",
       "              tensor([[[ 0.5900, -1.7115, -0.3799, -1.1737, -1.3522,  1.2476, -0.1131,\n",
       "                         0.5263, -0.0913, -0.8882, -0.1633,  1.1767,  0.5236, -0.8277,\n",
       "                         0.7619,  1.1423, -0.0305,  0.1122,  0.0198,  0.6153,  1.8497,\n",
       "                         0.8101,  1.3175,  1.2026, -0.6901, -0.6397, -0.4409,  0.4504,\n",
       "                        -0.2047, -0.0272,  0.7718, -2.3402,  0.1700, -1.4384, -0.1587,\n",
       "                         0.9391,  0.6130, -0.3367,  0.6688, -0.0990, -0.3199, -1.6032,\n",
       "                         1.9512, -0.5551, -1.2153,  1.6378,  0.7767,  0.3395,  0.0610,\n",
       "                         1.1436,  0.5334, -0.4963,  0.3517, -0.1968,  0.4588,  1.2061,\n",
       "                        -0.3842, -0.0418,  0.8565,  0.4937, -1.9154, -0.6947,  0.0486,\n",
       "                         0.7313, -0.8155, -0.2624, -0.1799,  2.5823, -0.0116, -0.2077,\n",
       "                         0.1386, -0.2878,  0.4487, -0.2127,  0.9555, -0.4534,  1.2566,\n",
       "                         0.1608,  0.7989,  1.1259,  0.9523,  0.6163, -0.2137, -0.3427,\n",
       "                         0.3244,  0.3285,  0.9778,  0.7491, -2.9658,  1.3189, -1.1533,\n",
       "                        -0.6337,  0.1467,  0.2722,  1.4966, -1.3864,  0.4918,  0.8998,\n",
       "                        -1.7909,  0.2149, -0.6663,  0.4729, -0.5913, -1.7897, -1.0988,\n",
       "                        -1.2566,  0.0889, -1.3075,  1.0121,  0.1042, -0.4709, -1.7771,\n",
       "                        -0.3910, -0.0158, -0.3701, -0.5351,  0.1801,  1.2604,  0.7713,\n",
       "                         0.0401, -1.1478, -1.8307,  0.6574, -0.5695, -1.4702,  0.6011,\n",
       "                         0.2679, -1.3444, -0.9940,  1.1192, -0.8508,  0.1102, -0.8910,\n",
       "                         0.1657,  0.8988, -0.5370, -1.1270, -0.6908,  0.4624, -0.6563,\n",
       "                        -0.1566, -0.3216, -0.0509, -0.4939,  0.8380,  0.0441,  1.4637,\n",
       "                         1.5111,  0.5156, -0.7965, -2.5633,  1.6244, -0.2415,  0.1478,\n",
       "                         0.3384, -0.6948,  0.9348, -1.6295,  0.6068,  0.7557, -0.8980,\n",
       "                         0.4122,  0.3925, -0.2510,  0.4141, -0.6373,  0.4615,  0.6512,\n",
       "                         0.5775, -1.7123,  2.0676, -0.5649, -1.6799, -1.0249, -0.6550,\n",
       "                         0.4336,  1.2309,  1.4078, -0.5798,  0.7183,  0.2499, -0.1482,\n",
       "                        -0.0540,  1.3402,  0.8100, -2.0016,  1.1523,  0.6139,  1.1416,\n",
       "                        -0.2798,  1.2278,  1.4741, -0.7897,  0.6280, -1.0526,  0.3279,\n",
       "                         0.6270,  0.0166,  0.2256,  0.1643,  1.4053, -0.7678, -1.2835,\n",
       "                         0.4268,  0.4262,  0.4924,  0.5024, -1.2419, -0.1338,  1.3752,\n",
       "                        -1.8384,  0.5506, -0.4786, -0.0428, -0.1771, -0.3298, -0.3478,\n",
       "                        -0.8887, -0.5261, -0.4952, -0.7339,  0.0870,  0.3693,  0.8479,\n",
       "                         0.3578, -0.0346,  0.9012,  0.8248,  0.7125, -3.0796,  0.4027,\n",
       "                        -0.9350,  0.3616, -1.4075,  0.4650, -1.5519, -1.0491, -1.7861,\n",
       "                        -1.2553, -0.5305,  0.2872,  0.4235,  1.6075, -0.6031, -0.7998,\n",
       "                        -0.0822, -0.0161,  0.5389, -0.7141, -1.4299,  0.0998, -1.4812,\n",
       "                         0.4224,  0.0921,  0.3531, -0.8969, -0.6601,  0.2369,  0.6333,\n",
       "                         1.1171,  0.2804,  0.7650, -2.5880,  0.2751,  0.0056,  2.0072,\n",
       "                         0.2356, -0.4912,  0.0358,  0.5460,  0.4817, -0.0330,  0.3960,\n",
       "                        -1.0382, -1.5498, -0.9010, -0.3401,  1.2023,  0.6620,  0.9998,\n",
       "                         0.1170,  0.2673, -1.4630, -1.6760,  0.0896,  1.8224,  1.8315,\n",
       "                        -0.1344, -1.3926,  1.0858,  0.6034,  0.2638,  1.1036,  0.4313,\n",
       "                        -0.6798,  1.3090, -1.0013,  0.1979, -0.3350, -0.8000, -1.0951,\n",
       "                        -0.5072,  0.5780, -0.7505, -1.2269, -0.4904, -0.8918,  1.0784,\n",
       "                         1.1015,  0.0521,  1.3632, -0.0137, -0.2207, -0.7442, -0.0844,\n",
       "                         0.1499,  1.2156, -1.4485, -0.1214, -0.2713, -0.2554, -0.1337,\n",
       "                         0.5558, -0.0833, -1.5752,  0.7147, -1.0402,  0.3612,  0.3440,\n",
       "                         1.7793,  0.4476, -1.0785,  0.7113, -0.2252, -0.3244,  0.4616,\n",
       "                        -1.9814,  0.3376,  0.7345,  0.7934,  0.8891, -1.5739,  0.0870,\n",
       "                        -0.4313, -0.2951,  0.5513, -0.1565, -0.5037, -1.3778,  0.9360,\n",
       "                        -0.3281, -0.5344, -0.2524, -0.3162, -0.9456,  0.3460, -1.0017,\n",
       "                        -1.3292,  0.2832, -0.7150, -0.6576, -1.5525, -0.9401,  0.8774,\n",
       "                         1.4886, -0.5561, -2.1336,  0.7190, -1.4932,  1.2115,  1.7732,\n",
       "                        -0.1405,  0.6708,  0.3852,  0.7014,  0.8813,  1.9944, -0.2517,\n",
       "                         0.6125,  1.3463, -0.5187, -0.4357, -0.0264,  0.3606, -0.0746,\n",
       "                         1.2546,  0.3814, -0.1979,  1.0666, -0.4510, -0.1758, -0.8931,\n",
       "                        -0.2939, -0.9800,  1.0780, -0.6304,  0.0513, -0.7592,  1.6329,\n",
       "                        -2.5094, -0.2550, -0.0919,  0.8479,  0.1849, -0.6429,  0.7382,\n",
       "                         1.5639,  0.6228,  0.2224, -0.3550, -0.1383,  0.5723,  0.4836,\n",
       "                         0.1884, -1.4051, -0.9649,  1.1755, -0.0874, -0.5216,  1.8137,\n",
       "                         0.0857,  0.5664, -1.0631,  1.5666,  0.2968,  1.8424, -0.8637,\n",
       "                         0.9615, -0.5386,  0.0207, -0.1422, -1.1336, -1.6548, -0.5835,\n",
       "                         0.4016, -0.4542, -0.5085, -1.2227, -1.9227,  0.0155, -1.7582,\n",
       "                         0.8306,  1.6266,  1.0401, -1.8576, -1.1138, -0.4233,  0.9640,\n",
       "                         1.1210, -0.7801,  0.8408, -0.0185, -0.2203,  0.6001, -0.4592,\n",
       "                        -0.6538, -1.6051,  0.6406,  0.1847, -0.7544,  2.5663,  2.2071,\n",
       "                         0.6151,  0.2205, -1.0536,  0.6284,  0.6618,  0.5503,  0.9398,\n",
       "                        -0.6534,  0.1142, -1.0714, -0.1180,  0.3872,  0.7015, -0.3496,\n",
       "                        -0.4677, -0.1805, -0.7381, -1.2968,  0.5699, -0.1754, -0.6077,\n",
       "                        -0.5681,  1.4940, -0.0377,  0.7334, -1.1054,  0.0117,  0.4051,\n",
       "                         1.0291, -0.8640,  1.7284, -0.0765,  1.0432,  0.4768,  0.1573,\n",
       "                        -0.2689, -0.2382, -3.3114,  1.1221,  0.4887,  1.3621,  0.6798,\n",
       "                        -0.2417, -0.6403, -0.4112, -0.1661, -1.0088, -0.8927,  2.2665,\n",
       "                        -0.1494, -0.8680, -0.1303, -0.8836,  0.0274,  0.6173,  1.4023,\n",
       "                        -0.9515,  0.5700,  1.1323,  0.2416, -1.1750, -0.4469,  0.0567,\n",
       "                        -0.2179, -1.2755, -0.6191, -0.0234,  0.5741,  0.4236,  0.2761,\n",
       "                        -0.0994,  0.0978,  0.6229,  0.9332, -1.2378, -1.3495,  1.5202,\n",
       "                         0.1675,  0.7471, -0.4621,  0.9077, -0.1056,  1.3586, -1.4219,\n",
       "                         1.0601, -1.1544,  0.6982, -0.7814, -1.7811, -2.2480,  0.8175,\n",
       "                        -1.2838, -0.1377,  1.0490,  1.1714,  1.4983,  0.9904,  0.4142,\n",
       "                         0.6738, -0.8141,  1.4031,  1.2068, -1.5113, -1.0172, -0.4072,\n",
       "                        -0.5326,  1.4034, -1.2077, -0.6689, -0.7940,  0.8642,  0.1614,\n",
       "                         0.9204,  1.0675,  0.5197, -1.4689, -1.5437, -0.2089,  0.2282,\n",
       "                         1.0002, -0.4924, -1.7499,  0.0165,  0.3180,  1.5488, -1.3272,\n",
       "                        -0.1834,  1.9468,  1.7843, -1.1960, -3.3472,  1.1859, -0.4308,\n",
       "                        -1.3183, -1.1765,  0.0628,  1.1233, -1.1889,  1.9856, -1.1440,\n",
       "                        -0.6679, -0.5926,  0.7542, -1.2211, -2.4353, -0.8881,  1.2054,\n",
       "                        -0.3222, -2.6715,  0.9684, -0.0482, -1.3415, -1.2767,  0.0428,\n",
       "                         0.9241, -1.9705,  2.5049,  0.2707,  0.6177,  1.9513,  0.8118,\n",
       "                        -0.3996, -1.0221,  2.1770,  0.3098,  1.1694, -0.3677,  0.6083,\n",
       "                        -0.6351, -1.8640, -0.3679, -0.4760,  0.7179, -1.1190,  0.1039,\n",
       "                        -2.7581, -0.6975,  1.4422,  1.1911,  0.6601, -2.2313,  1.0427,\n",
       "                        -0.1630,  0.3561, -0.9393,  0.8509, -0.4346,  1.4300, -0.5349,\n",
       "                         1.2946, -0.9191,  1.3123,  0.7672,  0.4750,  1.0259,  1.3476,\n",
       "                        -0.2964,  0.5067, -0.7566, -0.9315, -1.6814, -0.0342, -1.1455,\n",
       "                        -0.2325,  0.9702, -0.7289,  0.5113, -0.3480,  0.2623, -0.5071,\n",
       "                         1.3902, -1.1593,  0.5326,  0.4364, -2.1973, -0.6292,  0.4658,\n",
       "                        -1.9176,  0.0824,  0.6879, -0.3550, -1.4678, -0.5662,  0.1414,\n",
       "                         0.8291,  0.3402, -0.4281,  1.1248, -0.4163, -1.4798,  1.8328,\n",
       "                        -1.1867,  0.1528,  1.0155,  1.1223, -1.4605, -1.0663,  0.1362,\n",
       "                        -0.4639, -0.5444,  1.3076,  0.7213, -0.6872,  1.3532,  0.0750,\n",
       "                        -1.1217, -1.3404,  0.5059,  1.1270,  1.0734, -0.9984,  0.4528,\n",
       "                         0.3801,  0.0743,  0.6267,  0.4554, -1.6103,  0.0755, -0.8635,\n",
       "                        -0.9063,  0.6199,  2.0019,  1.4928,  2.0536,  1.4515, -0.8950,\n",
       "                         0.5440, -0.5134,  1.2701,  0.2523, -0.7744, -1.4002,  0.1490,\n",
       "                        -0.0896,  1.0664, -1.4380,  1.6443,  0.5875,  0.0035,  0.0970,\n",
       "                        -0.8251,  1.8884,  0.2824,  0.5141,  0.8754, -1.2197,  1.8791,\n",
       "                        -0.6869,  0.3634,  0.2125,  0.5454,  0.4563, -0.5905,  1.7984,\n",
       "                         2.2810, -0.5893, -0.0198, -0.0377,  1.5341,  1.5906,  0.7691,\n",
       "                        -0.9026, -1.4372, -0.4388,  1.7768, -0.9335]]])),\n",
       "             ('positions',\n",
       "              tensor([[-0.0703,  0.6077,  0.5972,  ...,  0.9972,  0.5693, -1.4851],\n",
       "                      [-1.7820,  2.4068,  0.5398,  ...,  0.5312,  0.9281,  0.2485],\n",
       "                      [ 0.2434, -1.3120, -0.2028,  ...,  0.6197, -0.5778, -1.3522],\n",
       "                      ...,\n",
       "                      [ 0.1505, -1.1126, -1.3860,  ..., -1.5342, -0.8123, -0.5161],\n",
       "                      [-1.2402,  1.5125,  0.9767,  ..., -0.4334,  0.9294, -1.0621],\n",
       "                      [ 0.2514, -1.3432,  1.0655,  ...,  0.1878, -0.5624,  1.0639]])),\n",
       "             ('projection.0.weight',\n",
       "              tensor([[[[ 2.2143e-02, -2.5906e-02, -2.9783e-02,  ...,  6.9042e-03,\n",
       "                          1.0269e-02,  1.9584e-02],\n",
       "                        [-2.2519e-02,  2.1502e-02,  1.0352e-02,  ...,  2.5635e-02,\n",
       "                          7.9747e-04,  2.2571e-02],\n",
       "                        [-1.9152e-02, -7.2781e-04,  2.9214e-02,  ..., -3.4918e-03,\n",
       "                          3.0819e-02,  8.0462e-03],\n",
       "                        ...,\n",
       "                        [-2.1080e-02, -3.4749e-02,  2.8317e-02,  ..., -7.5028e-04,\n",
       "                          2.4819e-02,  1.3444e-02],\n",
       "                        [-3.7063e-03, -2.7756e-02, -1.7964e-02,  ..., -1.0694e-02,\n",
       "                          1.8920e-02, -3.8849e-03],\n",
       "                        [-7.9053e-03,  1.1973e-02,  1.5795e-02,  ...,  2.9180e-02,\n",
       "                          2.1260e-02,  5.2221e-03]],\n",
       "              \n",
       "                       [[-2.1433e-02, -2.0124e-02, -6.3602e-03,  ...,  3.0999e-04,\n",
       "                          3.1703e-02, -6.3182e-04],\n",
       "                        [ 1.0958e-02, -7.9947e-04,  1.5595e-02,  ..., -4.8812e-03,\n",
       "                          3.2207e-03, -1.5762e-02],\n",
       "                        [-3.3861e-04,  1.0122e-02,  3.2699e-02,  ..., -1.3183e-02,\n",
       "                         -2.8057e-02,  5.6190e-03],\n",
       "                        ...,\n",
       "                        [ 1.6290e-02, -2.5504e-02, -2.0110e-02,  ...,  2.6608e-02,\n",
       "                         -2.8284e-02, -7.7760e-03],\n",
       "                        [-2.2035e-02,  2.4049e-02, -3.1991e-02,  ...,  3.0591e-03,\n",
       "                          1.1494e-02, -1.7623e-03],\n",
       "                        [-8.0352e-03,  2.9685e-02, -2.1670e-02,  ..., -1.9291e-02,\n",
       "                         -1.6386e-02, -2.1935e-02]],\n",
       "              \n",
       "                       [[ 1.4528e-02,  7.0614e-03, -3.4056e-02,  ...,  2.4401e-03,\n",
       "                          2.1563e-02,  1.9386e-02],\n",
       "                        [ 3.3734e-02,  2.1708e-02, -4.3566e-03,  ...,  2.4204e-02,\n",
       "                         -2.2530e-02, -2.5084e-02],\n",
       "                        [-3.3063e-02, -2.4997e-02, -1.2464e-02,  ...,  6.4713e-03,\n",
       "                         -1.3136e-03, -3.0659e-02],\n",
       "                        ...,\n",
       "                        [ 2.0084e-02,  1.4214e-02,  2.2907e-03,  ..., -3.6012e-02,\n",
       "                         -4.4903e-03,  2.6564e-02],\n",
       "                        [-2.7248e-02,  2.0481e-02,  3.1639e-02,  ...,  6.8619e-03,\n",
       "                         -2.0456e-03,  1.2549e-02],\n",
       "                        [ 3.1342e-02,  1.0023e-02,  4.4013e-03,  ...,  5.0623e-03,\n",
       "                          1.3163e-02, -1.2543e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.7844e-02, -2.2381e-03, -3.4842e-02,  ..., -3.2693e-02,\n",
       "                         -3.9946e-03,  1.0673e-02],\n",
       "                        [ 2.6587e-02,  2.7790e-02, -8.6909e-04,  ...,  7.5648e-03,\n",
       "                          3.4738e-02,  1.2429e-02],\n",
       "                        [-2.9486e-02, -2.0404e-02, -1.7091e-02,  ..., -2.9697e-02,\n",
       "                          1.6939e-03, -3.0112e-04],\n",
       "                        ...,\n",
       "                        [-3.2805e-02,  8.8459e-03, -1.2310e-02,  ..., -1.2599e-02,\n",
       "                         -3.1213e-02, -3.5308e-03],\n",
       "                        [-8.2593e-03, -3.0852e-02,  7.7668e-04,  ...,  9.6201e-05,\n",
       "                         -3.6416e-03, -2.8154e-02],\n",
       "                        [-2.4520e-02, -4.0091e-04,  6.8495e-03,  ..., -2.1774e-03,\n",
       "                          2.4671e-02,  2.8449e-02]],\n",
       "              \n",
       "                       [[-2.7405e-02,  4.1731e-03,  1.9709e-02,  ...,  1.6713e-02,\n",
       "                         -1.2079e-02, -3.4990e-02],\n",
       "                        [-2.4723e-02,  1.2010e-02,  1.7358e-02,  ..., -1.9542e-02,\n",
       "                         -1.3960e-02, -4.4803e-03],\n",
       "                        [ 1.5413e-02, -3.3352e-02, -3.2204e-03,  ..., -9.2517e-03,\n",
       "                          2.4425e-02, -3.3903e-02],\n",
       "                        ...,\n",
       "                        [-1.1650e-02, -1.0225e-02,  2.8720e-02,  ...,  3.2761e-02,\n",
       "                          3.1130e-02, -2.7420e-02],\n",
       "                        [-1.1101e-02,  3.4889e-02,  2.4873e-02,  ..., -4.2264e-03,\n",
       "                          5.7158e-03,  7.2236e-03],\n",
       "                        [ 2.2837e-02,  7.8971e-03,  4.7838e-03,  ..., -1.5796e-02,\n",
       "                          9.0450e-03,  1.9148e-02]],\n",
       "              \n",
       "                       [[ 1.1103e-02,  1.5646e-02, -1.0203e-02,  ..., -1.8418e-02,\n",
       "                          2.1415e-02,  1.9574e-02],\n",
       "                        [-2.1405e-02, -2.2934e-02,  3.4571e-02,  ...,  1.1261e-02,\n",
       "                         -1.1345e-02, -1.8980e-02],\n",
       "                        [ 2.3374e-02, -3.2549e-03, -5.4587e-03,  ...,  8.7171e-03,\n",
       "                         -7.6528e-03,  1.0935e-02],\n",
       "                        ...,\n",
       "                        [ 3.0085e-02,  9.5618e-03, -2.4864e-02,  ..., -1.1977e-03,\n",
       "                         -1.7568e-02,  2.8346e-02],\n",
       "                        [ 2.4324e-02, -2.1292e-02, -2.3413e-03,  ..., -3.4264e-02,\n",
       "                          3.3526e-02, -6.7777e-03],\n",
       "                        [ 2.1337e-02, -2.0339e-02,  2.4647e-02,  ...,  2.9556e-02,\n",
       "                         -9.5453e-03,  2.0970e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.2191e-02,  3.0552e-02, -1.7409e-02,  ...,  2.7607e-02,\n",
       "                         -8.4392e-04, -8.3810e-03],\n",
       "                        [ 2.2950e-02,  2.8660e-02,  2.9812e-02,  ...,  7.1075e-03,\n",
       "                         -1.4019e-02,  8.7795e-03],\n",
       "                        [-1.9390e-02, -1.3248e-02,  3.5445e-02,  ...,  3.8038e-03,\n",
       "                          8.2370e-03,  2.3292e-02],\n",
       "                        ...,\n",
       "                        [-1.8163e-02, -3.7971e-03,  3.3604e-02,  ...,  2.3557e-02,\n",
       "                         -8.7910e-03,  1.8894e-02],\n",
       "                        [-1.1047e-03,  1.7873e-02, -1.7097e-02,  ..., -3.5364e-02,\n",
       "                          3.4348e-02,  1.3738e-02],\n",
       "                        [-5.9008e-04,  2.2197e-02,  2.7673e-02,  ...,  3.5097e-02,\n",
       "                         -2.4341e-02,  9.2566e-03]],\n",
       "              \n",
       "                       [[-2.6467e-02,  1.5145e-02, -6.8013e-05,  ...,  1.1138e-02,\n",
       "                          1.2084e-02,  2.5954e-03],\n",
       "                        [-2.8587e-02, -1.0710e-02,  2.8588e-02,  ...,  3.4717e-02,\n",
       "                         -4.3013e-03,  2.8124e-02],\n",
       "                        [ 1.3760e-02, -1.6791e-02, -1.5197e-02,  ..., -1.7005e-02,\n",
       "                          2.0107e-02, -1.9122e-02],\n",
       "                        ...,\n",
       "                        [ 3.1859e-02,  1.4821e-02,  8.5075e-03,  ..., -1.6675e-02,\n",
       "                         -3.4704e-02, -1.7807e-02],\n",
       "                        [-1.9995e-02,  2.3520e-02, -1.1512e-02,  ..., -1.7717e-02,\n",
       "                          3.3552e-03,  3.0701e-02],\n",
       "                        [ 3.2255e-02, -1.1242e-02, -2.4283e-02,  ..., -1.7752e-03,\n",
       "                          2.1577e-02,  2.5985e-02]],\n",
       "              \n",
       "                       [[-3.2782e-02,  1.6233e-02, -1.3562e-02,  ...,  1.5567e-02,\n",
       "                          2.9487e-02,  1.3070e-02],\n",
       "                        [ 9.7625e-03,  3.0742e-02,  1.4919e-02,  ..., -2.3721e-02,\n",
       "                         -1.4051e-02,  2.0864e-02],\n",
       "                        [-3.0910e-02, -2.3403e-02, -1.7486e-02,  ...,  3.4391e-02,\n",
       "                          2.1457e-02, -2.1252e-02],\n",
       "                        ...,\n",
       "                        [-1.0938e-02, -2.0712e-02,  6.3769e-03,  ..., -1.2506e-02,\n",
       "                         -1.3198e-02,  2.0408e-03],\n",
       "                        [-1.1412e-02, -3.5287e-02,  2.2266e-02,  ...,  1.2658e-02,\n",
       "                          1.6746e-02,  2.9574e-02],\n",
       "                        [ 8.2388e-03, -2.9851e-02, -1.9780e-02,  ...,  1.6253e-02,\n",
       "                         -2.9602e-02, -3.5563e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-2.7417e-02,  1.3300e-02, -2.4472e-02,  ..., -3.3476e-02,\n",
       "                         -3.4542e-02, -3.0769e-02],\n",
       "                        [-9.6881e-04,  1.4070e-02, -1.3274e-02,  ...,  6.1373e-03,\n",
       "                         -2.2326e-02, -3.5252e-02],\n",
       "                        [-2.5291e-02,  1.1236e-02,  3.0748e-03,  ...,  7.5795e-03,\n",
       "                          2.9371e-02, -8.6265e-03],\n",
       "                        ...,\n",
       "                        [-1.7265e-02,  3.0560e-02, -1.1836e-02,  ..., -1.0020e-02,\n",
       "                         -1.5286e-02, -1.0117e-02],\n",
       "                        [-1.6856e-02,  3.5589e-02,  1.2548e-02,  ..., -4.3206e-03,\n",
       "                         -2.0856e-02,  1.3812e-02],\n",
       "                        [ 1.2103e-02,  2.2435e-03,  9.7092e-03,  ..., -3.3980e-02,\n",
       "                         -6.5606e-03, -2.9957e-02]],\n",
       "              \n",
       "                       [[ 5.2145e-03, -3.4510e-02,  1.2916e-03,  ...,  1.8023e-02,\n",
       "                          1.9273e-02,  2.0259e-02],\n",
       "                        [ 2.6754e-02, -1.9437e-02,  3.2246e-02,  ...,  3.2781e-02,\n",
       "                         -1.0209e-02, -2.7973e-02],\n",
       "                        [ 1.2997e-02, -6.4436e-03,  2.9893e-02,  ..., -1.3751e-02,\n",
       "                          2.6572e-02, -1.3623e-02],\n",
       "                        ...,\n",
       "                        [ 2.0720e-02, -1.5716e-02,  1.1533e-02,  ..., -1.3022e-02,\n",
       "                         -1.1892e-02, -1.1056e-02],\n",
       "                        [ 1.3040e-02,  3.0174e-02, -3.3098e-02,  ...,  3.2069e-02,\n",
       "                         -1.1221e-02, -7.1747e-03],\n",
       "                        [-2.5689e-02, -1.6771e-02, -2.3224e-02,  ..., -9.5865e-03,\n",
       "                         -1.1015e-02, -3.4099e-02]],\n",
       "              \n",
       "                       [[ 2.2057e-02,  1.4489e-02,  9.7497e-03,  ...,  3.4229e-02,\n",
       "                          3.5193e-03, -3.0575e-02],\n",
       "                        [-1.2786e-02,  3.4456e-02,  2.4152e-02,  ..., -1.0968e-02,\n",
       "                          1.7276e-02,  3.4254e-02],\n",
       "                        [ 1.0038e-02,  6.2508e-03, -2.0420e-02,  ..., -1.1022e-02,\n",
       "                          2.4482e-02, -9.6634e-03],\n",
       "                        ...,\n",
       "                        [-2.2430e-02, -2.7618e-02, -1.8502e-02,  ..., -1.4122e-02,\n",
       "                          1.0745e-02,  3.3486e-02],\n",
       "                        [-1.8660e-02,  3.4936e-02,  3.2217e-02,  ..., -2.0402e-02,\n",
       "                         -1.4725e-02, -1.5103e-02],\n",
       "                        [ 1.7227e-02, -2.2741e-02, -2.7641e-02,  ..., -8.1282e-03,\n",
       "                         -3.4397e-02, -2.9083e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.0202e-02,  1.8013e-02,  3.2200e-02,  ...,  3.7150e-03,\n",
       "                         -1.0557e-02, -9.9034e-03],\n",
       "                        [-7.5497e-03, -8.9817e-03,  2.5491e-02,  ..., -3.4789e-03,\n",
       "                          2.3128e-02, -3.2644e-02],\n",
       "                        [-1.8228e-02, -8.7158e-03,  3.6081e-02,  ...,  2.1000e-02,\n",
       "                         -5.0170e-03,  2.0502e-02],\n",
       "                        ...,\n",
       "                        [-1.4981e-02, -2.5932e-04,  1.5986e-02,  ...,  1.6599e-02,\n",
       "                         -7.7363e-03,  5.8934e-03],\n",
       "                        [ 1.5729e-02, -3.0198e-02, -3.3190e-02,  ..., -2.6923e-02,\n",
       "                          1.7797e-03,  4.4259e-03],\n",
       "                        [ 7.9910e-03, -3.0299e-02,  1.6143e-02,  ..., -3.0157e-02,\n",
       "                         -3.0418e-02,  1.7581e-02]],\n",
       "              \n",
       "                       [[-2.1587e-02,  1.4784e-02, -3.4656e-02,  ...,  2.1524e-02,\n",
       "                         -2.0656e-02, -4.2055e-03],\n",
       "                        [-1.9912e-05,  1.7941e-02,  1.9308e-02,  ...,  4.9008e-03,\n",
       "                          2.8072e-02, -3.3603e-02],\n",
       "                        [ 9.9163e-03,  2.8874e-02, -2.8596e-02,  ...,  1.0498e-03,\n",
       "                         -2.2358e-03,  3.3554e-03],\n",
       "                        ...,\n",
       "                        [-1.1782e-02,  1.6612e-02, -3.5277e-02,  ...,  1.9195e-02,\n",
       "                         -3.2529e-02,  3.0123e-02],\n",
       "                        [-2.2838e-02, -1.9470e-02, -2.8635e-02,  ...,  2.0925e-02,\n",
       "                          3.4343e-02, -7.6534e-03],\n",
       "                        [-1.1310e-02,  1.9295e-02,  1.8354e-02,  ..., -5.4577e-03,\n",
       "                         -3.5525e-02,  1.8912e-03]],\n",
       "              \n",
       "                       [[-3.5836e-02, -1.0088e-02, -3.4607e-02,  ..., -3.4459e-02,\n",
       "                         -1.2520e-02,  2.9554e-02],\n",
       "                        [-2.1489e-02,  8.2629e-03,  2.9465e-02,  ..., -3.2366e-03,\n",
       "                         -1.7719e-02,  5.5570e-04],\n",
       "                        [ 1.9632e-02,  3.4003e-02, -5.2285e-03,  ...,  1.6114e-02,\n",
       "                         -2.2166e-02, -3.4853e-02],\n",
       "                        ...,\n",
       "                        [ 1.1150e-02,  1.1596e-02, -3.4684e-02,  ..., -1.6592e-02,\n",
       "                          3.5108e-02,  1.8210e-02],\n",
       "                        [-3.4051e-02, -2.0338e-02,  1.8174e-02,  ..., -7.2417e-04,\n",
       "                         -3.4562e-03,  3.4076e-02],\n",
       "                        [-1.0109e-02, -1.4536e-02,  2.5940e-02,  ..., -1.2799e-02,\n",
       "                         -2.9819e-02, -5.2625e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.8673e-03, -2.5706e-02,  6.1884e-03,  ..., -1.4698e-03,\n",
       "                          1.6900e-02,  1.2977e-02],\n",
       "                        [-2.2504e-02, -2.2512e-02,  1.5642e-02,  ..., -6.9960e-03,\n",
       "                         -2.1434e-02, -3.3105e-02],\n",
       "                        [ 1.6899e-02,  2.2086e-02, -1.2253e-02,  ..., -3.1982e-03,\n",
       "                          9.0829e-03,  2.2409e-02],\n",
       "                        ...,\n",
       "                        [ 8.2729e-03, -1.3756e-02,  9.1300e-03,  ..., -1.2436e-02,\n",
       "                         -3.2539e-02,  1.9844e-02],\n",
       "                        [ 3.9265e-05,  2.4286e-02,  1.0201e-02,  ...,  2.7114e-02,\n",
       "                         -3.1951e-02, -3.0812e-03],\n",
       "                        [-2.8606e-02,  7.3319e-03, -2.6715e-02,  ...,  1.8461e-02,\n",
       "                         -1.3499e-02,  2.2139e-02]],\n",
       "              \n",
       "                       [[ 5.7368e-03,  2.6985e-02, -2.2016e-02,  ...,  2.6653e-02,\n",
       "                         -2.0469e-03, -1.0793e-02],\n",
       "                        [ 2.8174e-02, -7.8740e-03,  3.1409e-02,  ..., -3.5553e-02,\n",
       "                         -1.4914e-02, -1.7657e-02],\n",
       "                        [-3.3414e-02, -3.0138e-02, -2.8419e-02,  ...,  1.8285e-02,\n",
       "                          3.4190e-02, -1.6871e-02],\n",
       "                        ...,\n",
       "                        [-1.0944e-02,  1.4816e-02, -7.5946e-03,  ..., -5.3669e-03,\n",
       "                          3.1046e-02, -2.4657e-02],\n",
       "                        [-1.1851e-02, -2.8912e-02,  1.4997e-02,  ..., -2.6710e-02,\n",
       "                          5.2338e-05, -2.2360e-02],\n",
       "                        [ 1.0846e-03, -1.0806e-02, -2.6946e-02,  ...,  2.4147e-03,\n",
       "                          3.1020e-02,  3.2365e-03]],\n",
       "              \n",
       "                       [[ 1.5545e-02,  3.4656e-02, -1.4922e-02,  ..., -2.4375e-02,\n",
       "                          1.2407e-03,  3.0980e-02],\n",
       "                        [ 1.0054e-02, -1.9803e-02, -2.5332e-02,  ...,  7.4110e-03,\n",
       "                          1.3834e-02, -2.3072e-02],\n",
       "                        [ 3.1972e-02, -2.2141e-02, -8.3540e-03,  ...,  2.3153e-02,\n",
       "                          2.0500e-02,  3.3442e-02],\n",
       "                        ...,\n",
       "                        [ 2.7979e-02,  7.3420e-04, -2.8730e-02,  ..., -9.4302e-03,\n",
       "                         -1.5602e-02,  3.0962e-02],\n",
       "                        [-3.5866e-02, -2.7154e-02,  1.4984e-02,  ..., -2.8217e-02,\n",
       "                          1.9844e-02,  1.2258e-02],\n",
       "                        [ 1.2188e-02,  2.0141e-02,  3.4671e-02,  ...,  2.8121e-02,\n",
       "                         -1.9845e-02, -3.3658e-02]]]])),\n",
       "             ('projection.0.bias',\n",
       "              tensor([ 3.5702e-02,  7.0493e-03, -1.9081e-02, -2.7381e-02, -2.4379e-02,\n",
       "                      -4.8934e-03, -2.8930e-02, -2.7225e-03, -1.1242e-02, -1.7358e-02,\n",
       "                       1.4954e-02,  1.4730e-03,  2.6338e-02,  1.7759e-02,  7.4338e-03,\n",
       "                       8.0904e-03, -1.6253e-04, -3.4481e-02,  2.3973e-02,  3.5038e-03,\n",
       "                       2.6734e-02, -1.3048e-04,  2.0852e-02,  1.4127e-02,  7.4908e-05,\n",
       "                       2.6266e-02, -3.4245e-02, -2.4251e-02, -1.7061e-02,  2.0853e-02,\n",
       "                      -1.5790e-02, -1.1208e-02,  9.7838e-03,  8.4673e-04,  2.7347e-02,\n",
       "                       3.3422e-02,  2.5381e-02,  2.4230e-02,  3.3630e-02, -2.8170e-02,\n",
       "                      -3.5479e-02, -2.7866e-02, -1.8643e-02, -3.2640e-02, -5.6026e-03,\n",
       "                       1.5410e-02,  1.0245e-02, -2.6683e-02,  2.4413e-02,  7.7745e-03,\n",
       "                       7.9370e-03,  2.8115e-02,  1.3701e-02, -3.0088e-02, -1.4962e-02,\n",
       "                      -1.3636e-02,  2.5675e-02,  3.1446e-02,  1.6909e-02, -2.4173e-02,\n",
       "                       1.0283e-02, -3.3342e-02,  2.7861e-02,  7.3346e-03,  2.7538e-02,\n",
       "                       2.2016e-02, -1.3825e-02,  2.7925e-02,  5.6741e-03,  2.5565e-03,\n",
       "                       3.5887e-02, -3.2117e-02,  9.7145e-03, -3.3866e-03,  8.2821e-03,\n",
       "                       1.5044e-02,  6.2528e-03,  3.1610e-02,  3.1239e-02,  2.3065e-02,\n",
       "                       1.1384e-02, -1.4149e-02, -1.3105e-02, -3.8931e-03, -3.0023e-02,\n",
       "                       2.2357e-02, -3.1246e-02, -3.4582e-02, -3.1873e-02,  3.4428e-02,\n",
       "                       2.7835e-02,  1.5564e-02, -3.4577e-02, -2.3399e-02, -9.0851e-03,\n",
       "                       4.4890e-03,  9.0276e-03, -2.6820e-02, -1.7719e-02, -1.4854e-02,\n",
       "                      -1.3240e-02,  2.3619e-03, -1.5151e-03, -2.9455e-02, -3.7835e-03,\n",
       "                      -2.4983e-02, -1.8313e-02, -9.2181e-03, -3.4060e-02,  1.9527e-03,\n",
       "                       1.0319e-02, -2.7539e-02,  6.2397e-04, -2.3438e-02,  3.0884e-02,\n",
       "                       3.3020e-02, -8.3250e-03, -2.6562e-02, -1.1934e-02,  2.8739e-02,\n",
       "                       7.9616e-03,  3.0450e-02,  2.7460e-03,  2.5476e-02,  6.4163e-03,\n",
       "                       3.3634e-02,  1.6949e-02,  7.3819e-03,  3.1041e-02, -2.5703e-02,\n",
       "                      -3.3402e-02,  1.3964e-02,  4.0303e-03, -1.9699e-02,  3.4116e-02,\n",
       "                      -2.0636e-02,  2.6398e-02, -2.6081e-02, -1.6272e-03,  3.1573e-02,\n",
       "                       3.2208e-02,  2.2890e-02, -9.2271e-03, -1.8582e-02, -1.3031e-02,\n",
       "                      -2.1118e-02,  3.3786e-02, -7.8902e-03, -1.8311e-02,  2.2446e-02,\n",
       "                      -2.6304e-02, -1.6377e-02,  1.8698e-02, -3.5854e-02,  7.8595e-03,\n",
       "                       1.3285e-02,  2.7128e-02, -5.6398e-03, -2.4399e-02, -2.4291e-02,\n",
       "                       1.1742e-02, -2.4836e-02, -3.4954e-02, -3.2686e-02,  3.2309e-02,\n",
       "                       3.0956e-02, -1.1840e-02, -3.5060e-02,  2.1481e-02,  2.3507e-02,\n",
       "                       1.7865e-02, -1.6082e-02, -8.8655e-04, -6.8533e-03, -1.7903e-02,\n",
       "                       1.6495e-02, -2.0820e-02, -1.9059e-02, -1.1346e-02, -3.0820e-02,\n",
       "                      -2.6860e-03,  3.3725e-02, -3.5156e-02,  2.5946e-02, -1.8739e-02,\n",
       "                       3.6222e-04, -2.7990e-02, -2.6784e-03,  2.3495e-02, -1.1564e-02,\n",
       "                      -3.4012e-02, -1.1933e-02, -4.7722e-03, -3.4515e-02,  1.4225e-02,\n",
       "                      -3.4890e-03,  1.3145e-02, -2.7621e-02,  2.0838e-02, -2.0686e-03,\n",
       "                      -2.7644e-02, -3.1405e-02,  3.3955e-02,  4.3274e-03,  2.0151e-02,\n",
       "                       3.8958e-03,  1.3474e-02, -3.0936e-02,  2.3408e-02,  3.2201e-02,\n",
       "                      -3.3421e-02,  1.2226e-02, -6.7391e-03, -2.1450e-02,  2.3292e-02,\n",
       "                       1.0777e-02,  2.9866e-02,  6.9098e-03,  7.7868e-03,  1.5032e-02,\n",
       "                      -1.1471e-03, -7.2169e-03, -3.5149e-02, -1.2308e-03, -3.3010e-02,\n",
       "                      -2.0208e-04,  1.3695e-02,  1.7958e-03, -1.8414e-02, -1.9045e-02,\n",
       "                       2.0626e-02,  1.7701e-02,  5.9611e-03,  3.1603e-02,  2.8985e-02,\n",
       "                      -3.3286e-02, -1.8490e-02,  3.3751e-02, -2.5437e-02,  2.4418e-02,\n",
       "                       1.4508e-02,  8.0697e-04,  2.0361e-02, -7.3454e-04, -2.2002e-02,\n",
       "                      -1.4615e-02, -4.4590e-03, -1.4866e-02,  2.0440e-02,  2.1123e-02,\n",
       "                      -3.5638e-02, -9.9369e-03,  1.7134e-02,  1.9973e-02, -2.2007e-02,\n",
       "                       3.0828e-02, -1.3770e-02,  1.8437e-02, -3.1264e-02,  1.5494e-05,\n",
       "                       1.2865e-02, -2.8802e-02, -2.0251e-02, -2.6688e-02, -1.4006e-02,\n",
       "                      -6.6713e-03,  2.7367e-03,  4.7781e-03,  8.4867e-03,  1.8819e-02,\n",
       "                       3.0428e-02,  1.6606e-02,  2.4378e-02,  3.1650e-02,  2.1292e-03,\n",
       "                       2.5171e-02, -3.7005e-03, -2.4190e-02,  1.6764e-02,  1.1988e-02,\n",
       "                      -2.9090e-02,  3.5040e-02,  6.3939e-03,  2.7123e-02, -1.0657e-02,\n",
       "                       2.7930e-02, -1.5497e-02,  3.1661e-02,  7.3908e-03, -3.4865e-02,\n",
       "                       1.3156e-02, -1.9297e-02,  7.0368e-03,  2.3006e-02,  2.5064e-02,\n",
       "                      -3.6010e-02,  3.3397e-02, -2.7263e-03,  7.0604e-03, -1.9723e-02,\n",
       "                       1.1925e-03, -1.1721e-02, -1.3645e-02,  9.2631e-03,  2.8313e-02,\n",
       "                      -3.5050e-02,  1.2726e-02, -2.4652e-02, -1.2746e-02, -2.9072e-02,\n",
       "                      -2.1093e-02, -3.4945e-02,  3.3134e-02, -8.7349e-03,  1.1285e-02,\n",
       "                      -3.4827e-03, -6.3049e-03, -5.6273e-03, -3.6104e-03, -3.1020e-02,\n",
       "                      -1.0255e-02,  1.0628e-02, -1.7986e-02, -3.0724e-02, -2.7182e-03,\n",
       "                       2.4469e-02,  1.4819e-02,  6.3580e-03,  3.2621e-02, -1.9831e-02,\n",
       "                       2.1733e-02,  2.1061e-02, -1.6476e-02,  1.1531e-02,  2.6760e-02,\n",
       "                       2.7598e-02, -1.2875e-02,  2.4277e-02,  2.5881e-02,  5.1084e-03,\n",
       "                      -1.0678e-02, -2.1452e-02, -3.1868e-02,  1.6680e-02,  3.4874e-02,\n",
       "                      -2.0011e-02,  1.0270e-02,  2.7460e-02,  3.4251e-02, -3.1723e-02,\n",
       "                       2.1863e-02, -6.1525e-03, -6.4735e-03, -2.6363e-02,  4.9331e-03,\n",
       "                       3.4420e-02, -3.3791e-02,  2.7147e-03,  1.4738e-02, -9.4428e-04,\n",
       "                       5.3436e-03,  1.3242e-02,  2.1099e-02,  3.2443e-02, -3.2111e-02,\n",
       "                      -8.3226e-04,  2.9342e-02,  1.5800e-02, -2.3345e-02, -9.1818e-04,\n",
       "                       1.9087e-02, -6.3237e-03, -2.7598e-02,  2.7313e-02,  2.6258e-02,\n",
       "                      -3.0253e-02,  1.0096e-02,  2.8898e-02,  2.9865e-02,  7.3933e-03,\n",
       "                       1.4263e-02, -3.2027e-02, -9.9265e-03, -1.6987e-02, -6.9366e-03,\n",
       "                       3.5899e-02, -2.0761e-02,  2.9151e-02, -2.1270e-02,  3.5949e-02,\n",
       "                       2.8234e-03,  2.8563e-02,  2.8768e-02,  9.3472e-03, -2.5460e-02,\n",
       "                      -1.9364e-02, -3.5397e-02, -2.5109e-02,  1.3411e-02, -1.9232e-04,\n",
       "                      -2.7454e-02, -2.0259e-02,  1.1016e-02, -3.5178e-02,  1.5567e-03,\n",
       "                       2.5197e-02,  1.2210e-03, -3.5532e-02,  2.8303e-02, -1.1663e-02,\n",
       "                      -3.5948e-02, -6.7302e-03, -6.2094e-03,  1.3896e-02,  2.8736e-03,\n",
       "                       2.7742e-02,  1.6582e-02,  1.2069e-02,  1.2720e-02, -1.4004e-02,\n",
       "                      -9.7087e-03,  2.1086e-02, -1.4816e-02, -5.8829e-03,  2.5736e-02,\n",
       "                       4.8522e-03, -1.0495e-02,  2.2956e-02,  6.6945e-04, -8.3369e-03,\n",
       "                       1.7313e-02,  1.0739e-03,  1.0645e-02, -1.8192e-02, -2.4356e-02,\n",
       "                      -1.5132e-02, -1.5558e-02,  3.0916e-02,  1.3041e-02, -5.4018e-03,\n",
       "                      -8.0266e-03, -3.3205e-02, -4.8671e-03, -4.9232e-03, -9.6747e-03,\n",
       "                      -1.9930e-02,  2.5679e-02, -1.7075e-02, -1.2497e-02, -2.9018e-02,\n",
       "                      -3.5329e-02, -3.1956e-02, -8.6593e-03, -3.7677e-03, -3.1458e-02,\n",
       "                      -9.5188e-03, -1.4965e-02,  1.1422e-02,  1.8267e-02,  3.4202e-02,\n",
       "                       5.6194e-03,  1.1605e-02, -6.8602e-04,  2.8821e-02,  8.2730e-03,\n",
       "                      -9.8699e-03,  1.4545e-02, -2.3412e-02, -1.2808e-02,  1.4432e-02,\n",
       "                      -2.2421e-02,  1.8620e-02,  3.3297e-03, -1.3842e-03,  1.4841e-02,\n",
       "                       2.3605e-02,  2.0734e-03, -4.1542e-04, -9.8642e-03, -3.0339e-02,\n",
       "                       3.4549e-02, -3.5822e-02,  4.3430e-03, -1.7185e-02, -2.4726e-02,\n",
       "                       1.0646e-02,  1.8444e-02,  5.4800e-03,  3.5638e-02, -1.4208e-04,\n",
       "                       1.0561e-02,  2.6190e-02, -3.1873e-02, -1.7494e-02,  1.7786e-02,\n",
       "                       8.1657e-03,  9.6251e-03, -1.5370e-02, -8.5948e-03,  3.1587e-03,\n",
       "                       3.5128e-02,  2.4430e-02,  1.5475e-02, -1.3795e-02, -1.9167e-02,\n",
       "                       2.3699e-02, -2.5543e-02, -1.2887e-02,  3.2537e-02, -1.1549e-02,\n",
       "                      -3.5040e-02, -1.4496e-02, -3.0148e-02, -2.0211e-02,  6.4055e-03,\n",
       "                      -2.1085e-02,  7.8851e-03,  1.3548e-02, -3.0103e-02, -1.9822e-02,\n",
       "                       9.8060e-03, -3.5557e-03, -7.4330e-03, -1.5351e-03, -2.3761e-02,\n",
       "                       1.9479e-02,  2.5756e-02,  9.6349e-03,  1.4782e-02,  3.0909e-02,\n",
       "                       8.6846e-03, -7.8828e-03, -3.2920e-02, -2.8104e-02,  6.2113e-03,\n",
       "                      -7.9687e-03, -1.5528e-02, -1.4603e-02,  8.1776e-03,  1.7958e-02,\n",
       "                      -5.5188e-03,  2.7125e-02, -2.4677e-02, -1.8105e-02,  1.2530e-02,\n",
       "                       3.1415e-02,  9.3780e-03,  8.2740e-03, -3.0174e-02,  3.1113e-02,\n",
       "                      -2.4977e-02,  5.3414e-03, -2.0692e-02,  8.1038e-03,  2.6004e-02,\n",
       "                      -2.3350e-02, -8.7661e-03,  1.2707e-02,  2.1212e-02, -3.1100e-02,\n",
       "                      -2.7857e-02, -2.0064e-02, -1.0380e-02,  8.5711e-03, -8.7754e-03,\n",
       "                      -1.1677e-02, -4.1841e-03, -2.5882e-02, -2.8835e-02,  2.9874e-02,\n",
       "                      -1.1386e-02,  1.5886e-02,  3.3334e-02,  6.3230e-03,  8.1487e-03,\n",
       "                      -1.0587e-02,  1.5584e-03,  1.7936e-02,  1.8614e-02, -2.2201e-02,\n",
       "                       1.1043e-02, -3.4464e-02,  3.0405e-02, -2.3407e-02, -1.0386e-02,\n",
       "                      -2.8058e-02, -3.2852e-02,  2.8088e-02,  5.1664e-03,  3.8087e-03,\n",
       "                      -1.6481e-02, -1.8228e-02,  2.3594e-02,  1.7054e-02, -1.9702e-02,\n",
       "                      -2.4012e-02, -2.6367e-02,  8.6910e-03, -1.6168e-02,  1.6819e-02,\n",
       "                       2.6429e-02, -2.0170e-03,  3.3575e-02, -2.5632e-03,  2.8536e-02,\n",
       "                       3.0037e-02,  6.8479e-03, -1.0070e-02, -3.0853e-02,  6.6792e-03,\n",
       "                      -2.3659e-02,  1.1412e-04, -2.4703e-02, -4.0163e-03, -2.4850e-02,\n",
       "                       3.1242e-02,  6.2666e-03, -2.2100e-02, -3.3966e-02, -2.2563e-02,\n",
       "                      -2.8870e-02, -1.0698e-04, -3.4884e-02,  3.5836e-02,  3.0894e-02,\n",
       "                       8.2466e-03, -3.9402e-03,  1.5750e-02, -3.3052e-02,  1.7240e-02,\n",
       "                       1.5513e-03,  1.0049e-02, -1.0627e-03, -1.1598e-02, -4.1795e-04,\n",
       "                      -1.8957e-02, -7.5321e-03, -1.4094e-02,  3.0662e-02, -2.7977e-02,\n",
       "                      -2.2981e-02,  2.2593e-03, -1.1352e-02, -1.5656e-02, -2.0974e-02,\n",
       "                      -3.3590e-02, -2.6234e-02, -5.8613e-03,  1.9009e-02,  1.6723e-02,\n",
       "                       3.0277e-03,  1.9843e-02, -2.5589e-02,  3.1485e-03,  5.1066e-03,\n",
       "                      -3.0269e-02, -7.9123e-03, -1.2144e-02,  2.2059e-02,  7.4665e-03,\n",
       "                      -1.9788e-02,  2.5757e-02, -9.6829e-03,  3.2625e-02, -7.6661e-04,\n",
       "                      -2.6581e-02, -1.2255e-02,  2.3379e-02,  2.1590e-02,  4.3446e-03,\n",
       "                       1.5502e-02,  2.6093e-02,  3.0768e-02, -1.5515e-02,  3.7334e-03,\n",
       "                       3.5659e-02,  2.0186e-02, -1.3004e-02,  5.2762e-03, -2.2858e-02,\n",
       "                      -2.8327e-02, -4.9214e-03,  2.6911e-02, -2.5635e-03, -3.6627e-03,\n",
       "                      -1.1428e-02,  2.6319e-02,  1.4170e-02, -3.1620e-02,  1.8039e-02,\n",
       "                       1.8464e-02,  3.1557e-02, -2.3204e-02,  7.4479e-03, -9.6188e-03,\n",
       "                      -1.9466e-02, -5.3162e-03,  1.7144e-02,  1.4953e-02,  1.6618e-03,\n",
       "                       5.8354e-03, -1.3954e-02, -3.8852e-03,  1.6177e-02,  1.1537e-02,\n",
       "                       2.8712e-03, -3.3493e-03, -1.4792e-02,  3.2599e-02,  9.7735e-03,\n",
       "                      -2.6296e-02, -3.3848e-02, -1.7188e-02,  3.2091e-02, -2.9708e-02,\n",
       "                       1.1858e-02,  3.1420e-02,  1.2039e-02,  3.4933e-02, -2.4249e-02,\n",
       "                       3.0965e-02,  1.2708e-02, -2.6617e-02, -2.6464e-02, -6.5959e-03,\n",
       "                      -3.2858e-02, -3.3324e-02,  1.5549e-02, -1.9958e-02,  2.9341e-02,\n",
       "                       1.3354e-02,  1.8985e-02,  2.4663e-04,  8.4841e-03,  1.0983e-03,\n",
       "                       1.8865e-02, -3.2272e-02,  2.8317e-02,  1.0651e-02, -3.2277e-03,\n",
       "                       4.1540e-03, -2.7158e-03,  3.7698e-03, -3.1537e-03,  5.8811e-05,\n",
       "                       7.4564e-03,  1.6007e-02,  9.8318e-03,  2.9163e-02,  3.4235e-02,\n",
       "                      -3.2028e-02, -1.6730e-02, -3.5488e-02, -2.3904e-02, -1.4195e-02,\n",
       "                       1.3575e-02,  1.0177e-03,  3.5214e-02, -2.2008e-03,  2.9250e-02,\n",
       "                       2.5252e-02,  1.3523e-02,  1.0406e-02,  2.0144e-02, -3.1976e-02,\n",
       "                       2.8409e-02,  2.0079e-04, -1.2740e-02]))])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dst = ViT()\n",
    "dst.embedding.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from glasses.models import AutoModel, AutoConfig, EfficientNetLite\n",
    "import timm\n",
    "from transfer_weights import clone_model\n",
    "from benchmark import benchmark\n",
    "from glasses.models.classification.vit import ViT\n",
    "\n",
    "src = timm.create_model('vit_base_patch16_384', pretrained='True')\n",
    "dst = ViT.vit_base_patch16_384()\n",
    "\n",
    "# dst = VisionTransformer()\n",
    "dst = clone_model(src, dst, torch.randn((1, 3, 384, 384))).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of glasses.models.AutoConfig failed: Traceback (most recent call last):\n",
      "  File \"/home/zuppif/anaconda3/envs/dl/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/zuppif/anaconda3/envs/dl/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/home/zuppif/anaconda3/envs/dl/lib/python3.8/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/home/zuppif/anaconda3/envs/dl/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 779, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 916, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 846, in source_to_code\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/zuppif/Documents/torcheyes/glasses/models/AutoConfig.py\", line 36\n",
      "    }\n",
      "    ^\n",
      "SyntaxError: invalid syntax\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['vit_base_patch16_224',\n",
       " 'vit_base_patch16_384',\n",
       " 'vit_base_patch32_384',\n",
       " 'vit_base_resnet26d_224',\n",
       " 'vit_base_resnet50d_224',\n",
       " 'vit_huge_patch16_224',\n",
       " 'vit_huge_patch32_384',\n",
       " 'vit_large_patch16_224',\n",
       " 'vit_large_patch16_384',\n",
       " 'vit_large_patch32_384',\n",
       " 'vit_small_patch16_224',\n",
       " 'vit_small_resnet26d_224',\n",
       " 'vit_small_resnet50d_s3_224']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm.list_models('vit*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./imagenet_val_targets.pkl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1713fa3451945fd94ec0aacba7110c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=391.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9765625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = AutoConfig.from_name('vit_base_patch16_384').transform\n",
    "\n",
    "benchmark(src.cuda().eval(), transform, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./imagenet_val_targets.pkl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4227564e58334bd598d05c3d31f75b10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=391.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9765625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dst.embedding.positions.data.copy_(src.pos_embed.data.squeeze(0))\n",
    "dst.embedding.cls_token.data.copy_(src.cls_token.data)\n",
    "\n",
    "transform = AutoConfig.from_name('vit_base_patch16_384').transform\n",
    "\n",
    "benchmark(dst.cuda().eval(), transform, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 14, 14]         590,592\n",
      "        PatchEmbed-2             [-1, 196, 768]               0\n",
      "           Dropout-3             [-1, 197, 768]               0\n",
      "         LayerNorm-4             [-1, 197, 768]           1,536\n",
      "            Linear-5            [-1, 197, 2304]       1,771,776\n",
      "           Dropout-6         [-1, 12, 197, 197]               0\n",
      "            Linear-7             [-1, 197, 768]         590,592\n",
      "           Dropout-8             [-1, 197, 768]               0\n",
      "MultiHeadAttentionTimm-9             [-1, 197, 768]               0\n",
      "         Identity-10             [-1, 197, 768]               0\n",
      "        LayerNorm-11             [-1, 197, 768]           1,536\n",
      "           Linear-12            [-1, 197, 3072]       2,362,368\n",
      "             GELU-13            [-1, 197, 3072]               0\n",
      "          Dropout-14            [-1, 197, 3072]               0\n",
      "           Linear-15             [-1, 197, 768]       2,360,064\n",
      "          Dropout-16             [-1, 197, 768]               0\n",
      "              Mlp-17             [-1, 197, 768]               0\n",
      "         Identity-18             [-1, 197, 768]               0\n",
      "            Block-19             [-1, 197, 768]               0\n",
      "        LayerNorm-20             [-1, 197, 768]           1,536\n",
      "           Linear-21            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-22         [-1, 12, 197, 197]               0\n",
      "           Linear-23             [-1, 197, 768]         590,592\n",
      "          Dropout-24             [-1, 197, 768]               0\n",
      "MultiHeadAttentionTimm-25             [-1, 197, 768]               0\n",
      "         Identity-26             [-1, 197, 768]               0\n",
      "        LayerNorm-27             [-1, 197, 768]           1,536\n",
      "           Linear-28            [-1, 197, 3072]       2,362,368\n",
      "             GELU-29            [-1, 197, 3072]               0\n",
      "          Dropout-30            [-1, 197, 3072]               0\n",
      "           Linear-31             [-1, 197, 768]       2,360,064\n",
      "          Dropout-32             [-1, 197, 768]               0\n",
      "              Mlp-33             [-1, 197, 768]               0\n",
      "         Identity-34             [-1, 197, 768]               0\n",
      "            Block-35             [-1, 197, 768]               0\n",
      "        LayerNorm-36             [-1, 197, 768]           1,536\n",
      "           Linear-37            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-38         [-1, 12, 197, 197]               0\n",
      "           Linear-39             [-1, 197, 768]         590,592\n",
      "          Dropout-40             [-1, 197, 768]               0\n",
      "MultiHeadAttentionTimm-41             [-1, 197, 768]               0\n",
      "         Identity-42             [-1, 197, 768]               0\n",
      "        LayerNorm-43             [-1, 197, 768]           1,536\n",
      "           Linear-44            [-1, 197, 3072]       2,362,368\n",
      "             GELU-45            [-1, 197, 3072]               0\n",
      "          Dropout-46            [-1, 197, 3072]               0\n",
      "           Linear-47             [-1, 197, 768]       2,360,064\n",
      "          Dropout-48             [-1, 197, 768]               0\n",
      "              Mlp-49             [-1, 197, 768]               0\n",
      "         Identity-50             [-1, 197, 768]               0\n",
      "            Block-51             [-1, 197, 768]               0\n",
      "        LayerNorm-52             [-1, 197, 768]           1,536\n",
      "           Linear-53            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-54         [-1, 12, 197, 197]               0\n",
      "           Linear-55             [-1, 197, 768]         590,592\n",
      "          Dropout-56             [-1, 197, 768]               0\n",
      "MultiHeadAttentionTimm-57             [-1, 197, 768]               0\n",
      "         Identity-58             [-1, 197, 768]               0\n",
      "        LayerNorm-59             [-1, 197, 768]           1,536\n",
      "           Linear-60            [-1, 197, 3072]       2,362,368\n",
      "             GELU-61            [-1, 197, 3072]               0\n",
      "          Dropout-62            [-1, 197, 3072]               0\n",
      "           Linear-63             [-1, 197, 768]       2,360,064\n",
      "          Dropout-64             [-1, 197, 768]               0\n",
      "              Mlp-65             [-1, 197, 768]               0\n",
      "         Identity-66             [-1, 197, 768]               0\n",
      "            Block-67             [-1, 197, 768]               0\n",
      "        LayerNorm-68             [-1, 197, 768]           1,536\n",
      "           Linear-69            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-70         [-1, 12, 197, 197]               0\n",
      "           Linear-71             [-1, 197, 768]         590,592\n",
      "          Dropout-72             [-1, 197, 768]               0\n",
      "MultiHeadAttentionTimm-73             [-1, 197, 768]               0\n",
      "         Identity-74             [-1, 197, 768]               0\n",
      "        LayerNorm-75             [-1, 197, 768]           1,536\n",
      "           Linear-76            [-1, 197, 3072]       2,362,368\n",
      "             GELU-77            [-1, 197, 3072]               0\n",
      "          Dropout-78            [-1, 197, 3072]               0\n",
      "           Linear-79             [-1, 197, 768]       2,360,064\n",
      "          Dropout-80             [-1, 197, 768]               0\n",
      "              Mlp-81             [-1, 197, 768]               0\n",
      "         Identity-82             [-1, 197, 768]               0\n",
      "            Block-83             [-1, 197, 768]               0\n",
      "        LayerNorm-84             [-1, 197, 768]           1,536\n",
      "           Linear-85            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-86         [-1, 12, 197, 197]               0\n",
      "           Linear-87             [-1, 197, 768]         590,592\n",
      "          Dropout-88             [-1, 197, 768]               0\n",
      "MultiHeadAttentionTimm-89             [-1, 197, 768]               0\n",
      "         Identity-90             [-1, 197, 768]               0\n",
      "        LayerNorm-91             [-1, 197, 768]           1,536\n",
      "           Linear-92            [-1, 197, 3072]       2,362,368\n",
      "             GELU-93            [-1, 197, 3072]               0\n",
      "          Dropout-94            [-1, 197, 3072]               0\n",
      "           Linear-95             [-1, 197, 768]       2,360,064\n",
      "          Dropout-96             [-1, 197, 768]               0\n",
      "              Mlp-97             [-1, 197, 768]               0\n",
      "         Identity-98             [-1, 197, 768]               0\n",
      "            Block-99             [-1, 197, 768]               0\n",
      "       LayerNorm-100             [-1, 197, 768]           1,536\n",
      "          Linear-101            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-102         [-1, 12, 197, 197]               0\n",
      "          Linear-103             [-1, 197, 768]         590,592\n",
      "         Dropout-104             [-1, 197, 768]               0\n",
      "MultiHeadAttentionTimm-105             [-1, 197, 768]               0\n",
      "        Identity-106             [-1, 197, 768]               0\n",
      "       LayerNorm-107             [-1, 197, 768]           1,536\n",
      "          Linear-108            [-1, 197, 3072]       2,362,368\n",
      "            GELU-109            [-1, 197, 3072]               0\n",
      "         Dropout-110            [-1, 197, 3072]               0\n",
      "          Linear-111             [-1, 197, 768]       2,360,064\n",
      "         Dropout-112             [-1, 197, 768]               0\n",
      "             Mlp-113             [-1, 197, 768]               0\n",
      "        Identity-114             [-1, 197, 768]               0\n",
      "           Block-115             [-1, 197, 768]               0\n",
      "       LayerNorm-116             [-1, 197, 768]           1,536\n",
      "          Linear-117            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-118         [-1, 12, 197, 197]               0\n",
      "          Linear-119             [-1, 197, 768]         590,592\n",
      "         Dropout-120             [-1, 197, 768]               0\n",
      "MultiHeadAttentionTimm-121             [-1, 197, 768]               0\n",
      "        Identity-122             [-1, 197, 768]               0\n",
      "       LayerNorm-123             [-1, 197, 768]           1,536\n",
      "          Linear-124            [-1, 197, 3072]       2,362,368\n",
      "            GELU-125            [-1, 197, 3072]               0\n",
      "         Dropout-126            [-1, 197, 3072]               0\n",
      "          Linear-127             [-1, 197, 768]       2,360,064\n",
      "         Dropout-128             [-1, 197, 768]               0\n",
      "             Mlp-129             [-1, 197, 768]               0\n",
      "        Identity-130             [-1, 197, 768]               0\n",
      "           Block-131             [-1, 197, 768]               0\n",
      "       LayerNorm-132             [-1, 197, 768]           1,536\n",
      "          Linear-133            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-134         [-1, 12, 197, 197]               0\n",
      "          Linear-135             [-1, 197, 768]         590,592\n",
      "         Dropout-136             [-1, 197, 768]               0\n",
      "MultiHeadAttentionTimm-137             [-1, 197, 768]               0\n",
      "        Identity-138             [-1, 197, 768]               0\n",
      "       LayerNorm-139             [-1, 197, 768]           1,536\n",
      "          Linear-140            [-1, 197, 3072]       2,362,368\n",
      "            GELU-141            [-1, 197, 3072]               0\n",
      "         Dropout-142            [-1, 197, 3072]               0\n",
      "          Linear-143             [-1, 197, 768]       2,360,064\n",
      "         Dropout-144             [-1, 197, 768]               0\n",
      "             Mlp-145             [-1, 197, 768]               0\n",
      "        Identity-146             [-1, 197, 768]               0\n",
      "           Block-147             [-1, 197, 768]               0\n",
      "       LayerNorm-148             [-1, 197, 768]           1,536\n",
      "          Linear-149            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-150         [-1, 12, 197, 197]               0\n",
      "          Linear-151             [-1, 197, 768]         590,592\n",
      "         Dropout-152             [-1, 197, 768]               0\n",
      "MultiHeadAttentionTimm-153             [-1, 197, 768]               0\n",
      "        Identity-154             [-1, 197, 768]               0\n",
      "       LayerNorm-155             [-1, 197, 768]           1,536\n",
      "          Linear-156            [-1, 197, 3072]       2,362,368\n",
      "            GELU-157            [-1, 197, 3072]               0\n",
      "         Dropout-158            [-1, 197, 3072]               0\n",
      "          Linear-159             [-1, 197, 768]       2,360,064\n",
      "         Dropout-160             [-1, 197, 768]               0\n",
      "             Mlp-161             [-1, 197, 768]               0\n",
      "        Identity-162             [-1, 197, 768]               0\n",
      "           Block-163             [-1, 197, 768]               0\n",
      "       LayerNorm-164             [-1, 197, 768]           1,536\n",
      "          Linear-165            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-166         [-1, 12, 197, 197]               0\n",
      "          Linear-167             [-1, 197, 768]         590,592\n",
      "         Dropout-168             [-1, 197, 768]               0\n",
      "MultiHeadAttentionTimm-169             [-1, 197, 768]               0\n",
      "        Identity-170             [-1, 197, 768]               0\n",
      "       LayerNorm-171             [-1, 197, 768]           1,536\n",
      "          Linear-172            [-1, 197, 3072]       2,362,368\n",
      "            GELU-173            [-1, 197, 3072]               0\n",
      "         Dropout-174            [-1, 197, 3072]               0\n",
      "          Linear-175             [-1, 197, 768]       2,360,064\n",
      "         Dropout-176             [-1, 197, 768]               0\n",
      "             Mlp-177             [-1, 197, 768]               0\n",
      "        Identity-178             [-1, 197, 768]               0\n",
      "           Block-179             [-1, 197, 768]               0\n",
      "       LayerNorm-180             [-1, 197, 768]           1,536\n",
      "          Linear-181            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-182         [-1, 12, 197, 197]               0\n",
      "          Linear-183             [-1, 197, 768]         590,592\n",
      "         Dropout-184             [-1, 197, 768]               0\n",
      "MultiHeadAttentionTimm-185             [-1, 197, 768]               0\n",
      "        Identity-186             [-1, 197, 768]               0\n",
      "       LayerNorm-187             [-1, 197, 768]           1,536\n",
      "          Linear-188            [-1, 197, 3072]       2,362,368\n",
      "            GELU-189            [-1, 197, 3072]               0\n",
      "         Dropout-190            [-1, 197, 3072]               0\n",
      "          Linear-191             [-1, 197, 768]       2,360,064\n",
      "         Dropout-192             [-1, 197, 768]               0\n",
      "             Mlp-193             [-1, 197, 768]               0\n",
      "        Identity-194             [-1, 197, 768]               0\n",
      "           Block-195             [-1, 197, 768]               0\n",
      "       LayerNorm-196             [-1, 197, 768]           1,536\n",
      "          Linear-197                 [-1, 1000]         769,000\n",
      "VisionTransformer-198                 [-1, 1000]               0\n",
      "================================================================\n",
      "Total params: 86,415,592\n",
      "Trainable params: 86,415,592\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 407.40\n",
      "Params size (MB): 329.65\n",
      "Estimated Total Size (MB): 737.62\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(86415592), tensor(86415592), tensor(329.6493), tensor(737.6217))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "from glasses.models.classification.vit import ViT\n",
    "\n",
    "summary(VisionTransformer(), (3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p16_224-80ecf9dd.pth\" to /home/zuppif/.cache/torch/hub/checkpoints/jx_vit_base_p16_224-80ecf9dd.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 14, 14]         590,592\n",
      "        PatchEmbed-2             [-1, 196, 768]               0\n",
      "           Dropout-3             [-1, 197, 768]               0\n",
      "         LayerNorm-4             [-1, 197, 768]           1,536\n",
      "            Linear-5            [-1, 197, 2304]       1,771,776\n",
      "           Dropout-6         [-1, 12, 197, 197]               0\n",
      "            Linear-7             [-1, 197, 768]         590,592\n",
      "           Dropout-8             [-1, 197, 768]               0\n",
      "         Attention-9             [-1, 197, 768]               0\n",
      "         Identity-10             [-1, 197, 768]               0\n",
      "        LayerNorm-11             [-1, 197, 768]           1,536\n",
      "           Linear-12            [-1, 197, 3072]       2,362,368\n",
      "             GELU-13            [-1, 197, 3072]               0\n",
      "          Dropout-14            [-1, 197, 3072]               0\n",
      "           Linear-15             [-1, 197, 768]       2,360,064\n",
      "          Dropout-16             [-1, 197, 768]               0\n",
      "              Mlp-17             [-1, 197, 768]               0\n",
      "         Identity-18             [-1, 197, 768]               0\n",
      "            Block-19             [-1, 197, 768]               0\n",
      "        LayerNorm-20             [-1, 197, 768]           1,536\n",
      "           Linear-21            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-22         [-1, 12, 197, 197]               0\n",
      "           Linear-23             [-1, 197, 768]         590,592\n",
      "          Dropout-24             [-1, 197, 768]               0\n",
      "        Attention-25             [-1, 197, 768]               0\n",
      "         Identity-26             [-1, 197, 768]               0\n",
      "        LayerNorm-27             [-1, 197, 768]           1,536\n",
      "           Linear-28            [-1, 197, 3072]       2,362,368\n",
      "             GELU-29            [-1, 197, 3072]               0\n",
      "          Dropout-30            [-1, 197, 3072]               0\n",
      "           Linear-31             [-1, 197, 768]       2,360,064\n",
      "          Dropout-32             [-1, 197, 768]               0\n",
      "              Mlp-33             [-1, 197, 768]               0\n",
      "         Identity-34             [-1, 197, 768]               0\n",
      "            Block-35             [-1, 197, 768]               0\n",
      "        LayerNorm-36             [-1, 197, 768]           1,536\n",
      "           Linear-37            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-38         [-1, 12, 197, 197]               0\n",
      "           Linear-39             [-1, 197, 768]         590,592\n",
      "          Dropout-40             [-1, 197, 768]               0\n",
      "        Attention-41             [-1, 197, 768]               0\n",
      "         Identity-42             [-1, 197, 768]               0\n",
      "        LayerNorm-43             [-1, 197, 768]           1,536\n",
      "           Linear-44            [-1, 197, 3072]       2,362,368\n",
      "             GELU-45            [-1, 197, 3072]               0\n",
      "          Dropout-46            [-1, 197, 3072]               0\n",
      "           Linear-47             [-1, 197, 768]       2,360,064\n",
      "          Dropout-48             [-1, 197, 768]               0\n",
      "              Mlp-49             [-1, 197, 768]               0\n",
      "         Identity-50             [-1, 197, 768]               0\n",
      "            Block-51             [-1, 197, 768]               0\n",
      "        LayerNorm-52             [-1, 197, 768]           1,536\n",
      "           Linear-53            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-54         [-1, 12, 197, 197]               0\n",
      "           Linear-55             [-1, 197, 768]         590,592\n",
      "          Dropout-56             [-1, 197, 768]               0\n",
      "        Attention-57             [-1, 197, 768]               0\n",
      "         Identity-58             [-1, 197, 768]               0\n",
      "        LayerNorm-59             [-1, 197, 768]           1,536\n",
      "           Linear-60            [-1, 197, 3072]       2,362,368\n",
      "             GELU-61            [-1, 197, 3072]               0\n",
      "          Dropout-62            [-1, 197, 3072]               0\n",
      "           Linear-63             [-1, 197, 768]       2,360,064\n",
      "          Dropout-64             [-1, 197, 768]               0\n",
      "              Mlp-65             [-1, 197, 768]               0\n",
      "         Identity-66             [-1, 197, 768]               0\n",
      "            Block-67             [-1, 197, 768]               0\n",
      "        LayerNorm-68             [-1, 197, 768]           1,536\n",
      "           Linear-69            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-70         [-1, 12, 197, 197]               0\n",
      "           Linear-71             [-1, 197, 768]         590,592\n",
      "          Dropout-72             [-1, 197, 768]               0\n",
      "        Attention-73             [-1, 197, 768]               0\n",
      "         Identity-74             [-1, 197, 768]               0\n",
      "        LayerNorm-75             [-1, 197, 768]           1,536\n",
      "           Linear-76            [-1, 197, 3072]       2,362,368\n",
      "             GELU-77            [-1, 197, 3072]               0\n",
      "          Dropout-78            [-1, 197, 3072]               0\n",
      "           Linear-79             [-1, 197, 768]       2,360,064\n",
      "          Dropout-80             [-1, 197, 768]               0\n",
      "              Mlp-81             [-1, 197, 768]               0\n",
      "         Identity-82             [-1, 197, 768]               0\n",
      "            Block-83             [-1, 197, 768]               0\n",
      "        LayerNorm-84             [-1, 197, 768]           1,536\n",
      "           Linear-85            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-86         [-1, 12, 197, 197]               0\n",
      "           Linear-87             [-1, 197, 768]         590,592\n",
      "          Dropout-88             [-1, 197, 768]               0\n",
      "        Attention-89             [-1, 197, 768]               0\n",
      "         Identity-90             [-1, 197, 768]               0\n",
      "        LayerNorm-91             [-1, 197, 768]           1,536\n",
      "           Linear-92            [-1, 197, 3072]       2,362,368\n",
      "             GELU-93            [-1, 197, 3072]               0\n",
      "          Dropout-94            [-1, 197, 3072]               0\n",
      "           Linear-95             [-1, 197, 768]       2,360,064\n",
      "          Dropout-96             [-1, 197, 768]               0\n",
      "              Mlp-97             [-1, 197, 768]               0\n",
      "         Identity-98             [-1, 197, 768]               0\n",
      "            Block-99             [-1, 197, 768]               0\n",
      "       LayerNorm-100             [-1, 197, 768]           1,536\n",
      "          Linear-101            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-102         [-1, 12, 197, 197]               0\n",
      "          Linear-103             [-1, 197, 768]         590,592\n",
      "         Dropout-104             [-1, 197, 768]               0\n",
      "       Attention-105             [-1, 197, 768]               0\n",
      "        Identity-106             [-1, 197, 768]               0\n",
      "       LayerNorm-107             [-1, 197, 768]           1,536\n",
      "          Linear-108            [-1, 197, 3072]       2,362,368\n",
      "            GELU-109            [-1, 197, 3072]               0\n",
      "         Dropout-110            [-1, 197, 3072]               0\n",
      "          Linear-111             [-1, 197, 768]       2,360,064\n",
      "         Dropout-112             [-1, 197, 768]               0\n",
      "             Mlp-113             [-1, 197, 768]               0\n",
      "        Identity-114             [-1, 197, 768]               0\n",
      "           Block-115             [-1, 197, 768]               0\n",
      "       LayerNorm-116             [-1, 197, 768]           1,536\n",
      "          Linear-117            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-118         [-1, 12, 197, 197]               0\n",
      "          Linear-119             [-1, 197, 768]         590,592\n",
      "         Dropout-120             [-1, 197, 768]               0\n",
      "       Attention-121             [-1, 197, 768]               0\n",
      "        Identity-122             [-1, 197, 768]               0\n",
      "       LayerNorm-123             [-1, 197, 768]           1,536\n",
      "          Linear-124            [-1, 197, 3072]       2,362,368\n",
      "            GELU-125            [-1, 197, 3072]               0\n",
      "         Dropout-126            [-1, 197, 3072]               0\n",
      "          Linear-127             [-1, 197, 768]       2,360,064\n",
      "         Dropout-128             [-1, 197, 768]               0\n",
      "             Mlp-129             [-1, 197, 768]               0\n",
      "        Identity-130             [-1, 197, 768]               0\n",
      "           Block-131             [-1, 197, 768]               0\n",
      "       LayerNorm-132             [-1, 197, 768]           1,536\n",
      "          Linear-133            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-134         [-1, 12, 197, 197]               0\n",
      "          Linear-135             [-1, 197, 768]         590,592\n",
      "         Dropout-136             [-1, 197, 768]               0\n",
      "       Attention-137             [-1, 197, 768]               0\n",
      "        Identity-138             [-1, 197, 768]               0\n",
      "       LayerNorm-139             [-1, 197, 768]           1,536\n",
      "          Linear-140            [-1, 197, 3072]       2,362,368\n",
      "            GELU-141            [-1, 197, 3072]               0\n",
      "         Dropout-142            [-1, 197, 3072]               0\n",
      "          Linear-143             [-1, 197, 768]       2,360,064\n",
      "         Dropout-144             [-1, 197, 768]               0\n",
      "             Mlp-145             [-1, 197, 768]               0\n",
      "        Identity-146             [-1, 197, 768]               0\n",
      "           Block-147             [-1, 197, 768]               0\n",
      "       LayerNorm-148             [-1, 197, 768]           1,536\n",
      "          Linear-149            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-150         [-1, 12, 197, 197]               0\n",
      "          Linear-151             [-1, 197, 768]         590,592\n",
      "         Dropout-152             [-1, 197, 768]               0\n",
      "       Attention-153             [-1, 197, 768]               0\n",
      "        Identity-154             [-1, 197, 768]               0\n",
      "       LayerNorm-155             [-1, 197, 768]           1,536\n",
      "          Linear-156            [-1, 197, 3072]       2,362,368\n",
      "            GELU-157            [-1, 197, 3072]               0\n",
      "         Dropout-158            [-1, 197, 3072]               0\n",
      "          Linear-159             [-1, 197, 768]       2,360,064\n",
      "         Dropout-160             [-1, 197, 768]               0\n",
      "             Mlp-161             [-1, 197, 768]               0\n",
      "        Identity-162             [-1, 197, 768]               0\n",
      "           Block-163             [-1, 197, 768]               0\n",
      "       LayerNorm-164             [-1, 197, 768]           1,536\n",
      "          Linear-165            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-166         [-1, 12, 197, 197]               0\n",
      "          Linear-167             [-1, 197, 768]         590,592\n",
      "         Dropout-168             [-1, 197, 768]               0\n",
      "       Attention-169             [-1, 197, 768]               0\n",
      "        Identity-170             [-1, 197, 768]               0\n",
      "       LayerNorm-171             [-1, 197, 768]           1,536\n",
      "          Linear-172            [-1, 197, 3072]       2,362,368\n",
      "            GELU-173            [-1, 197, 3072]               0\n",
      "         Dropout-174            [-1, 197, 3072]               0\n",
      "          Linear-175             [-1, 197, 768]       2,360,064\n",
      "         Dropout-176             [-1, 197, 768]               0\n",
      "             Mlp-177             [-1, 197, 768]               0\n",
      "        Identity-178             [-1, 197, 768]               0\n",
      "           Block-179             [-1, 197, 768]               0\n",
      "       LayerNorm-180             [-1, 197, 768]           1,536\n",
      "          Linear-181            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-182         [-1, 12, 197, 197]               0\n",
      "          Linear-183             [-1, 197, 768]         590,592\n",
      "         Dropout-184             [-1, 197, 768]               0\n",
      "       Attention-185             [-1, 197, 768]               0\n",
      "        Identity-186             [-1, 197, 768]               0\n",
      "       LayerNorm-187             [-1, 197, 768]           1,536\n",
      "          Linear-188            [-1, 197, 3072]       2,362,368\n",
      "            GELU-189            [-1, 197, 3072]               0\n",
      "         Dropout-190            [-1, 197, 3072]               0\n",
      "          Linear-191             [-1, 197, 768]       2,360,064\n",
      "         Dropout-192             [-1, 197, 768]               0\n",
      "             Mlp-193             [-1, 197, 768]               0\n",
      "        Identity-194             [-1, 197, 768]               0\n",
      "           Block-195             [-1, 197, 768]               0\n",
      "       LayerNorm-196             [-1, 197, 768]           1,536\n",
      "          Linear-197                 [-1, 1000]         769,000\n",
      "VisionTransformer-198                 [-1, 1000]               0\n",
      "================================================================\n",
      "Total params: 86,415,592\n",
      "Trainable params: 86,415,592\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 407.40\n",
      "Params size (MB): 329.65\n",
      "Estimated Total Size (MB): 737.62\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(86415592), tensor(86415592), tensor(329.6493), tensor(737.6217))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(timm.create_model('vit_base_patch16_224', pretrained=True), (3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "root = Path('/home/zuppif/Documents/glasses-weights/')\n",
    "\n",
    "models = map(lambda x: x.stem, root.glob('*.pth'))\n",
    "\n",
    "for model in models:\n",
    "    print(f\"'{model}': BasicUrlHandler('https://github.com/FrancescoSaverioZuppichini/glasses-weights/blob/main/{model}.pth?raw=true'),\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glasses.utils.Tracker import Tracker\n",
    "import torch\n",
    "\n",
    "def trace(model, x):\n",
    "    return Tracker(model)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16)),\n",
       " Rearrange('b e (h) (w) -> b (h w) e'),\n",
       " LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=2304, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=3072, bias=True),\n",
       " GELU(),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " Linear(in_features=3072, out_features=768, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=2304, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=3072, bias=True),\n",
       " GELU(),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " Linear(in_features=3072, out_features=768, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=2304, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=3072, bias=True),\n",
       " GELU(),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " Linear(in_features=3072, out_features=768, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=2304, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=3072, bias=True),\n",
       " GELU(),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " Linear(in_features=3072, out_features=768, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=2304, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=3072, bias=True),\n",
       " GELU(),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " Linear(in_features=3072, out_features=768, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=2304, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=3072, bias=True),\n",
       " GELU(),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " Linear(in_features=3072, out_features=768, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=2304, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=3072, bias=True),\n",
       " GELU(),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " Linear(in_features=3072, out_features=768, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=2304, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=3072, bias=True),\n",
       " GELU(),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " Linear(in_features=3072, out_features=768, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=2304, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=3072, bias=True),\n",
       " GELU(),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " Linear(in_features=3072, out_features=768, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=2304, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=3072, bias=True),\n",
       " GELU(),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " Linear(in_features=3072, out_features=768, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=2304, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=3072, bias=True),\n",
       " GELU(),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " Linear(in_features=3072, out_features=768, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=2304, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=3072, bias=True),\n",
       " GELU(),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " Linear(in_features=3072, out_features=768, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " Reduce('b n e -> b e', 'mean'),\n",
       " LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=1000, bias=True)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((1, 3, 224, 224))\n",
    "\n",
    "trace(ViT(), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((1, 3, 224, 224))\n",
    "\n",
    "trace(timm.create_model('vit_base_patch16_224', pretrained=True), x).traced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16)),\n",
       " LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=2304, bias=True),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=3072, bias=True),\n",
       " Linear(in_features=3072, out_features=768, bias=True),\n",
       " LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=2304, bias=True),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=3072, bias=True),\n",
       " Linear(in_features=3072, out_features=768, bias=True),\n",
       " LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=2304, bias=True),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=3072, bias=True),\n",
       " Linear(in_features=3072, out_features=768, bias=True),\n",
       " LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=2304, bias=True),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=3072, bias=True),\n",
       " Linear(in_features=3072, out_features=768, bias=True),\n",
       " LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=2304, bias=True),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=3072, bias=True),\n",
       " Linear(in_features=3072, out_features=768, bias=True),\n",
       " LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=2304, bias=True),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=3072, bias=True),\n",
       " Linear(in_features=3072, out_features=768, bias=True),\n",
       " LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=2304, bias=True),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=3072, bias=True),\n",
       " Linear(in_features=3072, out_features=768, bias=True),\n",
       " LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=2304, bias=True),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=3072, bias=True),\n",
       " Linear(in_features=3072, out_features=768, bias=True),\n",
       " LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=2304, bias=True),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=3072, bias=True),\n",
       " Linear(in_features=3072, out_features=768, bias=True),\n",
       " LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=2304, bias=True),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=3072, bias=True),\n",
       " Linear(in_features=3072, out_features=768, bias=True),\n",
       " LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=2304, bias=True),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=3072, bias=True),\n",
       " Linear(in_features=3072, out_features=768, bias=True),\n",
       " LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=2304, bias=True),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=3072, bias=True),\n",
       " Linear(in_features=3072, out_features=768, bias=True),\n",
       " LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=1000, bias=True)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace(timm.create_model('vit_base_patch16_224', pretrained=True), x).parametrized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViT(\n",
       "  (0): PatchEmbedding(\n",
       "    (projection): Sequential(\n",
       "      (0): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (1): Rearrange('b e (h) (w) -> b (h w) e')\n",
       "    )\n",
       "  )\n",
       "  (1): TransformerEncoder(\n",
       "    (0): TransformerEncoderBlock(\n",
       "      (0): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): MultiHeadAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (att_drop): Dropout(p=0.0, inplace=False)\n",
       "            (projection): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): FeedForwardBlock(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerEncoderBlock(\n",
       "      (0): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): MultiHeadAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (att_drop): Dropout(p=0.0, inplace=False)\n",
       "            (projection): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): FeedForwardBlock(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerEncoderBlock(\n",
       "      (0): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): MultiHeadAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (att_drop): Dropout(p=0.0, inplace=False)\n",
       "            (projection): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): FeedForwardBlock(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerEncoderBlock(\n",
       "      (0): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): MultiHeadAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (att_drop): Dropout(p=0.0, inplace=False)\n",
       "            (projection): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): FeedForwardBlock(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerEncoderBlock(\n",
       "      (0): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): MultiHeadAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (att_drop): Dropout(p=0.0, inplace=False)\n",
       "            (projection): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): FeedForwardBlock(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerEncoderBlock(\n",
       "      (0): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): MultiHeadAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (att_drop): Dropout(p=0.0, inplace=False)\n",
       "            (projection): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): FeedForwardBlock(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): TransformerEncoderBlock(\n",
       "      (0): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): MultiHeadAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (att_drop): Dropout(p=0.0, inplace=False)\n",
       "            (projection): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): FeedForwardBlock(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): TransformerEncoderBlock(\n",
       "      (0): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): MultiHeadAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (att_drop): Dropout(p=0.0, inplace=False)\n",
       "            (projection): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): FeedForwardBlock(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): TransformerEncoderBlock(\n",
       "      (0): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): MultiHeadAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (att_drop): Dropout(p=0.0, inplace=False)\n",
       "            (projection): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): FeedForwardBlock(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): TransformerEncoderBlock(\n",
       "      (0): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): MultiHeadAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (att_drop): Dropout(p=0.0, inplace=False)\n",
       "            (projection): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): FeedForwardBlock(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): TransformerEncoderBlock(\n",
       "      (0): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): MultiHeadAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (att_drop): Dropout(p=0.0, inplace=False)\n",
       "            (projection): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): FeedForwardBlock(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): TransformerEncoderBlock(\n",
       "      (0): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): MultiHeadAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (att_drop): Dropout(p=0.0, inplace=False)\n",
       "            (projection): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): FeedForwardBlock(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): ClassificationHead(\n",
       "    (0): Reduce('b n e -> b e', 'mean')\n",
       "    (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): Linear(in_features=768, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ViT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm.create_model('vit_base_patch16_224', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
