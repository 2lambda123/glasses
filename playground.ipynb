{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glasses.utils.PretrainedWeightsProvider import PretrainedWeightsProvider\n",
    "from glasses.models import AutoModel\n",
    "from torch import nn\n",
    "\n",
    "class Dummy(nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super().__init__(nn.Conv2d(3, 32, kernel_size=3), \n",
    "                        nn.Conv2d(32, 64, kernel_size=3))\n",
    "\n",
    "dummy = Dummy()\n",
    "        \n",
    "provider = PretrainedWeightsProvider()\n",
    "# state_dict = provider['dummy']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                Models                 </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Name                   </span>┃<span style=\"font-weight: bold\"> Pretrained </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "│ resnet18               │ true       │\n",
       "│ resnet26               │ true       │\n",
       "│ resnet26d              │ true       │\n",
       "│ resnet34               │ true       │\n",
       "│ resnet34d              │ true       │\n",
       "│ resnet50               │ true       │\n",
       "│ resnet50d              │ true       │\n",
       "│ resnet101              │ true       │\n",
       "│ resnet152              │ true       │\n",
       "│ resnet200              │ false      │\n",
       "│ se_resnet18            │ false      │\n",
       "│ se_resnet34            │ false      │\n",
       "│ se_resnet50            │ false      │\n",
       "│ se_resnet101           │ false      │\n",
       "│ se_resnet152           │ false      │\n",
       "│ cse_resnet18           │ false      │\n",
       "│ cse_resnet34           │ false      │\n",
       "│ cse_resnet50           │ true       │\n",
       "│ cse_resnet101          │ false      │\n",
       "│ cse_resnet152          │ false      │\n",
       "│ resnext50_32x4d        │ true       │\n",
       "│ resnext101_32x8d       │ true       │\n",
       "│ resnext101_32x16d      │ false      │\n",
       "│ resnext101_32x32d      │ false      │\n",
       "│ resnext101_32x48d      │ false      │\n",
       "│ regnetx_002            │ true       │\n",
       "│ regnetx_004            │ true       │\n",
       "│ regnetx_006            │ true       │\n",
       "│ regnetx_008            │ true       │\n",
       "│ regnetx_016            │ true       │\n",
       "│ regnetx_032            │ true       │\n",
       "│ regnety_002            │ true       │\n",
       "│ regnety_004            │ true       │\n",
       "│ regnety_006            │ true       │\n",
       "│ regnety_008            │ true       │\n",
       "│ regnety_016            │ true       │\n",
       "│ regnety_032            │ true       │\n",
       "│ resnest14d             │ false      │\n",
       "│ resnest26d             │ false      │\n",
       "│ resnest50d             │ false      │\n",
       "│ resnest50d_1s4x24d     │ false      │\n",
       "│ resnest50d_4s2x40d     │ false      │\n",
       "│ resnest101e            │ false      │\n",
       "│ resnest200e            │ false      │\n",
       "│ resnest269e            │ false      │\n",
       "│ wide_resnet50_2        │ true       │\n",
       "│ wide_resnet101_2       │ true       │\n",
       "│ densenet121            │ true       │\n",
       "│ densenet169            │ true       │\n",
       "│ densenet201            │ true       │\n",
       "│ densenet161            │ true       │\n",
       "│ fishnet99              │ false      │\n",
       "│ fishnet150             │ false      │\n",
       "│ vgg11                  │ true       │\n",
       "│ vgg13                  │ true       │\n",
       "│ vgg16                  │ true       │\n",
       "│ vgg19                  │ true       │\n",
       "│ vgg11_bn               │ true       │\n",
       "│ vgg13_bn               │ true       │\n",
       "│ vgg16_bn               │ true       │\n",
       "│ vgg19_bn               │ true       │\n",
       "│ efficientnet_b0        │ true       │\n",
       "│ efficientnet_b1        │ true       │\n",
       "│ efficientnet_b2        │ true       │\n",
       "│ efficientnet_b3        │ true       │\n",
       "│ efficientnet_b4        │ false      │\n",
       "│ efficientnet_b5        │ false      │\n",
       "│ efficientnet_b6        │ false      │\n",
       "│ efficientnet_b7        │ false      │\n",
       "│ efficientnet_b8        │ false      │\n",
       "│ efficientnet_l2        │ false      │\n",
       "│ efficientnet_lite0     │ false      │\n",
       "│ efficientnet_lite1     │ false      │\n",
       "│ efficientnet_lite2     │ false      │\n",
       "│ efficientnet_lite3     │ false      │\n",
       "│ efficientnet_lite4     │ false      │\n",
       "│ vit_small_patch16_224  │ false      │\n",
       "│ vit_base_patch16_224   │ false      │\n",
       "│ vit_base_patch16_384   │ false      │\n",
       "│ vit_base_patch32_384   │ false      │\n",
       "│ vit_huge_patch16_224   │ false      │\n",
       "│ vit_huge_patch32_384   │ false      │\n",
       "│ vit_large_patch16_224  │ false      │\n",
       "│ vit_large_patch16_384  │ false      │\n",
       "│ vit_large_patch32_384  │ false      │\n",
       "│ deit_tiny_patch16_224  │ true       │\n",
       "│ deit_small_patch16_224 │ true       │\n",
       "│ deit_base_patch16_224  │ false      │\n",
       "│ deit_base_patch16_384  │ false      │\n",
       "│ mobilenetv2            │ false      │\n",
       "│ unet                   │ false      │\n",
       "└────────────────────────┴────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.table.Table at 0x131274d90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AutoModel.models_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glasses.models import AutoModel, AutoConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViT(\n",
       "  (embedding): PatchEmbedding(\n",
       "    (projection): Sequential(\n",
       "      (0): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (1): Rearrange('b e (h) (w) -> b (h w) e')\n",
       "    )\n",
       "    (tokens): ViTTokens()\n",
       "  )\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderBlock(\n",
       "        (0): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): MultiHeadAttention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (att_drop): Dropout(p=0.0, inplace=False)\n",
       "              (projection): Sequential(\n",
       "                (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForwardBlock(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): TransformerEncoderBlock(\n",
       "        (0): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): MultiHeadAttention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (att_drop): Dropout(p=0.0, inplace=False)\n",
       "              (projection): Sequential(\n",
       "                (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForwardBlock(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): TransformerEncoderBlock(\n",
       "        (0): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): MultiHeadAttention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (att_drop): Dropout(p=0.0, inplace=False)\n",
       "              (projection): Sequential(\n",
       "                (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForwardBlock(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): TransformerEncoderBlock(\n",
       "        (0): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): MultiHeadAttention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (att_drop): Dropout(p=0.0, inplace=False)\n",
       "              (projection): Sequential(\n",
       "                (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForwardBlock(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): TransformerEncoderBlock(\n",
       "        (0): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): MultiHeadAttention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (att_drop): Dropout(p=0.0, inplace=False)\n",
       "              (projection): Sequential(\n",
       "                (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForwardBlock(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): TransformerEncoderBlock(\n",
       "        (0): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): MultiHeadAttention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (att_drop): Dropout(p=0.0, inplace=False)\n",
       "              (projection): Sequential(\n",
       "                (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForwardBlock(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): TransformerEncoderBlock(\n",
       "        (0): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): MultiHeadAttention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (att_drop): Dropout(p=0.0, inplace=False)\n",
       "              (projection): Sequential(\n",
       "                (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForwardBlock(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): TransformerEncoderBlock(\n",
       "        (0): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): MultiHeadAttention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (att_drop): Dropout(p=0.0, inplace=False)\n",
       "              (projection): Sequential(\n",
       "                (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForwardBlock(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): TransformerEncoderBlock(\n",
       "        (0): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): MultiHeadAttention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (att_drop): Dropout(p=0.0, inplace=False)\n",
       "              (projection): Sequential(\n",
       "                (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForwardBlock(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): TransformerEncoderBlock(\n",
       "        (0): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): MultiHeadAttention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (att_drop): Dropout(p=0.0, inplace=False)\n",
       "              (projection): Sequential(\n",
       "                (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForwardBlock(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): TransformerEncoderBlock(\n",
       "        (0): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): MultiHeadAttention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (att_drop): Dropout(p=0.0, inplace=False)\n",
       "              (projection): Sequential(\n",
       "                (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForwardBlock(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): TransformerEncoderBlock(\n",
       "        (0): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): MultiHeadAttention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (att_drop): Dropout(p=0.0, inplace=False)\n",
       "              (projection): Sequential(\n",
       "                (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForwardBlock(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (head): ViTClassificationHead(\n",
       "    (pool): Lambda()\n",
       "    (fc): Linear(in_features=768, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModel.from_name('vit_base_patch16_224')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[-1.0900e+00,  5.9232e-01, -1.3475e+00, -8.8945e-03,  2.8032e-01,\n",
       "          -2.5296e+00, -1.2493e+00,  1.9137e-01, -8.4429e-01, -3.9187e-01,\n",
       "          -1.3274e-01,  1.9846e-01,  2.0594e-01,  3.8294e-01,  1.4616e+00,\n",
       "           7.3132e-02, -2.6978e-02,  1.2118e-01, -7.7703e-01,  7.8420e-01,\n",
       "          -1.9390e+00,  2.5998e+00,  1.5004e+00, -1.4453e+00, -1.0452e+00,\n",
       "          -3.9081e-01,  6.5189e-02, -6.0795e-01, -6.4043e-01, -1.3401e+00,\n",
       "           5.5202e-01, -6.9359e-01,  1.5587e+00,  6.1924e-01, -4.4311e-02,\n",
       "           1.2512e+00,  8.5140e-01,  2.0568e+00,  2.7636e-01,  4.0293e-01,\n",
       "           1.1603e+00,  1.2421e+00,  2.1889e-01, -2.3521e-01, -6.4439e-01,\n",
       "           9.5910e-01,  1.3049e+00,  2.0507e+00, -1.1315e-01,  1.2872e+00,\n",
       "           4.0509e-02, -1.9550e-01,  3.4365e-01, -3.1178e-01, -1.2283e+00,\n",
       "          -3.5690e-02,  5.9273e-02,  8.8650e-01,  8.9549e-01, -5.8535e-01,\n",
       "           6.3476e-01, -7.6959e-01,  4.5018e-01, -6.3732e-01, -5.4927e-01,\n",
       "           4.6874e-01, -1.5319e+00, -1.8565e+00,  5.1462e-01, -1.0542e+00,\n",
       "          -9.3327e-01, -1.0785e-01, -8.1992e-01, -1.8084e+00, -7.4689e-01,\n",
       "          -4.3132e-01,  2.6189e-01,  7.5025e-01, -3.1078e-02, -1.2794e-01,\n",
       "           1.2726e-01,  6.6611e-03,  1.6012e+00,  3.2580e-01,  8.5679e-02,\n",
       "          -1.7322e+00,  3.5419e-01, -9.8321e-01, -2.2448e+00, -4.1546e-01,\n",
       "           2.1053e-01, -1.3528e-01,  1.5467e+00, -6.7335e-01, -4.7675e-01,\n",
       "          -9.3845e-01, -5.5074e-01, -3.2094e-01, -5.1937e-01, -8.1332e-01,\n",
       "           6.1044e-01,  2.9012e+00,  4.9418e-01,  2.4208e+00,  5.6871e-01,\n",
       "          -2.5625e-02,  1.0912e+00, -2.9923e+00,  7.9205e-02,  1.4133e+00,\n",
       "          -9.6248e-01, -1.4989e+00,  5.8437e-01,  1.0214e+00,  8.5368e-02,\n",
       "          -7.3181e-01,  6.2549e-01, -3.3619e-02,  5.0234e-01,  4.9483e-01,\n",
       "           1.2996e-01, -7.6516e-01, -4.5927e-01,  5.5060e-01,  1.6467e+00,\n",
       "          -4.5678e-01,  1.0321e+00,  3.5360e-01, -2.2235e+00, -4.9513e-01,\n",
       "           1.3871e+00, -8.5038e-02,  6.9938e-02,  2.5721e-01,  3.2807e-01,\n",
       "          -8.7622e-01,  4.5472e-02,  2.2461e-01,  4.4584e-01,  7.8802e-01,\n",
       "           4.3910e-03,  9.2062e-01, -1.3662e+00, -5.1785e-02, -9.7134e-02,\n",
       "          -5.5681e-01,  6.4208e-01, -2.1335e+00, -9.4939e-01, -1.0725e-01,\n",
       "           7.9762e-01, -6.6328e-01, -3.7971e-01,  4.1706e-01, -4.2290e-01,\n",
       "          -5.3523e-01, -9.5426e-01, -1.0227e-01, -1.7574e-01,  1.2518e-01,\n",
       "          -4.2900e-02,  1.0310e+00, -1.0658e+00, -2.9768e+00,  9.4370e-02,\n",
       "           8.7603e-02, -2.9738e-01,  4.9579e-01,  9.3580e-01, -1.2140e+00,\n",
       "           6.7486e-01, -9.5397e-01, -1.8202e+00,  2.5582e-01, -1.3199e+00,\n",
       "           7.5853e-01,  5.7308e-01, -9.9339e-01, -8.8300e-01, -3.8961e-01,\n",
       "          -1.7053e-01,  1.3032e+00, -1.2577e+00, -1.6207e+00, -1.8583e+00,\n",
       "          -1.3336e-01,  4.3480e-01,  1.0299e+00,  1.4491e+00,  6.5932e-01,\n",
       "          -3.7181e-02, -2.1047e+00, -9.0154e-01,  6.0392e-02,  1.4666e+00,\n",
       "           1.9248e+00,  1.1382e+00,  1.5762e+00,  6.4158e-01,  1.5410e+00,\n",
       "           1.5976e+00, -2.0962e+00, -1.3901e+00, -1.0869e+00, -1.1602e+00,\n",
       "           3.5641e-01,  1.9545e+00, -5.6843e-01,  1.9785e-01, -1.2140e+00,\n",
       "           6.9817e-01, -8.3747e-01, -3.0752e-01, -6.9357e-02, -3.1526e-01,\n",
       "           4.8144e-03, -2.5217e-01,  1.5838e+00,  1.8952e+00,  9.2576e-01,\n",
       "           7.2924e-01,  7.4291e-01,  2.9076e-01, -1.0978e+00,  8.6329e-01,\n",
       "          -1.0081e+00, -1.0405e+00, -9.2291e-01, -1.3979e-01,  9.8224e-02,\n",
       "          -4.4015e-01, -6.2625e-01,  2.7156e-01,  1.3953e+00,  1.6370e-01,\n",
       "           1.1385e+00,  9.6359e-01,  5.0721e-01,  1.9046e-01, -2.0339e+00,\n",
       "          -1.0061e-01, -7.7351e-01, -7.8947e-01,  1.4525e+00,  1.8582e+00,\n",
       "          -9.0120e-01, -5.6805e-01, -7.2366e-01, -1.1086e+00, -1.6612e-01,\n",
       "           2.5675e-02, -4.1961e-01, -7.9940e-01,  1.5448e+00,  1.5532e+00,\n",
       "          -1.0710e+00, -8.1679e-01,  9.4079e-01,  9.5346e-02, -1.2771e-01,\n",
       "          -1.2069e+00,  3.0788e+00, -5.8542e-02,  1.4630e+00,  1.0304e-01,\n",
       "           2.1589e+00, -1.4353e+00,  1.3017e+00, -6.6899e-01,  8.1822e-02,\n",
       "          -9.0460e-01,  1.3085e+00, -1.3874e+00, -1.5780e+00,  9.1415e-01,\n",
       "          -1.5004e+00,  1.1185e-01, -5.3694e-01,  5.0390e-01, -1.0917e+00,\n",
       "          -3.2231e-01,  6.9508e-01, -9.7843e-01,  6.4026e-01,  6.8062e-01,\n",
       "           6.5405e-01, -4.5502e-01,  7.4053e-01, -1.3150e+00, -2.1695e+00,\n",
       "           2.4951e-01,  1.1186e-01, -7.9547e-01,  1.5309e-01,  1.6735e+00,\n",
       "           4.3535e-01,  6.7528e-01, -1.1907e+00,  1.3842e+00,  1.4155e+00,\n",
       "          -1.2830e+00,  4.8537e-01,  6.9995e-01,  4.0068e-01,  3.1991e-01,\n",
       "           2.2908e+00,  3.9141e-02,  1.1580e+00,  1.1082e+00, -1.0205e+00,\n",
       "          -6.4855e-01, -4.7265e-01,  3.8882e-01,  1.1385e+00,  6.4845e-01,\n",
       "           3.3756e-01, -7.0649e-01, -1.9557e+00, -8.5767e-01,  2.8132e-01,\n",
       "           8.0932e-01,  1.3471e+00,  1.8383e-01, -1.6618e-01, -3.8722e-01,\n",
       "          -2.4470e-01,  6.5527e-01,  1.5484e-01, -4.9395e-01,  1.0782e+00,\n",
       "          -9.6313e-01,  1.5246e-01,  4.6766e-01,  1.0884e-01, -3.9877e-01,\n",
       "           6.3453e-01,  2.3032e+00,  1.9180e+00,  9.0551e-01,  8.2178e-02,\n",
       "           4.2137e-01,  1.1123e+00, -5.2491e-01,  7.4049e-01, -1.1056e+00,\n",
       "          -2.3380e+00, -8.3061e-01, -5.3465e-01, -6.4004e-01,  6.5386e-01,\n",
       "           3.9099e-01, -7.9797e-01,  2.8115e-01, -7.3093e-02,  1.0892e+00,\n",
       "          -1.1192e+00, -5.6473e-01,  8.4140e-01,  1.0740e+00,  1.7930e-01,\n",
       "          -2.2187e-01,  1.2363e+00, -8.6513e-01, -1.0110e+00, -1.1728e+00,\n",
       "          -2.2888e+00,  1.0034e+00, -4.8461e-01, -1.0824e+00, -7.4733e-01,\n",
       "          -1.9479e+00,  1.7330e-01, -4.0416e-01, -2.3888e-01, -1.2456e-01,\n",
       "          -2.5553e-01, -4.0166e-02, -3.6764e-01,  1.5359e+00,  9.4066e-01,\n",
       "          -1.3969e-01, -4.6935e-01, -5.8249e-01,  5.7440e-01,  1.4132e+00,\n",
       "          -1.8693e+00, -1.3813e+00,  1.3932e+00, -2.8791e-01,  4.1152e-01,\n",
       "           5.5816e-01, -1.0274e+00, -1.0443e+00,  1.3560e+00, -4.5734e-01,\n",
       "           4.8372e-01, -1.1856e+00,  1.1947e+00, -5.2107e-01,  7.2829e-01,\n",
       "           7.5493e-01,  6.6788e-01, -2.0896e-01,  4.8523e-01,  8.2262e-01,\n",
       "           9.8797e-01,  1.3810e+00,  1.3595e+00,  7.7131e-01,  1.7739e+00,\n",
       "          -1.0825e+00,  3.4881e-01, -4.8890e-01,  1.2136e+00,  3.7662e-01,\n",
       "          -9.1033e-01,  6.5249e-01,  2.3791e-01, -8.6310e-01,  1.2974e+00,\n",
       "           5.6838e-01,  7.3629e-01, -9.7425e-01, -2.9722e-01, -1.5587e+00,\n",
       "           3.6263e-01, -1.9623e-01,  1.2352e+00, -3.5189e-01, -1.7154e+00,\n",
       "           1.6313e-01,  1.1602e+00,  8.0493e-01, -5.8007e-01, -6.6657e-01,\n",
       "           2.7709e-01,  6.9484e-01,  8.1018e-01,  8.1950e-01,  1.6425e+00,\n",
       "           1.5437e-01,  2.0072e+00, -4.4718e-01, -8.9735e-01,  1.1052e+00,\n",
       "           8.8772e-01,  1.6360e+00, -8.2787e-01, -9.9990e-01, -9.7302e-01,\n",
       "          -2.6320e-01,  1.0759e+00,  5.2235e-01, -1.3533e-01, -3.9964e-01,\n",
       "           2.9398e-01, -1.5023e-01,  3.9871e-01, -1.0475e+00, -3.7889e-01,\n",
       "          -1.9862e-01, -1.3813e+00, -8.6888e-01, -2.4549e+00, -2.2968e-01,\n",
       "           5.4831e-01,  4.9464e-01, -2.2210e-01,  1.6621e+00, -8.5390e-01,\n",
       "          -4.7426e-01, -1.2215e+00,  2.6082e-01,  7.8131e-01, -1.2321e+00,\n",
       "           7.4482e-01, -5.0444e-01, -1.9976e+00,  6.0403e-01, -8.4497e-01,\n",
       "           4.1903e-01,  1.1805e+00,  8.1306e-02, -8.5309e-01,  2.2782e-01,\n",
       "          -3.7042e-01,  4.3260e-01, -1.1336e+00, -9.7417e-01, -2.2201e+00,\n",
       "           4.4058e-01, -6.2459e-01,  7.5020e-03, -2.7010e-01, -6.7692e-01,\n",
       "           2.8807e-01, -2.4484e+00,  8.3555e-01, -1.0111e+00, -9.2949e-01,\n",
       "           4.8208e-01, -3.1039e-01, -2.4060e+00, -1.3419e-01,  1.3995e+00,\n",
       "          -9.2050e-01,  1.1411e+00,  1.5227e+00, -1.4104e-01,  1.7461e+00,\n",
       "           3.5730e-02, -4.5474e-01,  6.5016e-01, -2.6094e-01,  3.0947e-01,\n",
       "          -6.4080e-01, -1.1928e+00,  2.8778e-01,  1.1537e+00, -1.2832e-01,\n",
       "          -2.7913e+00,  3.1604e-02,  1.4573e+00, -2.2058e-01,  1.1436e+00,\n",
       "           3.8431e-01, -1.7020e+00,  1.9599e-01,  1.9123e+00, -2.1642e+00,\n",
       "          -1.5608e+00,  5.4421e-01, -1.7773e-01,  3.9388e-01, -1.5898e+00,\n",
       "          -1.1156e-01,  1.7062e-01, -5.9853e-01,  6.0199e-01,  1.6157e+00,\n",
       "           1.5776e+00,  4.4625e-01, -1.4673e+00, -5.0395e-01, -5.8981e-01,\n",
       "           1.2758e+00,  8.2441e-01,  9.2240e-01, -1.0312e+00,  9.8475e-02,\n",
       "           9.4924e-01, -1.6235e+00, -3.5080e-01, -1.9698e-01,  1.2541e-01,\n",
       "           2.5835e-01,  5.8370e-01,  6.8603e-02,  2.3831e-01, -1.4062e+00,\n",
       "          -6.6909e-01, -1.2951e+00, -1.4112e-01, -7.6895e-01,  5.3437e-01,\n",
       "          -1.1799e-01, -9.0051e-01,  1.8413e+00,  2.9967e-01,  1.7692e+00,\n",
       "          -9.1848e-01,  4.7715e-02,  3.8569e-01, -8.6352e-01, -2.6434e-01,\n",
       "           1.0756e+00,  7.7464e-02,  2.4063e-02,  7.2420e-01, -5.8407e-01,\n",
       "          -3.6077e-02, -7.2690e-01, -2.6187e-01, -7.5544e-01, -3.7882e-01,\n",
       "          -6.7151e-01,  5.1613e-02, -9.0002e-02,  1.2097e+00, -6.6205e-01,\n",
       "           1.2923e+00, -8.0329e-02,  1.0073e+00, -4.6025e-01, -1.7284e+00,\n",
       "           9.0676e-01,  2.5060e+00,  5.1357e-01,  2.2029e-01, -3.9936e-02,\n",
       "          -1.6847e+00, -6.1498e-01, -3.7015e-02, -1.6992e+00,  1.3033e+00,\n",
       "          -1.0206e+00,  7.5245e-02, -2.2145e+00, -1.5437e+00, -1.1924e+00,\n",
       "           1.1770e+00, -6.0050e-01, -3.9392e-01, -4.0685e-01,  3.0883e-01,\n",
       "           2.3546e+00, -3.5284e-01, -7.0271e-02,  8.2141e-01, -1.0619e+00,\n",
       "           1.5304e-01,  1.3038e+00,  8.1289e-02, -5.0256e-01, -4.7806e-01,\n",
       "           1.0880e+00,  5.9374e-01,  1.6556e+00,  8.8321e-01,  5.2498e-01,\n",
       "           8.6540e-01, -9.6427e-01, -5.4538e-01, -2.0624e+00, -9.5101e-01,\n",
       "           1.5097e+00,  1.3741e+00,  6.7275e-01, -1.5131e+00, -5.2218e-01,\n",
       "           7.9845e-01, -8.3951e-02, -5.1966e-01, -3.0574e-01,  3.2924e-01,\n",
       "          -1.3496e-01,  1.0154e+00, -1.7979e+00, -1.4562e+00,  5.6701e-01,\n",
       "          -3.5247e-01, -8.0082e-01,  1.4418e+00, -1.5711e+00,  3.9429e-01,\n",
       "          -1.3331e+00, -5.8025e-01,  3.7940e-01, -2.2804e+00,  2.2093e-01,\n",
       "          -7.0911e-01, -2.8341e-01, -1.7835e+00, -3.8241e-01,  1.3491e-03,\n",
       "           1.1205e+00, -8.0571e-01, -1.0484e+00, -3.8249e-01, -7.3989e-02,\n",
       "           8.7036e-01,  2.5033e-01, -9.3811e-01, -1.0347e+00, -1.5896e+00,\n",
       "           6.0197e-01,  1.1407e-02, -1.3800e+00,  6.0260e-01, -3.0351e-01,\n",
       "           1.1836e+00, -1.6408e+00,  6.1079e-01, -1.7683e+00,  1.6654e+00,\n",
       "          -2.4656e+00, -1.7454e+00,  8.5995e-01, -5.0630e-01, -3.7902e-01,\n",
       "          -4.3593e-01,  1.3127e-01, -6.2834e-01,  9.2038e-01,  1.0179e-02,\n",
       "           4.1303e-01,  1.1736e+00,  7.2534e-01,  2.2219e-01,  1.1086e+00,\n",
       "           6.3542e-01,  1.8113e-01,  9.1850e-01,  7.8797e-01, -1.9280e-01,\n",
       "           1.9326e+00, -2.0830e-01, -5.4141e-01,  7.5186e-01,  3.0698e-01,\n",
       "           8.6154e-01,  2.8947e-01, -4.3507e-01,  3.3811e-02,  1.2198e+00,\n",
       "          -3.8588e-01,  7.4613e-01, -2.3643e-03,  7.4513e-02, -1.2532e-01,\n",
       "           2.0901e+00,  1.3635e-01, -2.8220e-01,  1.4528e-01, -1.4518e+00,\n",
       "           6.9878e-01,  8.5450e-02, -1.6456e+00,  8.7270e-01,  2.6731e-01,\n",
       "          -3.2155e-02,  2.0820e-01,  1.6496e+00,  3.7624e-01,  7.5232e-01,\n",
       "          -3.2907e-01,  7.9029e-02, -9.9868e-01,  9.6144e-01,  1.5720e+00,\n",
       "          -1.6811e+00, -2.6061e-02,  2.8053e-01, -7.7090e-01, -2.1711e+00,\n",
       "           2.6635e+00, -1.2178e+00, -1.2020e+00, -1.8276e+00,  3.5189e-01,\n",
       "          -1.1718e+00,  1.3765e+00,  3.2913e-01, -3.0725e-01, -3.7107e-01,\n",
       "          -5.2215e-01, -6.3230e-03,  7.5891e-01, -2.4976e+00, -9.5139e-01,\n",
       "           4.2494e-01, -8.3677e-01,  1.8784e+00,  1.1923e+00,  2.8611e-01,\n",
       "           7.2239e-01,  1.6482e+00, -4.3692e-01]]], requires_grad=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glasses.models.classification.deit import DeiT\n",
    "DeiT().embedding.tokens.dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/facebookresearch/deit/archive/main.zip\" to /home/zuppif/.cache/torch/hub/main.zip\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/deit/deit_base_distilled_patch16_224-df68dfff.pth\" to /home/zuppif/.cache/torch/hub/checkpoints/deit_base_distilled_patch16_224-df68dfff.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2062dd83b5ee4974be5247286febf6b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/333M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "model = torch.hub.load('facebookresearch/deit:main', 'deit_base_distilled_patch16_224', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilledVisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       "  (head_dist): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/zuppif/.cache/torch/hub/facebookresearch_deit_main\n",
      "Using cache found in /home/zuppif/.cache/torch/hub/facebookresearch_deit_main\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from glasses.models import AutoModel, AutoConfig, EfficientNetLite\n",
    "import timm\n",
    "from transfer_weights import clone_model, deit_clone\n",
    "from benchmark import benchmark\n",
    "from glasses.models.classification.vit import ViTTokens\n",
    "\n",
    "src = torch.hub.load('facebookresearch/deit:main', 'deit_small_distilled_patch16_224', pretrained=True).eval()\n",
    "dst = deit_clone('deit_small_patch16_224').eval()\n",
    "# src = timm.create_model('vit_base_patch16_224', pretrained='True')\n",
    "# dst = AutoModel.from_name('vit_base_patch16_224')\n",
    "transform = AutoConfig.from_name('vit_base_patch16_224').transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./imagenet_val_targets.pkl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/50000 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 891/50000 [00:00<00:05, 8906.74it/s]\u001b[A\n",
      "  4%|▎         | 1803/50000 [00:00<00:05, 9029.59it/s]\u001b[A\n",
      "  5%|▌         | 2706/50000 [00:00<00:05, 8897.71it/s]\u001b[A\n",
      "  7%|▋         | 3596/50000 [00:00<00:05, 8802.66it/s]\u001b[A\n",
      "  9%|▉         | 4478/50000 [00:00<00:05, 8805.49it/s]\u001b[A\n",
      " 11%|█         | 5359/50000 [00:00<00:05, 8120.95it/s]\u001b[A\n",
      " 12%|█▏        | 6234/50000 [00:00<00:05, 8313.00it/s]\u001b[A\n",
      " 14%|█▍        | 7105/50000 [00:00<00:05, 8431.78it/s]\u001b[A\n",
      " 16%|█▌        | 7959/50000 [00:00<00:04, 8462.51it/s]\u001b[A\n",
      " 18%|█▊        | 8861/50000 [00:01<00:04, 8629.16it/s]\u001b[A\n",
      " 20%|█▉        | 9762/50000 [00:01<00:04, 8742.82it/s]\u001b[A\n",
      " 21%|██▏       | 10659/50000 [00:01<00:04, 8809.10it/s]\u001b[A\n",
      " 23%|██▎       | 11568/50000 [00:01<00:04, 8892.94it/s]\u001b[A\n",
      " 25%|██▍       | 12480/50000 [00:01<00:04, 8960.36it/s]\u001b[A\n",
      " 27%|██▋       | 13385/50000 [00:01<00:04, 8985.83it/s]\u001b[A\n",
      " 29%|██▊       | 14293/50000 [00:01<00:03, 9011.18it/s]\u001b[A\n",
      " 30%|███       | 15205/50000 [00:01<00:03, 9041.85it/s]\u001b[A\n",
      " 32%|███▏      | 16125/50000 [00:01<00:03, 9088.41it/s]\u001b[A\n",
      " 34%|███▍      | 17044/50000 [00:01<00:03, 9116.18it/s]\u001b[A\n",
      " 36%|███▌      | 17956/50000 [00:02<00:03, 9096.05it/s]\u001b[A\n",
      " 38%|███▊      | 18883/50000 [00:02<00:03, 9147.93it/s]\u001b[A\n",
      " 40%|███▉      | 19803/50000 [00:02<00:03, 9162.61it/s]\u001b[A\n",
      " 41%|████▏     | 20736/50000 [00:02<00:03, 9210.30it/s]\u001b[A\n",
      " 43%|████▎     | 21659/50000 [00:02<00:03, 9214.93it/s]\u001b[A\n",
      " 45%|████▌     | 22581/50000 [00:02<00:02, 9195.67it/s]\u001b[A\n",
      " 47%|████▋     | 23513/50000 [00:02<00:02, 9231.90it/s]\u001b[A\n",
      " 49%|████▉     | 24454/50000 [00:02<00:02, 9282.96it/s]\u001b[A\n",
      " 51%|█████     | 25383/50000 [00:02<00:02, 9244.31it/s]\u001b[A\n",
      " 53%|█████▎    | 26310/50000 [00:02<00:02, 9251.73it/s]\u001b[A\n",
      " 54%|█████▍    | 27236/50000 [00:03<00:02, 9204.99it/s]\u001b[A\n",
      " 56%|█████▋    | 28157/50000 [00:03<00:02, 9043.19it/s]\u001b[A\n",
      " 58%|█████▊    | 29087/50000 [00:03<00:02, 9117.55it/s]\u001b[A\n",
      " 60%|██████    | 30015/50000 [00:03<00:02, 9163.66it/s]\u001b[A\n",
      " 62%|██████▏   | 30944/50000 [00:03<00:02, 9200.81it/s]\u001b[A\n",
      " 64%|██████▎   | 31865/50000 [00:03<00:01, 9198.25it/s]\u001b[A\n",
      " 66%|██████▌   | 32786/50000 [00:03<00:01, 9139.01it/s]\u001b[A\n",
      " 67%|██████▋   | 33709/50000 [00:03<00:01, 9163.49it/s]\u001b[A\n",
      " 69%|██████▉   | 34632/50000 [00:03<00:01, 9180.76it/s]\u001b[A\n",
      " 71%|███████   | 35551/50000 [00:03<00:01, 9176.24it/s]\u001b[A\n",
      " 73%|███████▎  | 36469/50000 [00:04<00:01, 9160.79it/s]\u001b[A\n",
      " 75%|███████▍  | 37386/50000 [00:04<00:01, 9023.73it/s]\u001b[A\n",
      " 77%|███████▋  | 38289/50000 [00:04<00:01, 8806.61it/s]\u001b[A\n",
      " 78%|███████▊  | 39171/50000 [00:04<00:01, 8660.35it/s]\u001b[A\n",
      " 80%|████████  | 40046/50000 [00:04<00:01, 8684.35it/s]\u001b[A\n",
      " 82%|████████▏ | 40967/50000 [00:04<00:01, 8838.18it/s]\u001b[A\n",
      " 84%|████████▍ | 41877/50000 [00:04<00:00, 8913.40it/s]\u001b[A\n",
      " 86%|████████▌ | 42796/50000 [00:04<00:00, 8992.90it/s]\u001b[A\n",
      " 87%|████████▋ | 43696/50000 [00:04<00:00, 8926.25it/s]\u001b[A\n",
      " 89%|████████▉ | 44599/50000 [00:04<00:00, 8954.35it/s]\u001b[A\n",
      " 91%|█████████ | 45495/50000 [00:05<00:00, 8955.69it/s]\u001b[A\n",
      " 93%|█████████▎| 46430/50000 [00:05<00:00, 9070.89it/s]\u001b[A\n",
      " 95%|█████████▍| 47338/50000 [00:05<00:00, 9008.77it/s]\u001b[A\n",
      " 96%|█████████▋| 48240/50000 [00:05<00:00, 8986.93it/s]\u001b[A\n",
      "100%|██████████| 50000/50000 [00:05<00:00, 8977.28it/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.80258, 0.9512, 113.77364897727966)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark(dst.cuda(), transform, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./imagenet_val_targets.pkl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/50000 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 913/50000 [00:00<00:05, 9126.03it/s]\u001b[A\n",
      "  4%|▎         | 1835/50000 [00:00<00:05, 9178.92it/s]\u001b[A\n",
      "  6%|▌         | 2754/50000 [00:00<00:05, 9180.07it/s]\u001b[A\n",
      "  7%|▋         | 3673/50000 [00:00<00:05, 9179.99it/s]\u001b[A\n",
      "  9%|▉         | 4591/50000 [00:00<00:05, 9030.62it/s]\u001b[A\n",
      " 11%|█         | 5495/50000 [00:00<00:04, 8989.37it/s]\u001b[A\n",
      " 13%|█▎        | 6415/50000 [00:00<00:04, 9055.47it/s]\u001b[A\n",
      " 15%|█▍        | 7331/50000 [00:00<00:04, 9086.00it/s]\u001b[A\n",
      " 16%|█▋        | 8246/50000 [00:00<00:04, 9105.48it/s]\u001b[A\n",
      " 18%|█▊        | 9157/50000 [00:01<00:04, 9055.70it/s]\u001b[A\n",
      " 20%|██        | 10063/50000 [00:01<00:04, 9003.77it/s]\u001b[A\n",
      " 22%|██▏       | 10975/50000 [00:01<00:04, 9036.97it/s]\u001b[A\n",
      " 24%|██▍       | 11879/50000 [00:01<00:04, 8994.20it/s]\u001b[A\n",
      " 26%|██▌       | 12779/50000 [00:01<00:04, 8969.27it/s]\u001b[A\n",
      " 27%|██▋       | 13676/50000 [00:01<00:04, 8895.77it/s]\u001b[A\n",
      " 29%|██▉       | 14613/50000 [00:01<00:03, 9036.88it/s]\u001b[A\n",
      " 31%|███       | 15549/50000 [00:01<00:03, 9131.70it/s]\u001b[A\n",
      " 33%|███▎      | 16475/50000 [00:01<00:03, 9168.30it/s]\u001b[A\n",
      " 35%|███▍      | 17393/50000 [00:01<00:03, 9163.51it/s]\u001b[A\n",
      " 37%|███▋      | 18310/50000 [00:02<00:03, 9133.96it/s]\u001b[A\n",
      " 38%|███▊      | 19232/50000 [00:02<00:03, 9159.02it/s]\u001b[A\n",
      " 40%|████      | 20163/50000 [00:02<00:03, 9201.43it/s]\u001b[A\n",
      " 42%|████▏     | 21084/50000 [00:02<00:03, 9192.90it/s]\u001b[A\n",
      " 44%|████▍     | 22004/50000 [00:02<00:03, 9073.57it/s]\u001b[A\n",
      " 46%|████▌     | 22912/50000 [00:02<00:03, 9023.41it/s]\u001b[A\n",
      " 48%|████▊     | 23815/50000 [00:02<00:02, 8996.76it/s]\u001b[A\n",
      " 49%|████▉     | 24715/50000 [00:02<00:02, 8940.20it/s]\u001b[A\n",
      " 51%|█████     | 25616/50000 [00:02<00:02, 8959.68it/s]\u001b[A\n",
      " 53%|█████▎    | 26541/50000 [00:02<00:02, 9044.18it/s]\u001b[A\n",
      " 55%|█████▍    | 27446/50000 [00:03<00:02, 8795.47it/s]\u001b[A\n",
      " 57%|█████▋    | 28349/50000 [00:03<00:02, 8861.44it/s]\u001b[A\n",
      " 59%|█████▊    | 29258/50000 [00:03<00:02, 8928.08it/s]\u001b[A\n",
      " 60%|██████    | 30182/50000 [00:03<00:02, 9018.15it/s]\u001b[A\n",
      " 62%|██████▏   | 31085/50000 [00:03<00:02, 8973.81it/s]\u001b[A\n",
      " 64%|██████▍   | 32022/50000 [00:03<00:01, 9089.35it/s]\u001b[A\n",
      " 66%|██████▌   | 32957/50000 [00:03<00:01, 9164.63it/s]\u001b[A\n",
      " 68%|██████▊   | 33874/50000 [00:03<00:01, 8890.80it/s]\u001b[A\n",
      " 70%|██████▉   | 34774/50000 [00:03<00:01, 8920.28it/s]\u001b[A\n",
      " 71%|███████▏  | 35668/50000 [00:03<00:01, 8830.40it/s]\u001b[A\n",
      " 73%|███████▎  | 36553/50000 [00:04<00:01, 8809.13it/s]\u001b[A\n",
      " 75%|███████▍  | 37435/50000 [00:04<00:01, 8808.57it/s]\u001b[A\n",
      " 77%|███████▋  | 38338/50000 [00:04<00:01, 8873.63it/s]\u001b[A\n",
      " 79%|███████▊  | 39278/50000 [00:04<00:01, 9028.22it/s]\u001b[A\n",
      " 80%|████████  | 40215/50000 [00:04<00:01, 9129.34it/s]\u001b[A\n",
      " 82%|████████▏ | 41152/50000 [00:04<00:00, 9198.36it/s]\u001b[A\n",
      " 84%|████████▍ | 42073/50000 [00:04<00:00, 9174.35it/s]\u001b[A\n",
      " 86%|████████▌ | 42991/50000 [00:04<00:00, 9128.40it/s]\u001b[A\n",
      " 88%|████████▊ | 43906/50000 [00:04<00:00, 9134.55it/s]\u001b[A\n",
      " 90%|████████▉ | 44820/50000 [00:04<00:00, 9054.64it/s]\u001b[A\n",
      " 91%|█████████▏| 45729/50000 [00:05<00:00, 9064.24it/s]\u001b[A\n",
      " 93%|█████████▎| 46673/50000 [00:05<00:00, 9174.63it/s]\u001b[A\n",
      " 95%|█████████▌| 47618/50000 [00:05<00:00, 9254.80it/s]\u001b[A\n",
      " 97%|█████████▋| 48544/50000 [00:05<00:00, 9109.19it/s]\u001b[A\n",
      "100%|██████████| 50000/50000 [00:05<00:00, 9045.41it/s]\u001b[A\n",
      "/home/zuppif/anaconda3/envs/dl/lib/python3.8/site-packages/torch/cuda/memory.py:231: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.80258, 0.9512, 117.3070387840271)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark(src.cuda(), transform, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "├─PatchEmbedding: 1-1                         [1, 198, 384]             --\n",
      "|    └─Sequential: 2-1                        [1, 196, 384]             --\n",
      "|    |    └─Conv2d: 3-1                       [1, 384, 14, 14]          295,296\n",
      "|    |    └─Rearrange: 3-2                    [1, 196, 384]             --\n",
      "|    └─DeiTTokens: 2-2                        [1, 1, 384]               768\n",
      "├─TransformerEncoder: 1-2                     [1, 198, 384]             --\n",
      "|    └─ModuleList: 2                          []                        --\n",
      "|    |    └─TransformerEncoderBlock: 3-3      [1, 198, 384]             1,774,464\n",
      "|    |    └─TransformerEncoderBlock: 3-4      [1, 198, 384]             1,774,464\n",
      "|    |    └─TransformerEncoderBlock: 3-5      [1, 198, 384]             1,774,464\n",
      "|    |    └─TransformerEncoderBlock: 3-6      [1, 198, 384]             1,774,464\n",
      "|    |    └─TransformerEncoderBlock: 3-7      [1, 198, 384]             1,774,464\n",
      "|    |    └─TransformerEncoderBlock: 3-8      [1, 198, 384]             1,774,464\n",
      "|    |    └─TransformerEncoderBlock: 3-9      [1, 198, 384]             1,774,464\n",
      "|    |    └─TransformerEncoderBlock: 3-10     [1, 198, 384]             1,774,464\n",
      "|    |    └─TransformerEncoderBlock: 3-11     [1, 198, 384]             1,774,464\n",
      "|    |    └─TransformerEncoderBlock: 3-12     [1, 198, 384]             1,774,464\n",
      "|    |    └─TransformerEncoderBlock: 3-13     [1, 198, 384]             1,774,464\n",
      "|    |    └─TransformerEncoderBlock: 3-14     [1, 198, 384]             1,774,464\n",
      "|    └─LayerNorm: 2-3                         [1, 198, 384]             768\n",
      "├─DeiTClassificationHead: 1-3                 [1, 1000]                 --\n",
      "|    └─Linear: 2-4                            [1, 1000]                 385,000\n",
      "|    └─Linear: 2-5                            [1, 1000]                 385,000\n",
      "===============================================================================================\n",
      "Total params: 22,360,400\n",
      "Trainable params: 22,360,400\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 123.66\n",
      "===============================================================================================\n",
      "Input size (MB): 0.60\n",
      "Forward/backward pass size (MB): 1.23\n",
      "Params size (MB): 89.44\n",
      "Estimated Total Size (MB): 91.27\n",
      "===============================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "├─PatchEmbedding: 1-1                         [1, 198, 384]             --\n",
       "|    └─Sequential: 2-1                        [1, 196, 384]             --\n",
       "|    |    └─Conv2d: 3-1                       [1, 384, 14, 14]          295,296\n",
       "|    |    └─Rearrange: 3-2                    [1, 196, 384]             --\n",
       "|    └─DeiTTokens: 2-2                        [1, 1, 384]               768\n",
       "├─TransformerEncoder: 1-2                     [1, 198, 384]             --\n",
       "|    └─ModuleList: 2                          []                        --\n",
       "|    |    └─TransformerEncoderBlock: 3-3      [1, 198, 384]             1,774,464\n",
       "|    |    └─TransformerEncoderBlock: 3-4      [1, 198, 384]             1,774,464\n",
       "|    |    └─TransformerEncoderBlock: 3-5      [1, 198, 384]             1,774,464\n",
       "|    |    └─TransformerEncoderBlock: 3-6      [1, 198, 384]             1,774,464\n",
       "|    |    └─TransformerEncoderBlock: 3-7      [1, 198, 384]             1,774,464\n",
       "|    |    └─TransformerEncoderBlock: 3-8      [1, 198, 384]             1,774,464\n",
       "|    |    └─TransformerEncoderBlock: 3-9      [1, 198, 384]             1,774,464\n",
       "|    |    └─TransformerEncoderBlock: 3-10     [1, 198, 384]             1,774,464\n",
       "|    |    └─TransformerEncoderBlock: 3-11     [1, 198, 384]             1,774,464\n",
       "|    |    └─TransformerEncoderBlock: 3-12     [1, 198, 384]             1,774,464\n",
       "|    |    └─TransformerEncoderBlock: 3-13     [1, 198, 384]             1,774,464\n",
       "|    |    └─TransformerEncoderBlock: 3-14     [1, 198, 384]             1,774,464\n",
       "|    └─LayerNorm: 2-3                         [1, 198, 384]             768\n",
       "├─DeiTClassificationHead: 1-3                 [1, 1000]                 --\n",
       "|    └─Linear: 2-4                            [1, 1000]                 385,000\n",
       "|    └─Linear: 2-5                            [1, 1000]                 385,000\n",
       "===============================================================================================\n",
       "Total params: 22,360,400\n",
       "Trainable params: 22,360,400\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 123.66\n",
       "===============================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 1.23\n",
       "Params size (MB): 89.44\n",
       "Estimated Total Size (MB): 91.27\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dst.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
