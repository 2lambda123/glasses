

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>glasses.models.classification.vit &mdash; Glasses  documentation</title>
  

  
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../index.html" class="icon icon-home"> Glasses
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../Notes/tutorials/Interpretability.html">Interpretability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../Notes/tutorials/Segmentation.html">Segmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../Notes/tutorials/Transfer%20Learning.html">Transfer Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../Notes/tutorials/installation.html">Installation</a></li>
</ul>
<p class="caption"><span class="caption-text">Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../modules.html">glasses</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../glasses.nn.html">glasses.nn package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../glasses.models.html">glasses.models package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../glasses.utils.html">glasses.utils package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../glasses.data.html">glasses.data package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../glasses.interpretability.html">glasses.interpretability package</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">Glasses</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
        
      <li>glasses.models.classification.vit</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for glasses.models.classification.vit</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">glasses.nn.blocks.residuals</span> <span class="kn">import</span> <span class="n">ResidualAdd</span>
<span class="kn">from</span> <span class="nn">glasses.nn.blocks</span> <span class="kn">import</span> <span class="n">Lambda</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>
<span class="kn">from</span> <span class="nn">glasses.utils.weights.PretrainedWeightsProvider</span> <span class="kn">import</span> <span class="n">pretrained</span>
<span class="kn">from</span> <span class="nn">....models.base</span> <span class="kn">import</span> <span class="n">Encoder</span><span class="p">,</span> <span class="n">VisionModule</span>
<span class="kn">from</span> <span class="nn">einops</span> <span class="kn">import</span> <span class="n">rearrange</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">repeat</span>
<span class="kn">from</span> <span class="nn">einops.layers.torch</span> <span class="kn">import</span> <span class="n">Rearrange</span><span class="p">,</span> <span class="n">Reduce</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span>


<div class="viewcode-block" id="ViTTokens"><a class="viewcode-back" href="../../../../glasses.models.classification.vit.html#glasses.models.classification.ViTTokens">[docs]</a><span class="k">class</span> <span class="nc">ViTTokens</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cls</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">))</span>

<div class="viewcode-block" id="ViTTokens.forward"><a class="viewcode-back" href="../../../../glasses.models.classification.vit.html#glasses.models.classification.ViTTokens.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="c1"># for each token repeat itself over the batch dimension</span>
            <span class="n">tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">repeat</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="s2">&quot;() n e -&gt; b n e&quot;</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="n">b</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">tokens</span></div>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">()))</span></div>


<div class="viewcode-block" id="PatchEmbedding"><a class="viewcode-back" href="../../../../glasses.models.classification.vit.html#glasses.models.classification.PatchEmbedding">[docs]</a><span class="k">class</span> <span class="nc">PatchEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
        <span class="n">patch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
        <span class="n">emb_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">768</span><span class="p">,</span>
        <span class="n">img_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">224</span><span class="p">,</span>
        <span class="n">tokens</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">ViTTokens</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Patch Embedding layer used in ViT. In order to work with transformers, this layer decompose</span>
<span class="sd">        the input in multiple patches, add class token parameter and a position encoding (both learnable) and flat them.</span>

<span class="sd">        The following image from the authors shows the architecture.</span>


<span class="sd">        .. image:: https://github.com/FrancescoSaverioZuppichini/glasses/blob/develop/docs/_static/images/ViTPatchesPositionEmbedding.png?raw=true</span>

<span class="sd">        Example:</span>

<span class="sd">            Change the tokens</span>

<span class="sd">            &gt;&gt;&gt; class MyTokens(ViTTokens):</span>
<span class="sd">            &gt;&gt;&gt;     def __init__(self, emb_size: int):</span>
<span class="sd">            &gt;&gt;&gt;         super().__init__(emb_size)</span>
<span class="sd">            &gt;&gt;&gt;         self.my_new_token = nn.Parameter(torch.randn(1, 1, emb_size))</span>
<span class="sd">            &gt;&gt;&gt; PatchEmbedding(tokens=MyTokens)</span>

<span class="sd">        Args:</span>
<span class="sd">            in_channels (int, optional): Number of input&#39;s channels. Defaults to 3.</span>
<span class="sd">            patch_size (int, optional): Size of the each patch. Defaults to 16.</span>
<span class="sd">            emb_size (int, optional):  Embedding dimensions Defaults to 768.</span>
<span class="sd">            img_size (int, optional): Size of the input image, this is needed to calculate the final number of patches. Defaults to 224.</span>
<span class="sd">            tokens (nn.Module, optional): A module that contains the tokens as his parameters. Defaults to ViTTokens.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">patch_size</span> <span class="o">=</span> <span class="n">patch_size</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="c1"># using a conv layer instead of a linear one -&gt; performance gains</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">patch_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">patch_size</span><span class="p">),</span>
            <span class="n">Rearrange</span><span class="p">(</span><span class="s2">&quot;b e (h) (w) -&gt; b (h w) e&quot;</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokens</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">(</span><span class="n">emb_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">positions</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">img_size</span> <span class="o">//</span> <span class="n">patch_size</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokens</span><span class="p">),</span> <span class="n">emb_size</span><span class="p">)</span>
        <span class="p">)</span>

<div class="viewcode-block" id="PatchEmbedding.forward"><a class="viewcode-back" href="../../../../glasses.models.classification.vit.html#glasses.models.classification.PatchEmbedding.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># get the tokens</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokens</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># prepend the tokens to the input</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="o">*</span><span class="n">tokens</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># add position embedding</span>
        <span class="n">x</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">positions</span>
        <span class="k">return</span> <span class="n">x</span></div></div>


<div class="viewcode-block" id="MultiHeadAttention"><a class="viewcode-back" href="../../../../glasses.models.classification.vit.html#glasses.models.classification.MultiHeadAttention">[docs]</a><span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">emb_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">768</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">12</span><span class="p">,</span>
        <span class="n">att_drop_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">projection_drop_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
        <span class="n">qkv_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Classic multi head attention proposed in `Attention Is All You Need &lt;https://arxiv.org/abs/1706.03762&gt;`_</span>

<span class="sd">        Args:</span>
<span class="sd">            emb_size (int, optional):  Embedding dimensions Defaults to 768.</span>
<span class="sd">            num_heads (int, optional): Number of heads. Defaults to 12.</span>
<span class="sd">            att_drop_p (float, optional): Attention dropout probability. Defaults to 0..</span>
<span class="sd">            projection_drop_p (float, optional): Projection dropout probability. Defaults to 0..</span>
<span class="sd">            qkv_bias (bool, optional): If yes, apply bias to the qkv projection matrix. Defaults to False.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emb_size</span> <span class="o">=</span> <span class="n">emb_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="c1"># fuse the queries, keys and values in one matrix</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">qkv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">emb_size</span><span class="p">,</span> <span class="n">emb_size</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">qkv_bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">att_drop</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">att_drop_p</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">emb_size</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">projection_drop_p</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">emb_size</span> <span class="o">//</span> <span class="n">num_heads</span><span class="p">)</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span>

<div class="viewcode-block" id="MultiHeadAttention.forward"><a class="viewcode-back" href="../../../../glasses.models.classification.vit.html#glasses.models.classification.MultiHeadAttention.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="c1"># split keys, queries and values in num_heads</span>
        <span class="n">qkv</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">qkv</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s2">&quot;b n (qkv h d) -&gt; (qkv) b h n d&quot;</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">qkv</span><span class="o">=</span><span class="mi">3</span>
        <span class="p">)</span>

        <span class="n">queries</span><span class="p">,</span> <span class="n">keys</span><span class="p">,</span> <span class="n">values</span> <span class="o">=</span> <span class="n">qkv</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">qkv</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">qkv</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="c1"># dot product, Q V^T, here we don&#39;t transpose before, so this is why</span>
        <span class="c1"># the sum is made on the last index of  K</span>
        <span class="n">energy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bhij, bhkj -&gt; bhik&quot;</span><span class="p">,</span> <span class="n">queries</span><span class="p">,</span> <span class="n">keys</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span>
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">fill_value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">min</span>
            <span class="n">energy</span><span class="o">.</span><span class="n">mask_fill</span><span class="p">(</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="n">fill_value</span><span class="p">)</span>

        <span class="n">att</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">energy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">att</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">att_drop</span><span class="p">(</span><span class="n">att</span><span class="p">)</span>
        <span class="c1"># dot product</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bhij, bhjk -&gt; bhik &quot;</span><span class="p">,</span> <span class="n">att</span><span class="p">,</span> <span class="n">values</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="s2">&quot;b h n d -&gt; b n (h d)&quot;</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span></div></div>


<span class="c1"># TODO move it to blocks</span>


<div class="viewcode-block" id="ResidualAdd"><a class="viewcode-back" href="../../../../glasses.models.classification.vit.html#glasses.models.classification.ResidualAdd">[docs]</a><span class="k">class</span> <span class="nc">ResidualAdd</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fn</span> <span class="o">=</span> <span class="n">fn</span>

<div class="viewcode-block" id="ResidualAdd.forward"><a class="viewcode-back" href="../../../../glasses.models.classification.vit.html#glasses.models.classification.ResidualAdd.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">out</span>
        <span class="k">return</span> <span class="n">x</span></div></div>


<div class="viewcode-block" id="FeedForwardBlock"><a class="viewcode-back" href="../../../../glasses.models.classification.vit.html#glasses.models.classification.FeedForwardBlock">[docs]</a><span class="k">class</span> <span class="nc">FeedForwardBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">emb_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">expansion</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
        <span class="n">drop_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">activation</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">emb_size</span><span class="p">,</span> <span class="n">expansion</span> <span class="o">*</span> <span class="n">emb_size</span><span class="p">),</span>
            <span class="n">activation</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">drop_p</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">expansion</span> <span class="o">*</span> <span class="n">emb_size</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">drop_p</span><span class="p">),</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="TransformerEncoderBlock"><a class="viewcode-back" href="../../../../glasses.models.classification.vit.html#glasses.models.classification.TransformerEncoderBlock">[docs]</a><span class="k">class</span> <span class="nc">TransformerEncoderBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">emb_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">768</span><span class="p">,</span>
        <span class="n">forward_expansion</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
        <span class="n">forward_drop_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
        <span class="n">activation</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Transformer Encoder block proposed in `Attention Is All You Need &lt;https://arxiv.org/abs/1706.03762&gt;`_</span>

<span class="sd">        The following image from the authors shows the architecture.</span>


<span class="sd">        .. image:: https://github.com/FrancescoSaverioZuppichini/glasses/blob/develop/docs/_static/images/ViTTransformerBlock.png?raw=true</span>

<span class="sd">        Args:</span>
<span class="sd">            emb_size (int, optional):  Embedding dimensions Defaults to 768.</span>
<span class="sd">            forward_expansion (int, optional): [description]. Defaults to 4.</span>
<span class="sd">            forward_drop_p (float, optional): [description]. Defaults to 0..</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">ResidualAdd</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                    <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">emb_size</span><span class="p">),</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">emb_size</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="p">),</span>
            <span class="n">ResidualAdd</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                    <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">emb_size</span><span class="p">),</span>
                    <span class="n">FeedForwardBlock</span><span class="p">(</span>
                        <span class="n">emb_size</span><span class="p">,</span>
                        <span class="n">expansion</span><span class="o">=</span><span class="n">forward_expansion</span><span class="p">,</span>
                        <span class="n">drop_p</span><span class="o">=</span><span class="n">forward_drop_p</span><span class="p">,</span>
                        <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">),</span>
            <span class="p">),</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="TransformerEncoder"><a class="viewcode-back" href="../../../../glasses.models.classification.vit.html#glasses.models.classification.TransformerEncoder">[docs]</a><span class="k">class</span> <span class="nc">TransformerEncoder</span><span class="p">(</span><span class="n">Encoder</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">depth</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">12</span><span class="p">,</span>
        <span class="n">emb_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">786</span><span class="p">,</span>
        <span class="n">block</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">TransformerEncoderBlock</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Transformer Encoder proposed in `Attention Is All You Need &lt;https://arxiv.org/abs/1706.03762&gt;`_</span>

<span class="sd">        .. warning::</span>
<span class="sd">            Even if `TransformerEncoder` uses the `Encoder` APIs you won&#39;t be able to use it with `segmentation` models</span>
<span class="sd">            since they will expect 3-D tensors as inputs.</span>

<span class="sd">        Args:</span>
<span class="sd">            depth (int, optional): Number of transformer&#39;s blocks. Defaults to 12.</span>
<span class="sd">            block ( nn.Module, optional): Block used inside the transformer encoder. Defaults to TransformerEncoderBlock.</span>
<span class="sd">            emb_size (int, optional):  Embedding dimensions Defaults to 768.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">widths</span> <span class="o">=</span> <span class="p">[</span><span class="n">emb_size</span><span class="p">]</span> <span class="o">*</span> <span class="n">depth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">block</span><span class="p">(</span><span class="n">emb_size</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">depth</span><span class="p">)])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">emb_size</span><span class="p">)</span>

<div class="viewcode-block" id="TransformerEncoder.forward"><a class="viewcode-back" href="../../../../glasses.models.classification.vit.html#glasses.models.classification.TransformerEncoder.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span></div></div>


<div class="viewcode-block" id="ViTClassificationHead"><a class="viewcode-back" href="../../../../glasses.models.classification.vit.html#glasses.models.classification.ViTClassificationHead">[docs]</a><span class="k">class</span> <span class="nc">ViTClassificationHead</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">):</span>
    <span class="n">POLICIES</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="s2">&quot;mean&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">768</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">policy</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;token&quot;</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        ViT Classification Head</span>

<span class="sd">        Args:</span>
<span class="sd">            emb_size (int, optional):  Embedding dimensions Defaults to 768.</span>
<span class="sd">            n_classes (int, optional): [description]. Defaults to 1000.</span>
<span class="sd">            policy (str, optional): Pooling policy, can be token or mean. Defaults to &#39;token&#39;.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">policy</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">POLICIES</span>
        <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Only policies </span><span class="si">{</span><span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">POLICIES</span><span class="p">)</span><span class="si">}</span><span class="s2"> are supported&quot;</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">OrderedDict</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="s2">&quot;pool&quot;</span><span class="p">:</span> <span class="n">Reduce</span><span class="p">(</span><span class="s2">&quot;b n e -&gt; b e&quot;</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">policy</span> <span class="o">==</span> <span class="s2">&quot;mean&quot;</span>
                    <span class="k">else</span> <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]),</span>
                    <span class="s2">&quot;fc&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">emb_size</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">),</span>
                <span class="p">}</span>
            <span class="p">)</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="ViT"><a class="viewcode-back" href="../../../../glasses.models.classification.vit.html#glasses.models.classification.ViT">[docs]</a><span class="k">class</span> <span class="nc">ViT</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">,</span> <span class="n">VisionModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">embedding</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">PatchEmbedding</span><span class="p">,</span>
        <span class="n">encoder</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">TransformerEncoder</span><span class="p">,</span>
        <span class="n">head</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">ViTClassificationHead</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
        <span class="n">patch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
        <span class="n">emb_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">768</span><span class="p">,</span>
        <span class="n">img_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">224</span><span class="p">,</span>
        <span class="n">tokens</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">ViTTokens</span><span class="p">,</span>
        <span class="n">depth</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">12</span><span class="p">,</span>
        <span class="n">n_classes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Implementation of Vision Transformer (ViT) proposed in `An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale &lt;https://arxiv.org/pdf/2010.11929.pdf&gt;`_</span>

<span class="sd">        The following image from the authors shows the architecture.</span>

<span class="sd">        .. image:: https://github.com/FrancescoSaverioZuppichini/glasses/blob/develop/docs/_static/images/ViT.png?raw=true</span>

<span class="sd">        Examples:</span>

<span class="sd">            Default models</span>

<span class="sd">            &gt;&gt;&gt; ViT.vit_small_patch16_224()</span>
<span class="sd">            &gt;&gt;&gt; ViT.vit_base_patch16_224()</span>
<span class="sd">            &gt;&gt;&gt; ViT.vit_base_patch16_384()</span>
<span class="sd">            &gt;&gt;&gt; ViT.vit_base_patch32_384()</span>
<span class="sd">            &gt;&gt;&gt; ViT.vit_huge_patch16_224()</span>
<span class="sd">            &gt;&gt;&gt; ViT.vit_huge_patch32_384()</span>
<span class="sd">            &gt;&gt;&gt; ViT.vit_large_patch16_224()</span>
<span class="sd">            &gt;&gt;&gt; ViT.vit_large_patch16_384()</span>
<span class="sd">            &gt;&gt;&gt; ViT.vit_large_patch32_384()</span>

<span class="sd">            You can easily customize your model</span>


<span class="sd">            &gt;&gt;&gt; # change activation</span>
<span class="sd">            &gt;&gt;&gt; ViT.vit_base_patch16_224(activation = nn.SELU)</span>
<span class="sd">            &gt;&gt;&gt; # change number of classes (default is 1000 )</span>
<span class="sd">            &gt;&gt;&gt; ViT.vit_base_patch16_224(n_classes=100)</span>
<span class="sd">            &gt;&gt;&gt; # pass a different block, default is TransformerEncoderBlock</span>
<span class="sd">            &gt;&gt;&gt; ViT.vit_base_patch16_224(block=MyCoolTransformerBlock)</span>
<span class="sd">            &gt;&gt;&gt; # get features</span>
<span class="sd">            &gt;&gt;&gt; model = ViT.vit_base_patch16_224</span>
<span class="sd">            &gt;&gt;&gt; # first call .features, this will activate the forward hooks and tells the model you&#39;ll like to get the features</span>
<span class="sd">            &gt;&gt;&gt; model.encoder.features</span>
<span class="sd">            &gt;&gt;&gt; model(torch.randn((1,3,224,224)))</span>
<span class="sd">            &gt;&gt;&gt; # get the features from the encoder</span>
<span class="sd">            &gt;&gt;&gt; features = model.encoder.features</span>
<span class="sd">            &gt;&gt;&gt; print([x.shape for x in features])</span>
<span class="sd">            &gt;&gt;&gt; #[[torch.Size([1, 197, 768]),  torch.Size([1, 197, 768]), ...]</span>
<span class="sd">            &gt;&gt;&gt; # change the tokens, you have to subclass ViTTokens</span>
<span class="sd">            &gt;&gt;&gt; class MyTokens(ViTTokens):</span>
<span class="sd">            &gt;&gt;&gt;     def __init__(self, emb_size: int):</span>
<span class="sd">            &gt;&gt;&gt;         super().__init__(emb_size)</span>
<span class="sd">            &gt;&gt;&gt;         self.my_new_token = nn.Parameter(torch.randn(1, 1, emb_size))</span>
<span class="sd">            &gt;&gt;&gt; ViT(tokens=MyTokens)</span>

<span class="sd">        Args:</span>
<span class="sd">            in_channels (int, optional): [description]. Defaults to 3.</span>
<span class="sd">            patch_size (int, optional): [description]. Defaults to 16.</span>
<span class="sd">            emb_size (int, optional):  Embedding dimensions Defaults to 768.</span>
<span class="sd">            img_size (int, optional): [description]. Defaults to 224.</span>
<span class="sd">            tokens (nn.Module, optional): A module that contains the tokens as his parameters. Defaults to ViTTokens.</span>
<span class="sd">            depth (int, optional): [description]. Defaults to 12.</span>
<span class="sd">            n_classes (int, optional): [description]. Defaults to 1000.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">OrderedDict</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="s2">&quot;embedding&quot;</span><span class="p">:</span> <span class="n">embedding</span><span class="p">(</span>
                        <span class="n">in_channels</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">,</span> <span class="n">img_size</span><span class="p">,</span> <span class="n">tokens</span>
                    <span class="p">),</span>
                    <span class="s2">&quot;encoder&quot;</span><span class="p">:</span> <span class="n">encoder</span><span class="p">(</span><span class="n">depth</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">),</span>
                    <span class="s2">&quot;head&quot;</span><span class="p">:</span> <span class="n">head</span><span class="p">(</span><span class="n">emb_size</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">),</span>
                <span class="p">}</span>
            <span class="p">)</span>
        <span class="p">)</span>

<div class="viewcode-block" id="ViT.vit_small_patch16_224"><a class="viewcode-back" href="../../../../glasses.models.classification.vit.html#glasses.models.classification.ViT.vit_small_patch16_224">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">vit_small_patch16_224</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">depth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">forward_expansion</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="ViT.vit_base_patch16_224"><a class="viewcode-back" href="../../../../glasses.models.classification.vit.html#glasses.models.classification.ViT.vit_base_patch16_224">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">vit_base_patch16_224</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">depth</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">forward_expansion</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">qkv_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="ViT.vit_base_patch16_384"><a class="viewcode-back" href="../../../../glasses.models.classification.vit.html#glasses.models.classification.ViT.vit_base_patch16_384">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">vit_base_patch16_384</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">vit_base_patch16_224</span><span class="p">(</span><span class="n">img_size</span><span class="o">=</span><span class="mi">384</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="ViT.vit_base_patch32_384"><a class="viewcode-back" href="../../../../glasses.models.classification.vit.html#glasses.models.classification.ViT.vit_base_patch32_384">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">vit_base_patch32_384</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">vit_base_patch16_384</span><span class="p">(</span><span class="n">patch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="ViT.vit_large_patch16_224"><a class="viewcode-back" href="../../../../glasses.models.classification.vit.html#glasses.models.classification.ViT.vit_large_patch16_224">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">vit_large_patch16_224</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">emb_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">qkv_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="ViT.vit_large_patch16_384"><a class="viewcode-back" href="../../../../glasses.models.classification.vit.html#glasses.models.classification.ViT.vit_large_patch16_384">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">vit_large_patch16_384</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">vit_large_patch16_224</span><span class="p">(</span><span class="n">img_size</span><span class="o">=</span><span class="mi">384</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="ViT.vit_large_patch32_384"><a class="viewcode-back" href="../../../../glasses.models.classification.vit.html#glasses.models.classification.ViT.vit_large_patch32_384">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">vit_large_patch32_384</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">vit_large_patch16_384</span><span class="p">(</span><span class="n">patch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="ViT.vit_huge_patch16_224"><a class="viewcode-back" href="../../../../glasses.models.classification.vit.html#glasses.models.classification.ViT.vit_huge_patch16_224">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">vit_huge_patch16_224</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">emb_size</span><span class="o">=</span><span class="mi">1280</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="ViT.vit_huge_patch32_384"><a class="viewcode-back" href="../../../../glasses.models.classification.vit.html#glasses.models.classification.ViT.vit_huge_patch32_384">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">vit_huge_patch32_384</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">vit_huge_patch16_224</span><span class="p">(</span><span class="n">patch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">img_size</span><span class="o">=</span><span class="mi">384</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div></div>
</pre></div>

           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, Francesco Saverio Zuppichini &amp; Francesco Cicala.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>